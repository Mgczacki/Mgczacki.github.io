The dependency chain claim is asserted more than demonstrated. The paper's central structural conceit is that the principles form a "dependency chain where each builds on the last." But the actual dependencies are loose. Context engineering doesn't depend on structure in the way, say, compilation depends on parsing — it's more that structure helps context engineering. The paper would be more honest calling these "mutually reinforcing principles" rather than a chain. The dependency graph figure presumably shows arrows between nodes, but the text doesn't establish that removing any one principle actually breaks the ones downstream. Compare this to something like the CAP theorem, where the dependencies are formal and falsifiable. Here, the "chain" is more of a narrative ordering.
The cost model (Section 2.3) is the strongest idea but is left as intuition rather than made operational. The formula C_s < ΔC_f × P(failure) is presented as the economic argument, but it's never instantiated. What does C_s actually look like for a real system? How do you estimate P(failure) before building the structure that would reduce it? The error compounding table is effective rhetoric, but it assumes independence between steps (each step fails with the same probability regardless of prior steps), which is often false in practice — failures are correlated, and a model that gets step 3 wrong is more likely to get step 4 wrong too. That actually strengthens the argument for structure, but the paper misses the chance to say so, and instead presents a simplified model that a careful reader might dismiss.
The validation cost asymmetry table is misleading. Listing compiler checks and linters as "0 tokens" is technically true but obscures the real cost: someone had to write the code being compiled, define the schemas being validated, and build the test suites. The table compares marginal runtime cost of validation against marginal runtime cost of inference, which is an apples-to-oranges comparison when the argument is about whether to invest in structure. The honest comparison would include the amortized development cost of the validation infrastructure.
"Structure reduces problem space" conflates several distinct mechanisms. Section 2.1 lumps together constraint satisfaction (reducing search space), output validation (catching errors post-hoc), tool abstraction (encapsulating complexity), and delegation (splitting work). These are genuinely different things. Constraint satisfaction prevents bad states; validation detects them; tools hide complexity; delegation distributes it. Claiming they all flow from one principle ("structure") makes the principle so broad it explains everything and predicts nothing. A critic could argue that "structure helps" is almost tautological for engineered systems.
The flexibility-reliability tradeoff framing undersells the real tension. The paper presents this as a maturity curve — immature tasks need flexibility, mature tasks benefit from structure — which is reasonable but incomplete. The deeper tension is that structure encodes assumptions about the task distribution, and when those assumptions break, structured systems fail catastrophically rather than gracefully. A rigid pipeline that handles 99% of cases perfectly but can't handle the 1% edge case is sometimes worse than a flexible agent that handles everything at 90%. The paper gestures at "when structure doesn't pay" but treats those as edge cases rather than as a fundamental design consideration. The Bitter Lesson reference in the conclusion acknowledges this but doesn't engage with it seriously.
The agent model taxonomy (Section 3) doesn't clearly distinguish itself from existing surveys. Wang et al. [44], Weng [10], and Huang et al. [58] already provide agent taxonomies. The paper cites them but doesn't articulate what the "structured agency" lens adds. Is the claim that existing taxonomies don't emphasize harnesses? If so, the taxonomy should be organized around harness points — where can structure be injected, and what does it buy you at each point? Instead, the taxonomy reads as a conventional decomposition (actions, state, planning, composition, etc.) with "structured" as an adjective applied to each.
The state ≠ context window distinction is valuable but underexplored. This is one of the paper's better insights — that many practitioners conflate the model's context window with the agent's actual state, leading to fragile systems. But the paper states it and moves on. It would benefit from showing what goes wrong when you make this mistake (concrete failure modes), and from exploring the design implications: if state is structured (databases, knowledge graphs), then what is the interface contract between structured state and the unstructured context window? How do you serialize/deserialize? What information is lost? This is where real engineering guidance lives.
"Communication is a tool" is stated as a principle but not justified. Why should communication be modeled as tool use rather than as a first-class primitive? There are real arguments both ways — tool-use framing gives you validation and observability for free, but it also forces asynchronous patterns that may be awkward for real-time dialogue. The paper asserts the principle without engaging the counterargument.
The planning section is the weakest non-WIP section. Even the parts that aren't marked TODO are mostly a literature survey (tree-of-thought, LATS, etc.) with a table of plan representations. The "structured agency" angle would predict a specific claim here — something like "plans should be externally verifiable artifacts, not internal chain-of-thought" — but the paper doesn't commit to a position. The principle "plans should be treated as hypotheses, not commitments" is borrowed from Reflexion [57] and isn't developed further.
The safety section is solid but arrives too late and feels disconnected. Prompt injection, least privilege, and trust boundaries are treated as Section 3.8 rather than being woven into the core principles. Given that the paper argues structure enables safety (trust boundaries are a form of structure), safety should arguably be a motivating example for the principles, not a downstream application. The prompt injection defense strategies listed are also fairly standard (privilege boundaries, context tagging, output filtering) — the paper would be stronger if it showed how the structured agency framework generates these defenses rather than just listing them.
The reference list mixes authoritative and lightweight sources uncritically. Canonical references (Sutton & Barto, Russell & Norvig, Simon 1955) sit alongside blog posts from Eficode and Rebelion.la without any signaling about source quality. The paper would benefit from distinguishing between foundational references, empirical evidence, and practitioner commentary. Some citations also feel like padding — e.g., citing Nielsen on progressive disclosure [31] adds nothing that the concept's plain-English description doesn't already convey.
The writing occasionally confuses description with argumentation. Sections like 3.1 and 3.2 read as "here is how we define agents and tools" rather than "here is why this decomposition matters and what it buys you." A taxonomy is only valuable if it helps you make decisions; the paper should consistently connect each taxonomic distinction back to a design choice the reader faces.
One missing critique the paper should address preemptively: isn't this just good software engineering? The most natural objection is that "structured agency" is just "apply software engineering best practices to LLM agents" — type safety, validation, separation of concerns, integration testing, least privilege. The paper would be stronger if it directly confronted this and explained what's genuinely novel about applying these ideas to stochastic reasoning systems versus deterministic ones.
