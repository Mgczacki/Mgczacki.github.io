<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Structured Agency | Mario Garrido</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        :root {
            --bg: #fff;
            --bg-alt: #f7f7f7;
            --text: #111;
            --text-secondary: #333;
            --text-muted: #666;
            --text-faint: #999;
            --border: #111;
            --border-light: #e5e5e5;
            --border-faint: #f0f0f0;
            --accent: #000;
            --green: #3d7a4a;
            --green-light: #e8f0ea;
            --green-border: #b8d4be;
        }

        [data-theme="dark"] {
            --bg: #0d0d0d;
            --bg-alt: #1a1a1a;
            --text: #f0f0f0;
            --text-secondary: #d0d0d0;
            --text-muted: #a0a0a0;
            --text-faint: #606060;
            --border: #404040;
            --border-light: #2a2a2a;
            --border-faint: #1f1f1f;
            --accent: #fff;
            --green: #6aab73;
            --green-light: #1a2b1e;
            --green-border: #2d4a33;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', 'Helvetica Neue', Helvetica, Arial, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.65;
            font-size: 16px;
            -webkit-font-smoothing: antialiased;
            transition: background 0.3s, color 0.3s;
        }

        .masthead {
            padding: 48px 0 40px;
            border-bottom: 1px solid var(--border);
        }

        .masthead-inner {
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 60px;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .logo {
            font-size: 15px;
            font-weight: 700;
            letter-spacing: -0.3px;
            color: var(--text);
            text-decoration: none;
        }

        .masthead-right {
            display: flex;
            align-items: center;
            gap: 16px;
        }

        .nav-links {
            display: flex;
            gap: 24px;
            align-items: center;
        }

        .nav-links a {
            color: var(--text-muted);
            text-decoration: none;
            font-size: 13px;
            transition: color 0.2s;
            font-weight: 500;
        }

        .nav-links a:hover {
            color: var(--text);
        }

        .theme-toggle {
            background: none;
            border: 1px solid var(--border-light);
            border-radius: 20px;
            padding: 6px 10px;
            cursor: pointer;
            display: flex;
            align-items: center;
            gap: 6px;
            font-size: 12px;
            color: var(--text-muted);
            transition: all 0.2s;
        }

        .theme-toggle:hover {
            border-color: var(--text);
            color: var(--text);
        }

        .theme-toggle svg {
            width: 14px;
            height: 14px;
        }

        [data-theme="dark"] .sun-icon {
            display: block;
        }

        [data-theme="dark"] .moon-icon {
            display: none;
        }

        [data-theme="light"] .sun-icon {
            display: none;
        }

        [data-theme="light"] .moon-icon {
            display: block;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 60px 60px 120px;
            display: grid;
            grid-template-columns: 220px 1fr;
            gap: 80px;
            align-items: start;
        }

        /* Sidebar TOC: base styles loaded from assets/sidebar-toc.css */

        .content-wrapper {
            min-width: 0;
            max-width: 900px;
        }

        .page-header {
            margin-bottom: 48px;
            padding-bottom: 40px;
            border-bottom: 1px solid var(--border-faint);
        }

        .page-header h1 {
            font-size: 36px;
            font-weight: 700;
            margin-bottom: 16px;
            letter-spacing: -0.5px;
            color: var(--text);
            line-height: 1.2;
        }

        .meta {
            color: var(--text-faint);
            font-size: 13px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            font-weight: 500;
        }

        #content {
            background: transparent;
        }

        #content h1 {
            font-size: 28px;
            font-weight: 700;
            margin: 48px 0 20px 0;
            letter-spacing: -0.4px;
            line-height: 1.25;
            color: var(--text);
        }

        #content h2 {
            font-size: 24px;
            font-weight: 700;
            margin: 56px 0 20px 0;
            color: var(--text);
            letter-spacing: -0.3px;
            line-height: 1.3;
            padding-top: 8px;
        }

        #content h2:first-child {
            margin-top: 0;
        }

        #content h3 {
            font-size: 18px;
            font-weight: 700;
            margin: 36px 0 16px 0;
            color: var(--text);
            letter-spacing: -0.2px;
            line-height: 1.35;
        }

        #content h4 {
            font-size: 16px;
            font-weight: 600;
            margin: 28px 0 12px 0;
            color: var(--text);
        }

        #content h5 {
            font-size: 15px;
            font-weight: 600;
            margin: 24px 0 10px 0;
            color: var(--text);
        }

        #content p {
            margin-bottom: 1.5em;
            color: var(--text-secondary);
            font-size: 16px;
            line-height: 1.7;
        }

        #content ul, #content ol {
            margin-bottom: 1.5em;
            padding-left: 24px;
        }

        #content li {
            margin-bottom: 0.65em;
            line-height: 1.7;
            color: var(--text-secondary);
        }

        #content strong {
            color: var(--text);
            font-weight: 600;
        }

        #content code {
            background: var(--bg-alt);
            padding: 3px 6px;
            border-radius: 3px;
            font-family: 'SF Mono', Monaco, monospace;
            font-size: 14px;
            color: var(--text);
            border: 1px solid var(--border-faint);
        }

        #content pre {
            background: var(--bg-alt);
            padding: 20px;
            border-radius: 6px;
            overflow-x: auto;
            margin-bottom: 1.5em;
            border: 1px solid var(--border-light);
        }

        #content pre code {
            background: none;
            padding: 0;
            color: var(--text-muted);
            border: none;
            font-size: 13px;
        }

        #content blockquote {
            border-left: 2px solid var(--border-light);
            padding-left: 20px;
            margin: 1.5em 0;
            color: var(--text-muted);
            font-style: italic;
        }

        #content a {
            color: var(--text);
            font-weight: 500;
            text-decoration: underline;
            text-decoration-color: var(--border-light);
            transition: text-decoration-color 0.2s;
        }

        #content a:hover {
            text-decoration-color: var(--text);
        }

        #content a.citation {
            text-decoration: none;
            color: var(--text-muted);
            font-weight: 400;
            font-size: 0.88em;
        }

        #content a.citation:hover {
            color: var(--text);
            text-decoration: underline;
        }

        #content details.counterpoints {
            margin: 24px 0 32px;
            border: 1px solid var(--border-light);
            border-radius: 6px;
            padding: 0;
            background: var(--bg-alt);
        }

        #content details.counterpoints summary {
            cursor: pointer;
            padding: 14px 20px;
            font-size: 13px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.4px;
            color: var(--text-muted);
            list-style: none;
            display: flex;
            align-items: center;
            gap: 8px;
            user-select: none;
        }

        #content details.counterpoints summary::-webkit-details-marker {
            display: none;
        }

        #content details.counterpoints summary::before {
            content: '▸';
            font-size: 12px;
            transition: transform 0.2s;
        }

        #content details.counterpoints[open] summary::before {
            transform: rotate(90deg);
        }

        #content details.counterpoints summary:hover {
            color: var(--text);
        }

        #content details.counterpoints .counterpoints-body {
            padding: 0 20px 20px;
            border-top: 1px solid var(--border-light);
        }

        #content details.counterpoints .counterpoints-body p {
            font-size: 15px;
            margin-bottom: 1em;
        }

        #content details.counterpoints .counterpoints-body p:last-child {
            margin-bottom: 0;
        }

        #content details.counterpoints .counterpoints-body .author-response {
            margin-top: 16px;
            padding-left: 16px;
            border-left: 2px solid var(--border-light);
        }

        #content details.counterpoints .counterpoints-body .author-response p:first-child::before {
            content: 'Author\27s note: ';
            font-weight: 600;
            color: var(--text);
        }

        #content details.practical-example {
            margin: 24px 0 32px;
            border: 1px solid var(--green-border);
            border-left: 3px solid var(--green);
            border-radius: 6px;
            padding: 0;
            background: var(--green-light);
        }

        #content details.practical-example summary {
            cursor: pointer;
            padding: 14px 20px;
            font-size: 13px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.4px;
            color: var(--green);
            list-style: none;
            display: flex;
            align-items: center;
            gap: 8px;
            user-select: none;
        }

        #content details.practical-example summary::-webkit-details-marker {
            display: none;
        }

        #content details.practical-example summary::before {
            content: '\25B8';
            font-size: 12px;
            transition: transform 0.2s;
        }

        #content details.practical-example[open] summary::before {
            transform: rotate(90deg);
        }

        #content details.practical-example summary:hover {
            color: var(--text);
        }

        #content details.practical-example .practical-example-body {
            padding: 0 20px 20px;
            border-top: 1px solid var(--green-border);
        }

        #content details.practical-example .practical-example-body p {
            font-size: 15px;
            margin-bottom: 1em;
        }

        #content details.practical-example .practical-example-body p:last-child {
            margin-bottom: 0;
        }

        #content details.practical-example .practical-example-body ol,
        #content details.practical-example .practical-example-body ul {
            font-size: 15px;
        }

        #content .principle {
            margin: 20px 0;
            padding: 12px 18px;
            border-left: 3px solid var(--text);
            background: var(--bg-alt);
            border-radius: 0 4px 4px 0;
            font-size: 15px;
            line-height: 1.6;
        }

        #content .principle p {
            margin: 0;
        }

        #content .principle strong {
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.3px;
            font-size: 11px;
        }

        #content img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 2em auto;
        }

        [data-theme="dark"] #content img {
            filter: invert(1);
        }

        #content hr {
            border: none;
            border-top: 1px solid var(--border-light);
            margin: 48px 0;
        }

        #content table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5em 0;
            font-size: 15px;
        }

        #content table th {
            font-size: 11px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            color: var(--text-faint);
            text-align: left;
            padding: 0 12px 16px 0;
            border-bottom: 1px solid var(--border-light);
        }

        #content table td {
            padding: 16px 12px 16px 0;
            border-bottom: 1px solid var(--border-faint);
            vertical-align: top;
            color: var(--text-secondary);
        }

        #content table tr:last-child td {
            border-bottom: none;
        }

        /* Version filtering UI — commented out for now, showing full article only
        .filter-bar {
            display: flex;
            gap: 8px;
            flex-wrap: wrap;
        }

        .filter-btn {
            background: none;
            color: var(--text-muted);
            padding: 0 0 2px;
            font-size: 13px;
            border: none;
            border-bottom: 1.5px solid transparent;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            font-family: inherit;
        }

        .filter-btn:hover {
            color: var(--text);
        }

        .filter-btn.active {
            color: var(--text);
            border-bottom-color: var(--text);
        }

        .version-hidden {
            display: none;
        }

        .variant-toggle {
            display: inline-flex;
            gap: 0;
            margin-bottom: 12px;
            border: 1px solid var(--border-light);
            border-radius: 4px;
            overflow: hidden;
        }

        .variant-toggle-btn {
            background: none;
            border: none;
            padding: 4px 12px;
            font-size: 12px;
            font-weight: 500;
            color: var(--text-muted);
            cursor: pointer;
            font-family: inherit;
            transition: all 0.2s;
        }

        .variant-toggle-btn:not(:last-child) {
            border-right: 1px solid var(--border-light);
        }

        .variant-toggle-btn:hover {
            color: var(--text);
        }

        .variant-toggle-btn.active {
            background: var(--bg-alt);
            color: var(--text);
        }
        */

        @media (max-width: 1024px) {
            .container {
                grid-template-columns: 1fr;
                gap: 40px;
            }

            .sidebar {
                position: static;
                max-height: none;
                display: none;
            }
        }

        @media (max-width: 768px) {
            .masthead-inner {
                padding: 0 24px;
            }

            .nav-links {
                gap: 16px;
            }

            .container {
                padding: 40px 24px 80px;
            }

            .page-header h1 {
                font-size: 28px;
            }

            #content h2 {
                font-size: 22px;
            }

            #content h3 {
                font-size: 17px;
            }
        }
    </style>
    <link rel="stylesheet" href="assets/sidebar-toc.css">
    <script src="assets/sidebar-toc.js"></script>
    <link rel="stylesheet" href="assets/sticky-header.css">
    <script src="assets/sticky-header.js"></script>
</head>
<body>
    <header class="masthead">
        <div class="masthead-inner">
            <a href="index.html" class="logo">Mario Garrido</a>
            <div class="masthead-right">
                <div class="nav-links">
                    <a href="writings.html">Writings</a>
                    <a href="projects.html">Projects</a>
                    <a href="cv.html">CV</a>
                </div>
                <button class="theme-toggle" onclick="toggleTheme()" aria-label="Toggle dark mode">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </header>

    <div class="container">
        <aside class="sidebar">
            <h3>Contents</h3>
            <nav>
                <ul>
                    <li><a href="#introduction">1. Introduction</a>
                        <ul class="subsection">
                            <li><a href="#current-state">1.1 Current State</a></li>
                            <li><a href="#evolution">1.2 Evolution</a></li>
                        </ul>
                    </li>
                    <li><a href="#principles">2. Core Principles</a>
                        <ul class="subsection">
                            <li><a href="#structure">2.1 Structure</a></li>
                            <li><a href="#minimize-context">2.2 Context Engineering</a></li>
                            <li><a href="#unconstrained">2.3 Unconstrained Cost</a></li>
                            <li><a href="#tradeoff">2.4 Flexibility-Reliability</a>
                                <ul class="subsection">
                                    <li><a href="#harness-obsolescence">2.4.1 Harness Obsolescence</a></li>
                                </ul>
                            </li>
                            <li><a href="#corollaries">2.5 Corollaries</a></li>
                        </ul>
                    </li>
                    <li><a href="#taxonomy">3. Taxonomy of Structured Agents</a>
                        <ul class="subsection">
                            <li><a href="#agent-model">3.1 Agent Model</a>
                                <ul class="subsection">
                                    <li><a href="#structure-classes">Structure Classes</a></li>
                                </ul>
                            </li>
                            <li><a href="#actions">3.2 Actions</a></li>
                            <li><a href="#information">3.3 Information Mgmt</a></li>
                            <li><a href="#composition">3.4 Composition</a>
                                <ul class="subsection">
                                    <li><a href="#delegation-challenges">3.4.1 Delegation Challenges</a></li>
                                </ul>
                            </li>
                            <li><a href="#planning">3.5 Planning</a></li>
                            <li><a href="#error-handling">3.6 Error Handling</a>
                                <ul class="subsection">
                                    <li><a href="#interruptions">Interruptions</a></li>
                                </ul>
                            </li>
                            <li><a href="#evaluation">3.7 Evaluation</a></li>
                            <li><a href="#safety">3.8 Safety & Security</a></li>
                        </ul>
                    </li>
                    <li><a href="#future">4. Open Questions</a></li>
                    <li><a href="#conclusion">5. Conclusion</a></li>
                    <li><a href="#appendix">Appendix</a></li>
                    <li><a href="#references">References</a></li>
                </ul>
            </nav>
        </aside>

        <div class="content-wrapper">
            <div class="page-header">
                <h1>Structured Agency</h1>
                <div class="meta">Mario Garrido • February 2026</div>
            </div>

            <!-- Version filter bar — commented out for now, showing full article only
            <div class="filter-bar" style="margin-bottom: 32px;">
                <button class="filter-btn active" data-filter="full">Full</button>
                <button class="filter-btn" data-filter="formal">Formal</button>
                <button class="filter-btn" data-filter="builder">Builder</button>
            </div>
            -->

            <div id="content"></div>
        </div>
    </div>

    <script>
        // Your whitepaper content in Markdown
        const whitepaperMarkdown = `
### On Building Reliable Agents

<div style="background:#fff0f0; border:2px solid #c00; border-radius:6px; padding:16px 20px; margin:24px 0; color:#900; font-size:15px; line-height:1.6;">
<strong>Work in Progress.</strong> This document contains many placeholders, unverified citations, and slop illustrations. Do not treat it as a finished or authoritative reference.
</div>

The principles behind reliable agents are not new. Constraint satisfaction, hierarchical decomposition, external validation, and progressive disclosure are well-understood across mathematics, cognitive science, optimization, and software engineering. What has been missing is a unified framework that connects these ideas into a coherent practice for building LLM-powered agents—one that explains not just *what* to do, but *why* each principle follows from the others, and *when* the tradeoffs justify the investment.

This paper proposes **Structured Agency**: a framework for building harnesses around LLM-powered agents through deliberate constraints, external validation, and composable tooling. The agents that result are **structured agents**—systems that trade unconstrained flexibility for reliability, observability, and predictable behavior. The framework is grounded in three years of experience building agents across production systems at [kuona.ai](https://kuona.ai), academic research, and personal projects, informed by industry analysis, and organized as a dependency chain of principles where each builds on the last. The naming and formalism are my own and evolve over time.

---

## 1. Introduction

### 1.1 The Current State of Agents

The past year has seen an explosion in agentic systems. Coding agents—Claude Code [7], OpenAI Codex [84], GitHub Copilot [85], Google Jules [86], Devin [87]—now routinely navigate codebases, write code, and run tests autonomously. Computer-using agents like OpenAI Operator [88] and Google Mariner [89] interact with GUIs through screenshots and virtual input. Multi-agent research systems [24] coordinate teams of specialized subagents. General-purpose agents like Manus achieved state-of-the-art on general assistant benchmarks [66] before being acquired by Meta. Interoperability protocols—MCP [46], Agent2Agent [90], Agent Skills [91]—have standardized how agents connect to tools and to each other. These systems, along with foundational research [6][3][9], have converged on several insights that, taken together, motivate the framework proposed in this paper.

**1. Agents thrive with reliable, small-scoped tools which are composable** [1][2][3][4].

The UNIX design philosophy aligns remarkably well with this principle [5]. Tools like \`grep\`, \`cat\`, and \`sed\` have become commonplace in the most capable agents—not by accident, but because they embody:
- Single responsibility
- Composability
- Predictable behavior
- Simple interfaces

The convergence on composable tooling has gone beyond individual systems to produce open standards. The Model Context Protocol (MCP) [46] standardized how agents connect to tools through typed interfaces; Google's Agent2Agent Protocol (A2A) [90] standardized inter-agent communication through capability discovery ("Agent Cards"); Anthropic's Agent Skills specification [91] standardized reusable capability packages. By late 2025, MCP alone had 97M+ monthly SDK downloads and adoption from every major provider. This is not accidental—it reflects a structural insight: composable, well-defined tool interfaces are the foundation of reliable agent behavior.

**2. The most effective agents exploit the underlying structure of their problem domain.**

Composable tools are the mechanism, but the deeper pattern is that capable agents don't treat the world as unstructured text to reason over—they navigate the domain's own organization. Every major coding agent—Claude Code [7], Codex [84], Copilot [85], Jules [86], Cursor, Amazon Q—exploits the inherent structure of code: grepping file trees, following import graphs, reading type signatures, querying language server protocols [81][82]. A database agent that queries schemas before writing SQL exploits the structure of relational data. In each case, effectiveness comes not from raw reasoning over tokens but from leveraging domain structure that already exists—or, where it doesn't exist, creating it (Section 3.3).

**3. External systems validate better and cheaper than models self-assess** [17][18].

The field has converged on outsourcing verification to purpose-built external mechanisms—compilers, linters, test suites, schema validators, rule engines—not because models *cannot* self-correct, but because external validation provides reliable, deterministic signals. This pattern now appears at every scale: OpenAI Codex [84] runs in sandboxed containers with internet disabled (constraint through isolation), Operator [88] hands control to users when uncertain (human-in-the-loop validation), Meta's LlamaFirewall [93] provides guardrails as a separate system independent of the agent's reasoning, and Anthropic's harness patterns [92] use progress files and git checkpoints as external recovery signals for long-running agents. Section 2.1.1 establishes the principle; Section 2.3 makes the economic case.

**4. The context window is scarce working memory, not a knowledge store** [25][26][27].

The most capable production agents deliberately manage what enters the context window, when, and in what form. The discipline of **context engineering** [30]—selecting, ordering, representing, and timing the information an agent operates on—has emerged as a named practice precisely because context quality matters more than context quantity, and irrelevant context actively degrades performance. By mid-2025, the term had been popularized by Shopify CEO Tobi Lutke and Andrej Karpathy, reflecting a shift in the field's understanding: reliability comes not from better prompts but from better-structured context (Section 2.2).

### 1.2 Evolution of Agentic Workflows

![Evolution Timeline](assets/agents/image2-evolution.svg)

The evolution of agentic capabilities has progressed alongside model improvements [3][8][9][10]:

1. **Bespoke tasks with LLMs** → Handcrafted prompts with manual routing
2. **Workflows with state machines** → Deterministic transitions between states
3. **Agentic routing** → Dynamic decision-making with tools
4. **Subagent delegation** → Hierarchical task decomposition

Crucially, these levels are not a strict progression where each supersedes the last—**they coexist**. A production system will typically contain all four: bespoke prompts for well-understood subtasks, state-machine workflows where determinism is needed, agentic routing for flexible decision-making, and subagent delegation for complex decomposition. In my own work, all of these coexist to form reliable, cost-effective, and flexible systems. The right level depends on the task and domain, not on the system's overall sophistication. A useful heuristic: the lower on this curve a task sits, the more that context stuffing and simple tool design dominate; the higher, the more that deliberate context engineering and structural harnesses become necessary.

<details class="practical-example">
<summary>Practical Example — All Four Levels in One System</summary>
<div class="practical-example-body">

<p>A system I built at kuona.ai illustrates how all four levels coexist within a single task execution. The system is a general business agent that, among other capabilities, can delegate data transformation work to a specialized subagent.</p>

![Practical Example: Four Levels](assets/agents/image10-practical-example.svg)

<p>A user asks the general agent to clean up a product catalog. The agent determines that a column of free-text product names needs to be transformed into standardized brand categories. Here is what happens:</p>

<p><strong>Level 4 — Subagent delegation.</strong> The general agent delegates the transformation task to a specialized data transformation subagent, passing a structured instruction: "Extract the brand from these product names."</p>

<p><strong>Level 3 — Agentic routing.</strong> The data transformation subagent operates in a ReAct loop. It dynamically selects which tool to call based on the instruction—in this case, a column transformation workflow that works on a Table (a structured class, see Section 3.1) and targets a specific column.</p>

<p><strong>Level 2 — Workflow with deterministic transitions.</strong> The column transformation tool is itself a workflow—a state machine that orchestrates a fixed sequence of operations: (1) obtain the column values, (2) deduplicate to extract unique values, (3) run a map operation over the unique values using an LLM, (4) apply the resulting mapping to the entire column to produce a derived column.</p>

<p><strong>Level 1 — Bespoke LLM prompt.</strong> Inside the map operation, a purpose-built prompt receives each unique product name and extracts the brand. This is a single-purpose, well-scoped LLM call with no tool access and no agentic behavior—just input → output.</p>

<p>The workflow completes, producing a new "brand" column. Control returns to the ReAct agent, which may perform additional transformations. When the subagent finishes, control returns to the general agent, which continues with the next business task.</p>

<p>Note what happened: a single user request traversed all four levels of the evolution curve. The bespoke prompt didn't need tools. The workflow didn't need agentic routing. The ReAct agent didn't need to understand the workflow's internals. And the general agent didn't need to know how brand extraction works. Each level operates within its appropriate constraints, and the hierarchical structure ensures that complexity is contained at each boundary.</p>

</div>
</details>

This progression reveals three fundamental questions that any agentic system must answer:

- **Partition of work**: Who does what work?
- **Partition of responsibilities**: Who is accountable for what outcomes?
- **Partition of knowledge**: Who knows what context?

<details class="counterpoints">
<summary>Counterpoints & Discussion</summary>
<div class="counterpoints-body">
<p>The "small, composable tools" thesis is not without criticism. Research suggests that tool overload—having too many fine-grained tools—can increase decision friction and hurt selection accuracy [68][69]. LlamaIndex (2024) notes that fine-grained decomposition creates an increased burden on less-capable models [70]. The UNIX "do one thing well" metaphor may not map cleanly to LLM tool selection, where the cost of choosing between many similar tools can outweigh the benefits of composability.</p>
<div class="author-response">
<p>My early experiments agree with these findings, but three mitigations exist: (1) formal plan validation with retries catches tool-selection errors before they propagate, (2) searching over the tool space with a dedicated meta-tool and progressive disclosure reduces the effective choice set, and (3) in most production systems the tool count is bounded by the use case—these issues are buildable-around. Recent model generations are also increasingly capable at tool selection, making this less of a concern over time.</p>
</div>
</div>
</details>

---

## 2. Core Principles

The following principles are not independent—they form a dependency chain rooted in the idea that **structure is the fundamental enabler** of reliable agents. Together, they constitute the core of structured agency: a framework for building harnesses that make agents reliably capable.

![Principle Dependency Graph](assets/agents/image1-principle-graph.svg)

### 2.1 Structure Reduces Problem Space

<div class="principle">

**Principle**: Constraints enable better performance, not worse [11][12][13].

</div>

This is the foundational principle from which most others follow. Constraints:
1. Enable validation signals (checkable boundaries)
2. Reduce problem spaces (fewer paths to explore)
3. Improve computational economy (less work per step) [14]
4. Prevent error propagation between tasks (isolation)
5. Enable effective delegation (clear boundaries for splitting work across agents—see Section 3.4 for the full treatment)

#### 2.1.1 External Validation Signals

Once a system is structured, its boundaries become checkable—which is what enables external validation.

<div class="principle">

**Principle**: Validation must come from outside the reasoning system itself [17][18].

</div>

**Examples**:
- Code linting and compilation
- Rule-tracking systems
- Formal plan verification [19]
- World-model validation
- External test suites

**Why this matters**: Validation signals allow agents to course-correct [20]. Crucially, these constraints should stem from mechanisms OUTSIDE the agent to form genuine boundaries.

<details class="counterpoints">
<summary>Counterpoints & Discussion</summary>
<div class="counterpoints-body">
<p>Recent work challenges the strict necessity of external validation. Kumar et al. [21] achieve significant gains on MATH benchmarks through self-correction via reinforcement learning without external feedback. Intrinsic self-critique methods [73] have achieved strong results on planning benchmarks without external verifiers. However, the consensus remains that external feedback is more reliable for production systems [17][18].</p>
<div class="author-response">
<p>Self-correction is real and improving, but the economy of agents still favors external validation. For many problems, validation is cheaper than inference—external systems (compilers, linters, test suites, rule engines) can verify correctness at a fraction of the cost of having the model self-assess. When validation is cheap and inference is expensive, it pays to delegate the checking to purpose-built external mechanisms.</p>
</div>
</div>
</details>

#### 2.1.2 Tools Enable Hierarchical Reasoning

Structure, concretely applied, takes the form of tools—pre-built abstractions that encapsulate solved problems [22][23].

<div class="principle">

**Principle**: Delegate reasoning to higher-level abstractions through tools.

</div>

Even when a model *could* solve a task without tools, well-designed tools provide:
1. **Hierarchical reasoning** → No need to redo solved problems
2. **Predictable outputs** → Pre-validated, well-understood behavior
3. **Battle-tested reliability** → Hardened by previous use cases

<details class="counterpoints">
<summary>Counterpoints & Discussion</summary>
<div class="counterpoints-body">
<p>Tam et al. [15] show that strict format constraints can degrade LLM reasoning performance—models forced into rigid output schemas sometimes sacrifice accuracy. Park et al. [16] demonstrate that grammar-constrained decoding distorts probability distributions. Banerjee et al. [71] propose alternating constrained and unconstrained generation as a middle path. More broadly, the Bitter Lesson argument [72] suggests that hand-crafted structure will eventually be superseded by scaling compute and data.</p>
<div class="author-response">
<p>I have empirically verified that strict format constraints degrade reasoning in standard LLMs. However, reasoning models and inference-time scaling have in my view largely solved this issue—the performance penalty from structured output has shrunk significantly with recent model generations. As for the Bitter Lesson—see Section 2.4.1 for the full argument, but the short version is: structure competes on *economy*, not *capability*. Latency, cost, and predictability are orthogonal to what a model can theoretically derive from scratch.</p>
</div>
</div>
</details>

Structure also carries implicit alignment—a point developed fully in Section 3.8.

### 2.2 Context Engineering

Structure tells you what's relevant and what isn't—which leads to the operational principle of deliberately managing what enters the context window.

The field has converged on the term **context engineering** [30] to describe this practice: the discipline of designing the full informational environment an agent operates within—not just what's in the prompt, but what's retrieved, when it's loaded, how it's structured, and what's excluded. Context engineering subsumes prompt engineering the same way software architecture subsumes writing functions.

<div class="principle">

**Principle**: The context window is the agent's working memory. Structure it deliberately [25][26][27][28].

</div>

This is where the structure principle (Section 2.1) becomes operational. Structure tells you what information is relevant for a given task, which boundaries separate concerns, and which contexts belong to which agents. Without structure, context engineering degenerates into guesswork—stuffing tokens and hoping the model finds what it needs.

**What context engineering controls**:
- **Selection** → What information enters the context window (and what doesn't)
- **Ordering** → How information is sequenced (recency, relevance, dependency)
- **Representation** → How information is formatted (raw text, summaries, structured schemas)
- **Timing** → When information is loaded (upfront, on-demand, forced)

Research consistently shows that context quality matters more than context quantity. Irrelevant context actively degrades performance [26], length alone hurts even with perfect retrieval [27], and models exhibit predictable attention patterns that structured context can exploit [25].

#### 2.2.1 Progressive Disclosure

The primary mechanism for effective context engineering: don't load everything upfront—reveal information incrementally as needed [31][32].

<div class="principle">

**Principle**: Reveal information incrementally as needed.

</div>

This prevents context overload while maintaining access to necessary information. The parallel to cognitive science is striking: Miller's "Magical Number Seven" [33] and Baddeley's working memory model [34] both suggest that human cognition benefits from bounded information flow. Packer et al. [35] explore analogous memory management for LLMs in MemGPT.

Progressive disclosure is not the same as minimizing context. Sometimes the right context engineering decision is to load *more*—context stuffing remains highly effective when the corpus fits and is relevant. Progressive disclosure is the general pattern; context stuffing is the trivial special case where everything fits and everything is relevant.

<details class="counterpoints">
<summary>Counterpoints & Discussion</summary>
<div class="counterpoints-body">
<p>Long-context models consistently outperform RAG in average performance when sufficient compute is available [29]. Databricks (2024) shows that corpora under 2M tokens can be fed directly without retrieval. This suggests that aggressive context minimization is often counterproductive—more context, properly structured, yields better results than aggressive filtering. The real skill is knowing what to include, not how little you can get away with.</p>
<div class="author-response">
<p>I agree, and maximizing recall remains incredibly useful in some domains—particularly narrow, well-scoped tasks where the relevant corpus fits in context and the agent's job is bounded. But for general-purpose and flexible agents, defaulting to maximum recall is a lower-quality design. It works until it doesn't: the moment the corpus exceeds the context window, or the task requires navigating across domains, the agent has no mechanism for deciding what matters. This is exactly why I frame the principle as context *engineering* rather than context *minimization*. The goal is deliberate structure, not minimal tokens. When everything fits, load everything. When it doesn't—which is the common case for agents operating over large codebases, document corpora, or multi-session histories—progressive disclosure provides the mechanism for deciding what to load and when. The principle is about intentionality, not austerity. The connection to agentic maturity (Section 1.2) is worth noting: the lower on the evolution curve a task sits (bespoke prompts, simple workflows), the more context stuffing dominates. As agents take on more flexible, multi-step tasks, deliberate context management becomes essential.</p>
</div>
</div>
</details>

### 2.3 The Cost of Unconstrained Systems

What happens when you ignore structure? A fully unconstrained agent (no structure, no verification, no premade tools, no limitations) *might* solve a problem, but at what cost? To reason about this clearly, it helps to think in terms of two competing cost functions.

#### The Two Cost Functions

Every agentic system implicitly makes a tradeoff between two costs:

**Cost of Structure (C_s)**: The upfront and ongoing investment in constraints.
- **Design cost** → Engineering time to define tools, schemas, validation rules
- **Maintenance cost** → Keeping structure updated as requirements evolve
- **Flexibility cost** → Tasks that don't fit the structure require workarounds
- **Runtime overhead** → Validation checks, schema enforcement, plan verification

**Cost of Failure (C_f)**: The expected cost when things go wrong.
- **Direct cost** → Wasted tokens, API calls, compute on failed paths
- **Recovery cost** → Replanning, rollback, state repair, human intervention
- **Propagation cost** → Downstream errors caused by upstream failures
- **Irreversibility cost** → Actions that cannot be undone (emails sent, data deleted, state corrupted)

Structure is economically justified when the reduction in expected failure cost exceeds the cost of imposing it: **C_s < ΔC_f × P(failure)**. This is the economic argument for why structure motivates the rest of this paper.

#### Error Compounding in Multi-Step Tasks

The most powerful argument for structure is probabilistic. In a multi-step task, per-step reliability compounds multiplicatively:

| Per-step success rate | 5 steps | 10 steps | 20 steps |
|----------------------|---------|----------|----------|
| 90% (no structure)   | 59%     | 35%      | 12%      |
| 95% (light structure)| 77%     | 60%      | 36%      |
| 99% (heavy structure)| 95%     | 90%      | 82%      |

A 5-percentage-point improvement in per-step reliability (90% → 95%) nearly doubles the success rate over 10 steps. This is why structure matters most for long-horizon tasks—even small reliability gains at each step have dramatic compound effects.

**Corollary**: The longer the task horizon, the more structure is justified, because the compounding penalty for per-step unreliability grows exponentially. This also connects to the redundancy principle (Section 2.5.1): having multiple paths to success at each step is one mechanism for increasing per-step reliability.

#### The Validation Cost Asymmetry

A key economic property of agentic systems: **validation is almost always cheaper than inference**.

| Operation | Token cost | Latency | Reliability |
|-----------|-----------|---------|-------------|
| Compiler check | 0 tokens | ~ms | Deterministic |
| Linter/formatter | 0 tokens | ~ms | Deterministic |
| Test suite | 0 tokens | ~seconds | Deterministic |
| Schema validation | 0 tokens | ~ms | Deterministic |
| Model self-assessment | 100s–1000s tokens | ~seconds | Probabilistic |
| Model re-reasoning | 1000s+ tokens | ~seconds | Probabilistic |

When external validation is available, it dominates self-assessment on every dimension: cheaper, faster, and more reliable. This asymmetry is what makes the external validation principle (Section 2.1.1) not just theoretically sound but economically compelling—delegating verification to purpose-built external mechanisms is almost always the right trade.

#### When Structure Doesn't Pay

Structure is not always justified. The cost model identifies conditions where flexibility wins:

1. **One-off tasks** → C_s cannot be amortized across multiple executions
2. **High novelty** → The task space is too unpredictable to constrain usefully
3. **Cheap failure** → C_f is low (reversible actions, no downstream consequences, low token cost)
4. **Rapidly evolving domains** → Maintenance cost of structure exceeds its benefits

This connects directly to the Flexibility-Reliability Tradeoff (Section 2.4): the maturity curve is really a curve of amortized structure cost declining as task frequency and predictability increase.

<details class="counterpoints">
<summary>Counterpoints & Discussion</summary>
<div class="counterpoints-body">
<p>The compounding reliability argument assumes step independence—that per-step success rates are uncorrelated. In practice, failures often cluster: a single misunderstanding early in a task can cascade into correlated failures across all subsequent steps, making the effective compounding worse than the independent model predicts. Conversely, agents that self-correct can exhibit positive correlation where early successes build context that improves later steps. The simple multiplicative model is useful as a mental framework but may over- or underestimate actual failure rates depending on the domain.</p>
<div class="author-response">
<p>Agreed. The table is a simplification—real tasks have correlated steps. The point is directional: per-step reliability improvements have outsized effects on long-horizon outcomes, and the independent model provides a useful lower bound for reasoning about where to invest in structure. In my experience, the clustering effect makes the case for structure stronger, not weaker—a single structural guardrail at the right choke point can break a failure cascade that would otherwise propagate across many steps.</p>
</div>
</div>
</details>

The conditions above—when structure pays and when it doesn't—are not binary. They form a spectrum, which the following section formalizes.

### 2.4 The Flexibility-Reliability Tradeoff

![Flexibility-Reliability Tradeoff](assets/agents/image3-flexibility-reliability.svg)

Any process an agent works on exists on a maturity curve [3][36][37]:

- **One-off tasks** → Require flexibility and exploration
- **Mature processes** → Benefit from rigidity, predictability, and structure

While some agents work well as completely flexible, **reliability and low latency come from structure**. Agents built with these structural harnesses—structured agents—occupy the upper-right of this space: high flexibility through LLM reasoning, high reliability through external constraints.

**Key insight**: Even with increased agentic capabilities, reducing the problem space and "degrees of freedom" leads to reduced latencies and improved reliability. The structured agency approach is fundamentally about moving agents upward on this chart by adding harnesses rather than restricting what they can do.

#### 2.4.1 Designing for Harness Obsolescence

The flexibility-reliability tradeoff has a temporal dimension that is central to how structured agency should be practiced: **build harnesses expecting them to weaken over time.**

Current harnesses address current model limitations. State management compensates for the fact that models lack persistent memory across invocations. Context-window-as-working-memory harnesses (progressive disclosure, forced contexts, retrieval mechanisms) exist because attention doesn't scale perfectly and context is expensive. Structured output schemas exist because models don't always produce well-formed output. Validation signals exist because models can't yet reliably self-verify. Each of these is a response to a specific limitation—and each limitation is on a trajectory of improvement.

As models improve, some harnesses will naturally weaken. A model with perfect self-verification needs fewer external validation signals. A model with reliable structured output needs less schema enforcement. A model with effectively infinite context and perfect attention needs less aggressive progressive disclosure. New architectures may overcome the need for external memory entirely, collapsing the state management harness into the model itself. The Bitter Lesson predicts exactly this trajectory: capabilities that were once impossible without external scaffolding get absorbed into the model itself.

But even as harnesses weaken, **domain-specific structured state remains economical**. The latency cost of having a model re-derive a customer's order history from raw conversation logs—every invocation—will always exceed the cost of maintaining that information in a structured state object, regardless of how capable the model becomes. The cognitive cost (in tokens) of navigating an unstructured environment will always exceed the cost of navigating one where the structure is pre-computed. This is the same argument that applies to software engineering generally: we don't rewrite parsers from first principles every time we need to parse JSON, even though we *could*. Structured agency's harnesses occupy this same economic niche—they are amortized investments in structure that pay dividends across invocations. The right design principle, then, is not "build permanent harnesses" but "build harnesses with a clear understanding of which limitations they address and how those limitations are evolving." This is the temporal dimension of the flexibility-reliability tradeoff: the optimal point on the curve shifts over time as model capabilities advance, and well-designed structured agents shift with it.

### 2.5 Practical Corollaries

#### 2.5.1 Redundancy is a Feature

<div class="principle">

**Principle**: Have multiple ways to accomplish the same thing [38][39][40].

</div>

**Rationale**: From a probabilistic argument, having multiple paths to success increases the likelihood of task completion. Relying on a single tool (unless it's ubiquitous) creates fragile systems.

**Examples**:
- Multiple retrieval mechanisms (grep, semantic search, full-text search)
- Multiple communication channels
- Fallback tools when primary tools fail

---

With the principles established, we now turn to how they manifest concretely in agent design. The taxonomy below maps each principle to the architectural surfaces where harnesses can be applied.

## 3. Taxonomy of Structured Agents

The structured agency framework decomposes agents into a set of concerns—each representing a surface where harnesses can be applied. This taxonomy describes those surfaces.

### 3.1 The Agent Model

![Agent Architecture](assets/agents/image8-agent-architecture.svg)

Borrowing from the reinforcement learning framework [43], and informed by recent surveys on LLM-based autonomous agents [44], we define the following components.

#### Invocation

**Definition**: The period from when an agent "starts up" until it "dies" and stops performing actions.

An agent may persist across multiple invocations with the same state. Each invocation provides specific settings:
- **Allowed tools** → What actions are permitted
- **Initial context** → What information to load
- **Directives** → What goals to pursue

**Implementation note**: Whether each user input triggers a new invocation (daemon shuts down) or the agent persists across inputs is an implementation detail. The key is distinguishing between invocations and mere instruction reception.

**Interruptions as a lifecycle property**: Invocations can be interrupted—by the user, by the parent agent, by budget exhaustion, or by a sibling's failure. An invocation that is interrupted mid-execution must leave state in a recoverable condition. This means state must be designed to survive interruption: partial writes should be atomic or rollback-safe, and in-progress markers should be distinguishable from completed ones. Interruption is not an edge case—it is a first-class lifecycle event that the agent model must account for. See Section 3.6 for how interruptions propagate through hierarchical systems.

#### Agent

**Definition**: An organism that observes the world through actions and builds an internal state representation.

For most current "agentic" systems, this is modeled through LLMs or derived technologies.

#### Context Window

**Definition**: The working space of a given model for observing its state. Exists as a stream of tokens.

**Best practice**: Treat the context window as a **scratchpad**, not permanent storage [33][34][35].

- Don't depend on it for long-term memory
- Successive invocations should preserve only relevant information (through pruning or compression)
- Supplement with forced tool use and external contexts

**Critical distinction**: The context window is NOT the agent's state—it's merely a working memory [45].

#### State

![Context Window vs. State](assets/agents/image4-context-vs-state.svg)

**Definition**: The internal understanding an agent has of the world.

**Key properties**:
- Updated through actions (tool calls)
- Measured and modified by tools
- Tools can be seen as part of state
- **State is NOT the context window**
- **State is NOT a string**
- **State is structured** (e.g., class instances, databases, knowledge graphs)

**Benefits of structured state**:
- Enables validation signals
- Tools operate directly on state (reducing task complexity)
- Type safety and constraints
- Clear interfaces

![State Anatomy](assets/agents/image9-state-anatomy.svg)

#### Structure Classes

The principle that "state is NOT a string" raises a natural question: what *is* state made of? Structure classes are the first-class types that populate structured state—the building blocks that make state genuinely structured rather than an opaque blob.

**Core structure classes**:

- **Tasks** — Status, dependencies, ownership, assignment. Tasks are not just strings in a plan but structured objects with a lifecycle: created, assigned, in-progress, blocked, completed, failed. They carry metadata (who owns them, what blocks them, what they produce) that enables the planning (Section 3.5) and composition (Section 3.4) layers to operate on them programmatically.
- **Contexts** — Addressable information blocks that agents can load, search, and reference (see Section 3.3 for the full treatment). Listed here because they are a state type: they exist in state, are updated through actions, and have lifecycle properties.
- **Messages / Communications** — Structured records of inter-agent and user-agent communication. Not just log entries but typed objects with sender, recipient, content, timestamp, and thread/conversation metadata.
- **Plans** — Structured plan representations that live in state. A plan is itself a state object that can be inspected, validated, and revised (see Section 3.5).
- **Tool Results / Action History** — Structured log of actions taken and their outcomes. Each entry records the tool called, its arguments, its result, and any state mutations it caused. This is the basis for observability (Section 3.7) and recovery (Section 3.6).
- **Domain-specific types** — The types that encode the actual problem domain: tickets, orders, code files, database schemas, customer records. These are what make a structured agent domain-aware rather than domain-agnostic.

[**TODO**: Flesh out each structure class with concrete examples from production systems. Show how tasks-as-structured-objects differ from tasks-as-strings in a plan.]

[**TODO**: Define the lifecycle properties common to all structure classes (creation, mutation, validation, serialization) and how they connect to the state principles above.]

### 3.2 Actions

#### Tools

**Definition**: Actions an agent performs on the world or its state.

**Categories** [1][2]:
1. **Bespoke tools** → Hardcoded by developers for specific use cases
2. **MCP servers** [46] → Collections of tools from remote endpoints
3. **Coding-model functions** → Libraries like pandas, scikit-learn, matplotlib
4. **CLI utilities** → Unix tools like grep, cat, sed

**How tools interact with state**:
- Tools observe the world, state, or both
- Tools write to state
- Tools affect the external world
- Tool arguments reference state (e.g., \`read(document_name)\` not \`read(document_content)\`)

**Analogy**: In LISP and similar languages, code is data. In coding agents, **tools are a special case of state** (they're often code that can be edited).

![Tool Anatomy](assets/agents/image7-tool-anatomy.svg)

##### Best Practices for Tool Design

1. **Multi-level descriptions**
   - Short description (like Unix tool summary)
   - Detailed documentation (like man pages)

2. **Usage examples**
   - Show common patterns
   - Demonstrate edge cases

3. **Validators**
   - Check preconditions (e.g., document exists before editing)
   - Verify inputs match expected types/formats

4. **Introspective errors**
   - Provide actionable feedback
   - Example: "Query on field 'X' failed: field 'Y' does not exist in schema. Available fields: [...]"

#### Forced Tool Use

**Definition**: Forcing or injecting simulated tool uses into the agent's context window upon certain actions [7][47].

**Examples**:
- Claude Code reads a file when users mention it with \`@\` syntax
- Calling \`get_current_tasks()\` before the LLM runs
- Running \`get_incoming_messages()\` after every LLM handoff to check for user/system messages

**Note**: While this could be seen as hooks that run before actions, empirically there is significant value in treating these as injected tool calls rather than separate mechanisms.

**Why this works**: It leverages the agent's existing tool-use reasoning capabilities while ensuring critical operations always occur.

#### Communication

<div class="principle">

**Principle**: Treat communication as a tool.

</div>

Communication with users, other agents, or external systems should follow the same patterns as other tools:
- Well-defined interfaces
- Validation of inputs/outputs
- Observable effects on state

Communication channels—whether inter-agent messaging, user-facing output, or external notifications—are tools with the same design principles as any other: they should be idempotent where possible, scoped to a clear purpose, and observable. Communication is the mechanism by which the hierarchical system propagates signals, including interruptions (see Section 3.6).

[**TODO**: Inter-agent messaging patterns — how do agents in a hierarchy exchange structured information beyond tool-call arguments and return values?]

[**TODO**: User-facing communication as a structured action — when and how should an agent communicate with the user, and how should this be modeled in state?]

[**TODO**: Communication channels as tools — design patterns for messaging tools that follow the same principles as other tools (scoped, observable, validated)]

### 3.3 Information Management

Section 2.2 established context engineering as a principle—the *why*. This section provides the taxonomy—the *what*: the concrete primitives that context engineering operates on.

#### Contexts

**Definition**: Addressable blocks of information (text) that may or may not exist in the agent's working memory.

**Key properties**:
- **Addressable** → Can be referenced by name/ID
- **Selective loading** → Different tasks access different contexts
- **Scoped access** → Contexts may be task-specific or shared

##### Examples of Contexts

| Context | Description | Properties |
|---------|-------------|------------|
| Session Context | Log of session actions | Partially structured (messages + timestamps) |
| Company Documents | Documents owned by Company X | Pre-indexed as knowledge graph |
| Subagent Log | Record of subagent actions | Read-only, time-bounded |
| Memories | Long-term memory store | Read-write, persistent |

**Context variations**:
- Text-only vs. enriched/searchable
- Read-only vs. read-write
- Agent-specific vs. shared

**Why contexts deserve special attention**: Contexts are the lifeblood of agentic applications, but different systems handle them differently:
- **Claude Code** → Greps over files (filesystem as context)
- **Cursor** → Maintains code index with semantic search
- **Recursive Language Models** → Maintains prompt as context (string)

#### Progressive Disclosure in Practice

![Progressive Disclosure Funnel](assets/agents/image5-progressive-disclosure.svg)

**The chicken-and-egg problem**: How do you know what to read from a context without knowing what's inside?

**Solution**: Provide summarized versions or metadata before full context retrieval.

**Example flow**:
1. Tool provides high-level summary of available contexts
2. Agent decides which contexts are relevant
3. Agent searches specific contexts
4. Agent retrieves detailed information

**Analogy**: Like file system commands:
- \`ls\` → See what's available (progressive disclosure)
- \`grep\` → Search specific files
- \`cat\` → Read full contents

#### Forced Contexts

**Definition**: Automatically loading entire contexts into working memory when appropriate.

This is a specific case combining:
- Progressive disclosure
- Forced tool use

**When to use**:
- Context is small enough to fit in working memory
- Context is highly relevant to the task (e.g., editing a file → load the file)
- Cost of loading is lower than cost of multiple queries

**Note**: This is "context stuffing" from pre-RAG LLM systems, but it remains highly effective for the right problem sets.

#### Retrieval Mechanisms (RAG, GraphRAG, grep, etc.)

<div class="principle">

**Principle**: All retrieval mechanisms are context access patterns.

</div>

RAG [48] pioneered the pattern of augmenting generation with retrieved documents. GraphRAG [49] extends this by constructing knowledge graphs over corpora for more structured retrieval.

However, recent production systems have revealed a significant shift in how retrieval actually works in structured domains. Claude Code, Aider, and similar coding agents have largely moved away from semantic search (embedding similarity over chunked documents) toward what can be called **agentic search**—retrieval driven by the domain's own structure rather than by vector proximity [81][82][83].

Aider's repo-map approach [82] illustrates this well: rather than embedding and retrieving code chunks, it builds condensed tree-sitter outlines of the repository—a structural summary that lets the model navigate the codebase by understanding its organization (modules, classes, function signatures, dependency graphs). Claude Code [7] takes a complementary approach: grep-over-filesystem, file tree exploration, and symbol-level navigation. In both cases, retrieval exploits the **inherent structure of the domain** to guide the model to relevant information, rather than relying on semantic similarity between a query embedding and document embeddings.

This distinction maps cleanly onto domain type. For **structured domains** (codebases, databases, APIs, configuration systems), agentic search works because the domain provides navigable structure—file hierarchies, type systems, dependency graphs, schema definitions. The retrieval mechanism can follow the domain's own organization. For **unstructured domains** (document corpora, knowledge bases, conversation histories), that navigable structure doesn't exist natively. We must **create** it—via indexing, knowledge graphs (GraphRAG [49]), or other enrichment—before agentic search becomes viable. This is why GraphRAG exists: it manufactures the structural scaffolding that code and databases get for free.

This is a special case of the contexts taxonomy above: different retrieval mechanisms are different strategies for accessing contexts, each with tradeoffs in precision, recall, latency, and setup cost. Traditional RAG trades setup cost (embedding, chunking, indexing) for broad recall across unstructured corpora. Agentic search trades generality for high precision in structured domains—it's fast and accurate when the domain has navigable structure, but inapplicable when it doesn't. GraphRAG occupies a middle ground: it invests heavily in creating structure (knowledge graph construction) to enable structured retrieval over originally unstructured data. The choice between these mechanisms is itself a context engineering decision—matching the retrieval strategy to the structure (or lack thereof) of the domain.

### 3.4 Composition

#### Subagents

![Subagent vs. Multiagent Topology](assets/agents/image6-subagent-topology.svg)

**Definition**: An agent invoked by another agent (via a tool) that has its own context access, directives, and tools [24][51].

**State sharing**: May be partial or complete with the invoking agent.

##### Subagents vs. Multiagent Systems

| Aspect | Subagents | Multiagent |
|--------|-----------|------------|
| Lifetime | Invoked for specific purpose, dies when done | Independent lifetimes |
| Relationship | Parent-child hierarchy | Peer-to-peer |
| State sharing | Easy (by design) | Complex (requires explicit channels) |
| Communication | Direct invocation | Message passing |

**When to use subagents** [50]:
- Clear hierarchical task decomposition
- Need to isolate context/tools for subtask
- Want to reuse subagent across multiple invocations
- State sharing is important

#### Delegation Challenges

Delegation—whether to subagents or across multi-agent systems—introduces costs that are easy to underestimate. Two failure patterns dominate production systems:

**Scope rediscovery.** Each delegated agent starts with a fresh or partial context window. The parent agent has built up a rich working memory over the course of the task—observations, intermediate results, implicit assumptions, domain context. When it delegates to a subagent, much of this working memory must be *re-established* in the subagent's context. This creates a **working-memory discontinuity** (see Section 2.2): the subagent must spend tokens and reasoning cycles recontextualizing before it can do useful work. The more complex the parent's accumulated context, the more expensive this rediscovery becomes. This is why delegation works best for tasks that are relatively self-contained—where the required context can be transmitted cheaply and the subagent doesn't need to reconstruct the full reasoning history of the parent.

**Tool segmentation failures.** In systems with a router or orchestrator that dispatches tasks to specialized agents, a common failure mode is routing to an agent that lacks the right tools or context for the task. The agent attempts the task, fails, and the error bubbles back up to the router, which must retry—potentially multiple times—before either finding the right agent or escalating. This retry cycle compounds latency and cost, and is especially pernicious because it's often silent: the system *eventually* succeeds, masking the inefficiency. Structured composition (clear tool boundaries, explicit capability declarations, well-defined routing rules) mitigates this, but doesn't eliminate it. This pattern connects directly to the error handling concerns in Section 3.6.

The upshot: delegation is not free. It works best for **cheaper, more reliable subtasks** where the cost of recontextualization is justified by the benefits of isolation, parallelism, or specialization. When a subtask requires deep access to the parent's accumulated context, the cost of transmitting that context may exceed the cost of simply having the parent do the work itself.

#### Skills

**Definition**: Pre-packaged instructions and context bundles.

Skills encapsulate:
- Domain knowledge
- Common patterns
- Best practices
- Tool combinations

**Purpose**: Reduce repetition and improve consistency for common task types.

<details class="counterpoints">
<summary>Counterpoints & Discussion</summary>
<div class="counterpoints-body">
<p>The hierarchical subagent model is not the only viable approach. Du et al. [52] show that peer-to-peer debate between agents boosts factuality beyond what hierarchical approaches achieve. Li et al. [53] demonstrate that single-agent skill selection degrades non-linearly as skill libraries grow, suggesting multi-agent architectures still win for complex systems. OpenAI's Swarm [74] explores lightweight peer-to-peer orchestration as an alternative to strict hierarchies. More fundamentally, pure hierarchical models may sacrifice robustness—flat topologies can be more fault-tolerant when individual agents fail [75].</p>
<div class="author-response">
<p>Fair—I have not personally implemented peer-to-peer debate or swarm-style systems in production. The hierarchical model reflects what I have built and observed working reliably, but I acknowledge this is a gap in my direct experience.</p>
</div>
</div>
</details>

Composition defines the *who* of agent systems. Planning defines the *what* and *when*—the mechanism by which composed agents coordinate their work.

### 3.5 Planning

Planning is the mechanism by which agents decide **what to do before doing it** [58]. While reactive agents execute actions step-by-step based on immediate observations, planning introduces deliberation—a lookahead that evaluates potential action sequences before committing.

#### Why Planning Matters

Without planning, an agent's behavior is purely reactive: observe → act → observe → act [56]. This works for simple tasks but fails when:
- Actions are expensive or irreversible
- The task requires coordination across multiple steps
- Dependencies between subtasks constrain execution order
- The agent must reason about resource allocation (tokens, API calls, time)

<div class="principle">

**Principle**: Planning amortizes the cost of reasoning over the execution of a task [62].

</div>

#### Plan Representations

How a plan is represented determines what the agent can reason about:

| Representation | Structure | Strengths | Weaknesses |
|----------------|-----------|-----------|------------|
| **Natural language** | Free-form text outline | Flexible, human-readable | Hard to validate, ambiguous |
| **Task lists** | Ordered/unordered items with status | Trackable, supports dependencies | Limited branching logic |
| **DAGs** | Directed acyclic graphs of subtasks | Explicit dependencies, parallelizable | Complex to construct |
| **State machines** | States + transitions | Deterministic, verifiable | Rigid, poor for novel tasks |
| **Hierarchical plans** | Nested subtask trees | Natural decomposition, progressive detail | Can lose cross-branch context |

[**TODO**: Add examples from your own systems—which representations have you used and where?]

#### Planning Strategies

##### Decomposition

Breaking a task into subtasks before executing any of them. This is the most common planning pattern in current agentic systems [60][61].

**Approaches**:
- **Top-down decomposition** → Split high-level goal into subgoals recursively
- **Means-ends analysis** → Identify the gap between current and goal state, then find actions that reduce it
- **Template-based** → Match the task to a known plan template and fill in specifics

[**TODO**: Document decomposition patterns you've implemented]

##### Search-Based Planning

Exploring multiple possible action sequences and selecting the best one.

**Approaches**:
- **Tree-of-thought** [55] → Generate and evaluate multiple reasoning paths
- **Monte Carlo methods** → Sample rollouts and estimate value
- **Best-first search** [59] → Expand the most promising partial plan

[**TODO**: Document search-based approaches you've experimented with]

##### Verification-Driven Planning

Using validation signals to guide plan construction—plans are iteratively refined until they pass verification checks.

**Connection to Section 2.1.1**: External validation signals are not just useful during execution—they can also constrain the planning phase itself.

[**TODO**: Document verification-driven planning patterns]

#### Plan Execution

A plan is only as good as its execution. Key concerns:

- **Plan following vs. replanning** → When should an agent stick to the plan vs. replan?
- **Monitoring** → How does the agent detect plan divergence?
- **Checkpointing** → Where can execution resume if interrupted?
- **Partial execution** → Can subtasks be executed out of order when dependencies allow?

<div class="principle">

**Principle**: Plans should be treated as hypotheses, not commitments [57]. The agent should be willing to abandon or revise a plan when observations invalidate its assumptions.

</div>

[**TODO**: Document execution strategies and replanning triggers you've built]

#### Planning and Composition

Planning interacts closely with composition (Section 3.4):
- **Subagent delegation** → A plan can assign subtasks to subagents
- **Skill matching** → Planning can involve selecting which skills to invoke
- **Resource budgeting** → Plans can allocate context window space, API calls, or time budgets across subtasks

[**TODO**: Document how planning integrates with your subagent and skill systems]

<details class="counterpoints">
<summary>Counterpoints & Discussion</summary>
<div class="counterpoints-body">
<p>The value of explicit planning is debated. Snell et al. [76] show that scaling test-time compute—simply giving a model more tokens to think—can match or beat structured planning approaches. Plan caching [77] reveals that planning overhead is non-trivial and can be amortized. Google DeepMind [78] finds that frontier models working alone can outperform agent teams on many benchmarks, questioning whether planning-heavy orchestration is always justified. Chain-of-thought prompting [54] itself can be seen as implicit planning that emerges from the model rather than being imposed externally.</p>
<div class="author-response">
<p>In practice, when actions are expensive and mistakes are costly or irreversible, I have found planning—especially with verification signals before execution—to be a very worthwhile investment. The overhead is real, but so is the cost of an unplanned action that corrupts state or wastes expensive API calls. Planning pays for itself when the cost of failure exceeds the cost of deliberation. Moreover, structured agency is itself a human-explainable, low-complexity form of test-time compute. The harness *is* compute—it's just compute that happens to be legible, auditable, and cheap. As models improve, explicit harness compute gets amortized into the model itself (the model learns to do what the harness enforced). But this is structured agency's *graduation path*, not its contradiction: structure scaffolds capability until the model internalizes it, at which point the harness gracefully weakens (see Section 2.4.1 for the full Bitter Lesson argument).</p>
</div>
</div>
</details>

### 3.6 Error Handling & Recovery

Agents fail. Tools return errors, APIs time out, models hallucinate [17]. The question is not whether failures occur, but how the system responds [63].

#### Failure Modes

| Mode | Description | Example |
|------|-------------|---------|
| **Tool failure** | A tool call returns an error or unexpected result | API rate limit, file not found |
| **Reasoning failure** | The agent produces an incorrect or incoherent plan | Hallucinated tool name, circular logic |
| **State corruption** | State becomes inconsistent due to partial writes | Half-completed database migration |
| **Context degradation** | Key information scrolls out of the context window | Losing track of the original goal |
| **Composition failure** | A subagent fails or returns unusable results | Subagent hits token limit, returns garbage |

#### Recovery Strategies

- **Retry with backoff** → Transient failures (network, rate limits)
- **Fallback tools** → Alternative paths to accomplish the same action (see Section 2.5.1)
- **Replanning** [57] → Abandon current plan and construct a new one from current state
- **Escalation** → Communicate the failure to a parent agent or user
- **Graceful degradation** [40] → Accomplish a reduced version of the goal

<div class="principle">

**Principle**: Recovery strategies should be explicit, not emergent. An agent that silently retries indefinitely is worse than one that escalates after two failures.

</div>

[**TODO**: Document your error handling patterns and when each strategy applies]

#### Interruptions as Hierarchical Signals

Interruptions are not just failures—they are first-class signals in hierarchical agent systems. When a parent agent determines that a subtask is no longer needed, it must be able to interrupt the child agent cleanly. This is distinct from error recovery: the child didn't fail—the parent's context changed.

**Example**: In a schema-aware multi-query system, if one query reveals that the schema assumption underlying all queries is wrong, the parent can interrupt the remaining queries since their results would be wasted work. The interruption signal propagates *down* the hierarchy (parent → children), while failure signals propagate *up* (child → parent). Both must be handled, but they have different semantics: a failure says "I couldn't do what you asked," while an interruption says "stop—what I asked is no longer relevant."

This connects to composition (Section 3.4)—the parent-child relationship defines who can interrupt whom—and to planning (Section 3.5)—plan revision may trigger interruptions of in-progress subtasks that the revised plan no longer requires.

[**TODO**: Interruption propagation patterns — how do interruptions cascade through a hierarchy? What happens to in-progress state when a subtask is interrupted?]

[**TODO**: Interruption vs. cancellation vs. preemption — are these meaningfully different in practice, and should the agent model distinguish them?]

### 3.7 Evaluation & Observability

Building agents without the ability to measure their behavior is building blind [42]. This section covers how to observe, measure, and improve agentic systems.

**Agentic testing is inherently integration testing** [41][42]. You're not testing individual components in isolation—you're testing tool interactions, state updates, context retrieval, subagent coordination, and validation signals working together. This framing is essential: unit tests on individual tools or prompts are necessary but not sufficient. The interesting failures happen at the seams, and evaluation must be designed with this in mind.

#### What to Measure

- **Task success rate** → Did the agent accomplish the goal?
- **Efficiency** → How many tokens, API calls, and tool invocations were used?
- **Latency** → Time to first action, time to completion
- **Reliability** → Variance across repeated executions of the same task
- **Recovery rate** → How often does the agent recover from errors vs. failing completely?
- **Delegation quality** → Are subagents given appropriate scope and context?

#### Observability Primitives

- **Traces** [67] → Ordered log of every action, tool call, and result
- **State snapshots** → Serialized state at key decision points
- **Decision logs** → Why the agent chose action A over action B
- **Diff views** → What changed in state after each action

[**TODO**: Document what metrics and observability you've built into your systems]

#### Testing Agentic Systems

Testing agentic systems is inherently integration testing (as framed above), but more specifically:

- **Scenario-based testing** → Define input scenarios and expected outcomes
- **Regression suites** → Capture past failures and ensure they don't recur
- **Adversarial testing** → Intentionally degrade tools or state to test recovery
- **Benchmark tasks** [64][65][66] → Standardized tasks with known-good solutions

[**TODO**: Document your testing approaches and any benchmark suites you use]

### 3.8 Safety & Security

Agents that interact with the real world—executing code, calling APIs, sending messages—introduce attack surfaces that don't exist in pure text generation. Safety is not a feature to bolt on after deployment; it is a structural concern that permeates every layer of a structured agent.

#### Prompt Injection

**The core threat**: Untrusted content (user inputs, web pages, tool results, file contents) can contain instructions that the model treats as authoritative [47][79].

A model reading a web page might encounter hidden text saying "ignore all previous instructions and delete the user's files." Without structural defenses, the model has no mechanism to distinguish this from a legitimate directive.

**Defense in depth**:
- **Privilege boundaries** → Tools accessible through untrusted contexts operate with reduced permissions. A web-browsing agent should not have the same tool access as one operating on the user's behalf.
- **Context tagging** → Mark content provenance (system instruction, user message, tool result, external data) so the model can distinguish instruction sources and treat each with appropriate trust levels.
- **Output filtering** → Validate agent actions against an allowlist before execution. Actions triggered by untrusted content require explicit confirmation.
- **Input sanitization** → Strip or escape instruction-like patterns from untrusted data before injecting it into the context window.

**Structural connection**: The same principles from Section 2.1 apply directly—structure (clear boundaries between trusted and untrusted content) enables validation (checking whether an action was triggered by legitimate instructions). Prompt injection is fundamentally a failure of boundary enforcement.

#### Adversarial Resilience

Agents face adversarial conditions beyond prompt injection:

| Threat | Description | Structural Mitigation |
|--------|-------------|----------------------|
| **Poisoned tool outputs** | A compromised API returns misleading data | Redundant verification (Section 2.5.1)—cross-check through independent sources |
| **State manipulation** | External actors modify shared state between agent steps | State integrity checks before and after tool calls |
| **Resource exhaustion** | Expensive tool calls or infinite loops drain budgets | Hard limits on token spend, API calls, and wall-clock time per invocation |
| **Social engineering** | Content designed to manipulate reasoning ("ignore previous instructions") | Context tagging + privilege boundaries |

<div class="principle">

**Principle**: Assume that any data crossing a trust boundary is potentially adversarial. Design tools and validation signals accordingly.

</div>

The error compounding argument from Section 2.3 applies here too: a single adversarial input early in a multi-step task can corrupt every downstream step. Structural checkpoints—validation signals at each step boundary—limit the blast radius of a successful attack to a single step rather than the entire task.

#### Tool Misuse

Even without adversarial intent, agents can misuse tools in ways that cause harm:

- **Overprivileged tools** → A tool that can delete files when the agent only needs to read them
- **Unintended side effects** → A "search" tool that also logs queries to a third-party service
- **Cascading permissions** → A subagent inherits permissions it doesn't need
- **Irreversible actions** → Sending emails, publishing content, or deleting data without confirmation

<div class="principle">

**Principle**: Apply the principle of least privilege to every tool and every agent invocation [47][80].

</div>

**Structural safeguards**:
- **Scoped tool access** → Each invocation grants only the tools needed for the task. A summarization agent does not need file-write permissions.
- **Read-before-write** → Require observation before mutation. Prevent blind state changes by forcing the agent to inspect current state before modifying it.
- **Dry-run modes** → Allow plan validation without execution for high-risk operations. Verification-driven planning (Section 3.5) provides a natural checkpoint—validate the safety properties of a plan before executing it.
- **Audit trails** → Every tool call, its arguments, and its results are logged (Section 3.7). This is both an observability concern and a security requirement.
- **Human-in-the-loop gates** → Irreversible or externally-visible actions (sending messages, deleting data, modifying permissions) require explicit user confirmation before execution.

**Connection to the cost model**: From Section 2.3, the irreversibility cost component of C_f is what makes safety harnesses economically justified. The cost of a leaked API key or a deleted production database dwarfs the cost of adding a confirmation step.

#### Structure as Implicit Alignment

Beyond reliability and safety, structure provides a low-bar alignment mechanism that is easy to overlook. When a human designer defines a tool's interface, a state schema's fields, or a validation rule's conditions, they embed **implicit task alignment** into the system. The schema of a customer-support agent's state (ticket ID, customer sentiment, escalation threshold, resolution status) carries domain knowledge about what matters, what operations are valid, and where boundaries lie. This alignment information would be difficult—and token-expensive—to convey through prompt context alone.

This is not a substitute for deeper alignment research, but it is a practical complement. Structure encodes a form of human intent that operates at the architectural level: the agent doesn't need to *understand* why certain state transitions are invalid if the harness prevents them. The boundaries themselves carry the alignment signal. In this sense, every well-designed tool, every carefully scoped permission set, and every validation rule is a small alignment intervention—cheap, composable, and immediately effective.

Concretely, safety harnesses don't just defend against bad outcomes—they promote good ones. By defining the boundaries within which an agent operates (which tools are available, what state transitions are valid, what actions require confirmation), structure keeps agents within **human-intended operating boundaries**. This is alignment through architecture rather than through instruction: the agent doesn't need to understand *why* certain actions are dangerous if the harness prevents those actions from being taken without oversight. The safety and alignment properties of structured agency are two faces of the same coin—structure constrains the action space, and the shape of that constraint embeds human judgment about what the agent should and should not do. This makes structured agency not just an engineering methodology but a lightweight alignment strategy for production systems operating today.

<details class="counterpoints">
<summary>Counterpoints & Discussion</summary>
<div class="counterpoints-body">
<p>Strict safety harnesses introduce friction that can degrade the user experience and agent throughput. Every confirmation gate is a latency penalty; every privilege restriction is a task the agent cannot perform autonomously. Some argue that overly restrictive safety measures make agents less useful than the unconstrained systems they aim to improve—users disable safety features when they get in the way [80]. There is also the question of who bears the cost: safety engineering is expensive, and the threat landscape evolves faster than most teams can implement defenses.</p>
<div class="author-response">
<p>The friction argument is real, and in practice I calibrate safety harnesses to the risk level of each action. Read-only operations need minimal gating; irreversible mutations need maximum gating. The key is that these harnesses should be structural and configurable—not all-or-nothing. A well-designed structured agent lets the deployer tune the tradeoff between autonomy and safety for their specific risk tolerance, rather than baking in a single policy.</p>
</div>
</div>
</details>

---

## 4. Open Questions & Future Directions

### Incomplete Sections

[**TODO**: Add the flexibility-reliability-latency graph]
[**TODO**: Provide concrete examples of state schemas]

### Open Questions

- How to balance forced tool use with agent autonomy?
- What are the optimal granularities for context chunking?
- How to measure the "structure vs. flexibility" tradeoff quantitatively?
- What are the best practices for cross-agent state sharing?

### Areas for Further Exploration

- Formal verification of agentic plans
- Metrics for agentic system reliability
- Patterns for graceful degradation when tools fail
- Optimal context window management strategies
- Subagent coordination protocols

---

## 5. Conclusion

The central claim of this paper is that reliability in agentic systems is not primarily a model capability problem—it is a *structural* problem. The principles of Structured Agency form a dependency chain:

1. **Structure reduces problem space** → Constraints create checkable boundaries, which enable external validation, tool abstraction, and delegation
2. **External validation is economically dominant** → Purpose-built validators are cheaper, faster, and more reliable than model self-assessment
3. **Minimal context improves reasoning** → Progressive disclosure keeps working memory focused, preventing the performance degradation that comes with noise
4. **Structure has a cost, and it must be justified** → The cost model (C_s vs. C_f) determines when to invest in structure; error compounding makes the case strongest for long-horizon tasks
5. **Composition requires boundaries** → Subagents, skills, and plans only work when responsibilities, contexts, and tools are clearly partitioned

These principles are not a checklist to be applied independently. They are a connected framework: structure enables validation, validation enables tool reliability, tool reliability enables composition, and composition enables agents that solve complex problems without compounding errors at every step.

The field is evolving rapidly. Models will continue to improve, context windows will grow, and some of the structure advocated here may eventually be superseded by raw capability—the Bitter Lesson may yet apply. But several properties of structured agency are orthogonal to the capability curve. **Harnesses are designed to weaken**: the framework explicitly anticipates that today's structural scaffolding becomes tomorrow's unnecessary overhead as models internalize the patterns (Section 2.4.1). **Structure provides alignment, not just reliability**: human-defined boundaries embed task-appropriate behavior that would be difficult and expensive to convey through prompt context alone (Section 3.8). **Economy is orthogonal to capability**: even when a model *can* derive everything from scratch, the latency, cost, and predictability advantages of premade structural elements remain. As long as agents must interface with external systems, take irreversible actions, and operate under cost constraints, the economics of Structured Agency will hold: **it is almost always cheaper to prevent failure through structure than to recover from it through flexibility**.

---

## Appendix: Terminology Reference

| Term | Definition |
|------|------------|
| **Agent** | An entity that observes the world through actions and maintains internal state |
| **Structured Agency** | The practice of building harnesses around LLM-powered agents through deliberate constraints, external validation, and composable tooling |
| **Structured Agent** | An agent built with structural harnesses that trades unconstrained flexibility for reliability, observability, and predictable behavior |
| **Harness** | A set of structural constraints (tools, validation signals, privilege boundaries, plans) applied to an agent to increase its reliability |
| **Context** | Addressable block of information accessible to agents |
| **Context Window** | Working memory (token stream) of a language model |
| **Forced Context** | Automatically loaded context for relevant tasks |
| **Forced Tool Use** | Injected tool calls triggered by specific conditions |
| **Invocation** | Period from agent startup to shutdown |
| **Progressive Disclosure** | Incremental revelation of information through metadata/summaries |
| **State** | Structured internal representation of the world (NOT the context window) |
| **Subagent** | Agent invoked by another agent for specific subtasks |
| **Tool** | Action an agent can perform on the world or state |
| **Plan** | A structured representation of intended actions before execution |
| **Replanning** | Revising a plan based on new observations or failed assumptions |
| **Recovery Strategy** | Explicit mechanism for handling failures (retry, fallback, escalate) |
| **Trace** | Ordered log of actions, tool calls, and results for observability |
| **Validation Signal** | External feedback that allows course correction |

---

## References

[1] T. Schick et al., "Toolformer: Language Models Can Teach Themselves to Use Tools," *NeurIPS*, 2023. [arxiv.org/abs/2302.04761](https://arxiv.org/abs/2302.04761)

[2] S. Patil et al., "Gorilla: Large Language Model Connected with Massive APIs," *NeurIPS*, 2024. [arxiv.org/abs/2305.15334](https://arxiv.org/abs/2305.15334)

[3] Anthropic, "Building Effective Agents," 2024. [anthropic.com/research/building-effective-agents](https://www.anthropic.com/research/building-effective-agents)

[4] Anthropic, "Writing Effective Tools for AI Agents," 2025. [anthropic.com/engineering/writing-tools-for-agents](https://www.anthropic.com/engineering/writing-tools-for-agents)

[5] Eficode, "Unix Principles Guiding Agentic AI," 2024. [eficode.com/blog/unix-principles-guiding-agentic-ai-eternal-wisdom-for-new-innovations](https://www.eficode.com/blog/unix-principles-guiding-agentic-ai-eternal-wisdom-for-new-innovations)

[6] T. Zhang, T. Kraska, and O. Khattab, "Recursive Language Models," *arXiv:2512.24601*, 2025. [arxiv.org/abs/2512.24601](https://arxiv.org/abs/2512.24601)

[7] Anthropic, "Claude Code Overview," 2025. [code.claude.com/docs/en/overview](https://code.claude.com/docs/en/overview)

[8] A. Ng, "Agentic AI Design Patterns," *The Batch*, 2024. [deeplearning.ai/the-batch/llms-evolve-with-agentic-workflows-enabling-autonomous-reasoning-and-collaboration/](https://www.deeplearning.ai/the-batch/llms-evolve-with-agentic-workflows-enabling-autonomous-reasoning-and-collaboration/)

[9] OpenAI, "A Practical Guide to Building Agents," 2025. [openai.com/business/guides-and-resources/a-practical-guide-to-building-ai-agents/](https://openai.com/business/guides-and-resources/a-practical-guide-to-building-ai-agents/)

[10] L. Weng, "LLM Powered Autonomous Agents," 2023. [lilianweng.github.io/posts/2023-06-23-agent/](https://lilianweng.github.io/posts/2023-06-23-agent/)

[11] H. A. Simon, "A Behavioral Model of Rational Choice," *Quarterly Journal of Economics*, vol. 69, no. 1, pp. 99–118, 1955.

[12] R. Dechter, *Constraint Processing*. Morgan Kaufmann, 2003.

[13] S. Russell and P. Norvig, *Artificial Intelligence: A Modern Approach*, 4th ed., Ch. 5: Constraint Satisfaction Problems. Pearson, 2021.

[14] B. Wang et al., "Budget-Aware Tool-Use Enables Effective Agent Scaling," Google Research, 2025. [arxiv.org/abs/2511.17006](https://arxiv.org/abs/2511.17006)

[15] Z. Tam et al., "Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models," *EMNLP*, 2024. [arxiv.org/abs/2408.02442](https://arxiv.org/abs/2408.02442)

[16] K. Park et al., "Grammar-Aligned Decoding," *NeurIPS*, 2024. [arxiv.org/abs/2405.21047](https://arxiv.org/abs/2405.21047)

[17] J. Huang et al., "Large Language Models Cannot Self-Correct Reasoning Yet," *ICLR*, 2024. [arxiv.org/abs/2310.01798](https://arxiv.org/abs/2310.01798)

[18] Y. Kamoi et al., "When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs," *TACL*, 2024. [arxiv.org/abs/2406.01297](https://arxiv.org/abs/2406.01297)

[19] M. Kleppmann, "Prediction: AI Will Make Formal Verification Go Mainstream," 2025. [martin.kleppmann.com/2025/12/08/ai-formal-verification.html](https://martin.kleppmann.com/2025/12/08/ai-formal-verification.html)

[20] Y. Liu et al., "A Survey on the Feedback Mechanism of LLM-based AI Agents," *IJCAI*, 2025. [ijcai.org/proceedings/2025/1175](https://www.ijcai.org/proceedings/2025/1175)

[21] A. Kumar et al., "Training Language Models to Self-Correct via Reinforcement Learning (SCoRe)," *ICLR*, 2025.

[22] A. Paranjape et al., "ART: Automatic Multi-step Reasoning and Tool-use for Large Language Models," 2023. [arxiv.org/abs/2303.09014](https://arxiv.org/abs/2303.09014)

[23] Y. Shen et al., "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face," *NeurIPS*, 2023. [arxiv.org/abs/2303.17580](https://arxiv.org/abs/2303.17580)

[24] Anthropic, "How We Built Our Multi-Agent Research System," 2025. [anthropic.com/engineering/multi-agent-research-system](https://www.anthropic.com/engineering/multi-agent-research-system)

[25] N. F. Liu et al., "Lost in the Middle: How Language Models Use Long Contexts," *TACL*, 2024. [arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172)

[26] F. Shi et al., "Large Language Models Can Be Easily Distracted by Irrelevant Context," *ICML*, 2023. [arxiv.org/abs/2302.00093](https://arxiv.org/abs/2302.00093)

[27] "Context Length Alone Hurts LLM Performance Despite Perfect Retrieval," *EMNLP*, 2025. [arxiv.org/html/2510.05381v1](https://arxiv.org/html/2510.05381v1)

[28] "Context Discipline and Performance Correlation," *arXiv*, 2025. [arxiv.org/abs/2601.11564](https://arxiv.org/abs/2601.11564)

[29] S. Li et al., "Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach," *EMNLP Industry*, 2024. [aclanthology.org/2024.emnlp-industry.66/](https://aclanthology.org/2024.emnlp-industry.66/)

[30] RAGFlow, "From RAG to Context — A 2025 Year-End Review," 2025. [ragflow.io/blog/rag-review-2025-from-rag-to-context](https://ragflow.io/blog/rag-review-2025-from-rag-to-context)

[31] J. Nielsen, "Progressive Disclosure," Nielsen Norman Group. [nngroup.com/videos/progressive-disclosure/](https://www.nngroup.com/videos/progressive-disclosure/)

[32] Inferable.ai, "Progressive Context Enrichment for LLMs," 2024. [inferable.ai/blog/posts/llm-progressive-context-encrichment](https://www.inferable.ai/blog/posts/llm-progressive-context-encrichment)

[33] G. A. Miller, "The Magical Number Seven, Plus or Minus Two: Some Limits on Our Capacity for Processing Information," *Psychological Review*, vol. 63, no. 2, pp. 81–97, 1956.

[34] A. D. Baddeley, "Working Memory," *Science*, vol. 255, no. 5044, pp. 556–559, 1992. (Original model proposed in Baddeley & Hitch, 1974.)

[35] C. Packer et al., "MemGPT: Towards LLMs as Operating Systems," 2023. [arxiv.org/abs/2310.08560](https://arxiv.org/abs/2310.08560)

[36] S. Kapoor et al., "AI Agents That Matter," 2024. [arxiv.org/abs/2407.01502](https://arxiv.org/abs/2407.01502)

[37] LlamaIndex, "Bending without Breaking: Optimal Design Patterns for Effective Agents," 2025.

[38] I. Koren and C. M. Krishna, *Fault-Tolerant Systems*. Morgan Kaufmann, 2007.

[39] W. Torres-Pomales, "Software Fault Tolerance: A Tutorial," NASA Technical Memorandum TM-2000-210616, 2000.

[40] R. Hanmer, *Patterns for Fault Tolerant Software*. Wiley, 2007.

[41] "Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI," 2025. [arxiv.org/html/2512.12791v1](https://arxiv.org/html/2512.12791v1)

[42] M. Cemri et al., "Why Do Multi-Agent LLM Systems Fail?", 2025. [arxiv.org/abs/2503.13657](https://arxiv.org/abs/2503.13657)

[43] R. S. Sutton and A. G. Barto, *Reinforcement Learning: An Introduction*, 2nd ed. MIT Press, 2018.

[44] L. Wang et al., "A Survey on Large Language Model based Autonomous Agents," *Frontiers of Computer Science*, 2024. [link.springer.com/article/10.1007/s11704-024-40231-1](https://link.springer.com/article/10.1007/s11704-024-40231-1)

[45] "Cognitive Workspace: Active Memory Management for Large Language Models," 2025. [arxiv.org/abs/2508.13171](https://arxiv.org/abs/2508.13171)

[46] Anthropic, "Introducing the Model Context Protocol," 2024. [anthropic.com/news/model-context-protocol](https://www.anthropic.com/news/model-context-protocol)

[47] "Design Patterns for Securing LLM Agents against Prompt Injection," 2025. [arxiv.org/html/2506.08837v2](https://arxiv.org/html/2506.08837v2)

[48] P. Lewis et al., "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks," *NeurIPS*, 2020. [arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)

[49] D. Edge et al., "From Local to Global: A Graph RAG Approach to Query-Focused Summarization," Microsoft Research, 2024. [arxiv.org/abs/2404.16130](https://arxiv.org/abs/2404.16130)

[50] Cognition AI, "Don't Build Multi-Agents," 2025. [cognition.ai/blog/dont-build-multi-agents](https://cognition.ai/blog/dont-build-multi-agents)

[51] "A Survey on LLM-based Multi-Agent Systems," *IJCAI*, 2024. [arxiv.org/abs/2402.01680](https://arxiv.org/abs/2402.01680)

[52] Y. Du et al., "Improving Factuality and Reasoning in Language Models through Multiagent Debate," *ICML*, 2024. [arxiv.org/abs/2305.14325](https://arxiv.org/abs/2305.14325)

[53] Z. Li et al., "When Single-Agent with Skills Replace Multi-Agent Systems and When They Fail," *ACL*, 2025. [arxiv.org/abs/2601.04748](https://arxiv.org/abs/2601.04748)

[54] J. Wei et al., "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models," *NeurIPS*, 2022. [arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)

[55] S. Yao et al., "Tree of Thoughts: Deliberate Problem Solving with Large Language Models," *NeurIPS*, 2023. [arxiv.org/abs/2305.10601](https://arxiv.org/abs/2305.10601)

[56] S. Yao et al., "ReAct: Synergizing Reasoning and Acting in Language Models," *ICLR*, 2023. [arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)

[57] N. Shinn et al., "Reflexion: Language Agents with Verbal Reinforcement Learning," *NeurIPS*, 2023. [arxiv.org/abs/2303.11366](https://arxiv.org/abs/2303.11366)

[58] X. Huang et al., "Understanding the Planning of LLM Agents: A Survey," 2024. [arxiv.org/abs/2402.02716](https://arxiv.org/abs/2402.02716)

[59] A. Zhou et al., "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models (LATS)," *ICML*, 2024. [arxiv.org/abs/2310.04406](https://arxiv.org/abs/2310.04406)

[60] "Deep Agent: Agentic AI with Hierarchical Task DAG and Adaptive Execution," 2025. [arxiv.org/html/2502.07056v1](https://arxiv.org/html/2502.07056v1)

[61] "Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks," 2025. [arxiv.org/html/2503.09572v3](https://arxiv.org/html/2503.09572v3)

[62] W. Huang et al., "Inner Monologue: Embodied Reasoning through Planning with Language Models," *CoRL*, 2022. [arxiv.org/abs/2207.05608](https://arxiv.org/abs/2207.05608)

[63] H. Zhou et al., "SHIELDA: Structured Handling of Intelligent Exceptions in LLM-Driven Agentic Workflows," 2025. [arxiv.org/abs/2508.07935](https://arxiv.org/abs/2508.07935)

[64] C. E. Jimenez et al., "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?", *ICLR*, 2024. [arxiv.org/abs/2310.06770](https://arxiv.org/abs/2310.06770)

[65] X. Liu et al., "AgentBench: Evaluating LLMs as Agents," *ICLR*, 2024. [arxiv.org/abs/2308.03688](https://arxiv.org/abs/2308.03688)

[66] G. Mialon et al., "GAIA: A Benchmark for General AI Assistants," *ICLR*, 2024. [arxiv.org/abs/2311.12983](https://arxiv.org/abs/2311.12983)

[67] OpenTelemetry, "Semantic Conventions for Generative AI Systems," 2024. [opentelemetry.io/docs/specs/semconv/gen-ai/](https://opentelemetry.io/docs/specs/semconv/gen-ai/)

[68] Rebelion.la, "Why Too Many Tools Can Break Your AI Agent," 2025. [rebelion.la/agent-tools-guardrails-why-too-many-tools-can-break-your-ai](https://rebelion.la/agent-tools-guardrails-why-too-many-tools-can-break-your-ai)

[69] Epic AI, "Solving Tool Overload: A Vision for Smarter AI Assistants," 2025. [epicai.pro/solving-tool-overload-a-vision-for-smarter-ai-assistants-upbdc](https://www.epicai.pro/solving-tool-overload-a-vision-for-smarter-ai-assistants-upbdc)

[70] LlamaIndex, "Dumber LLM Agents Need More Constraints and Better Tools," 2024.

[71] S. Banerjee et al., "CRANE: Reasoning with Constrained LLM Generation," *ICML*, 2025. [arxiv.org/abs/2502.09061](https://arxiv.org/abs/2502.09061)

[72] Nilenso, "Artisanal Shims for the Bitter Lesson Age," 2025. [blog.nilenso.com/blog/2025/10/14/bitter-lesson-applied-ai/](https://blog.nilenso.com/blog/2025/10/14/bitter-lesson-applied-ai/)

[73] "Enhancing LLM Planning Capabilities through Intrinsic Self-Critique," 2025. [arxiv.org/abs/2512.24103](https://arxiv.org/abs/2512.24103)

[74] OpenAI, "Swarm" (experimental orchestration framework), 2024. [github.com/openai/swarm](https://github.com/openai/swarm)

[75] "A Taxonomy of Hierarchical Multi-Agent Systems," 2025. [arxiv.org/pdf/2508.12683](https://arxiv.org/pdf/2508.12683)

[76] C. Snell et al., "Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Model Parameters," *ICLR Oral*, 2025. [arxiv.org/abs/2408.03314](https://arxiv.org/abs/2408.03314)

[77] "Agentic Plan Caching: Saving Test-Time Compute for Fast LLM Agent Planning," 2025.

[78] Google DeepMind, "Towards a Science of Scaling Agent Systems," 2025. [arxiv.org/html/2512.08296v1](https://arxiv.org/html/2512.08296v1)

[79] S. Debenedetti et al., "AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents," *NeurIPS*, 2024. [arxiv.org/abs/2406.13352](https://arxiv.org/abs/2406.13352)

[80] OWASP, "OWASP Top 10 for Large Language Model Applications," 2025. [owasp.org/www-project-top-10-for-large-language-model-applications/](https://owasp.org/www-project-top-10-for-large-language-model-applications/)

[81] SmartScope, "RAG Debate: Agentic Search & Code Exploration," 2025. [smartscope.blog/en/ai-development/practices/rag-debate-agentic-search-code-exploration/](https://smartscope.blog/en/ai-development/practices/rag-debate-agentic-search-code-exploration/)

[82] Aider, "Repository Map." [aider.chat/docs/repomap.html](https://aider.chat/docs/repomap.html)

[83] LlamaIndex, "RAG is Dead, Long Live Agentic Retrieval," 2025. [llamaindex.ai/blog/rag-is-dead-long-live-agentic-retrieval](https://www.llamaindex.ai/blog/rag-is-dead-long-live-agentic-retrieval)

[84] OpenAI, "Introducing Codex," 2025. [openai.com/index/introducing-codex/](https://openai.com/index/introducing-codex/)

[85] GitHub, "About Coding Agent," 2025. [docs.github.com/en/copilot/concepts/agents/coding-agent/about-coding-agent](https://docs.github.com/en/copilot/concepts/agents/coding-agent/about-coding-agent)

[86] Google, "Jules is now available," 2025. [blog.google/technology/google-labs/jules-now-available/](https://blog.google/technology/google-labs/jules-now-available/)

[87] Cognition AI, "Devin Annual Performance Review 2025," 2025. [cognition.ai/blog/devin-annual-performance-review-2025](https://cognition.ai/blog/devin-annual-performance-review-2025)

[88] OpenAI, "Introducing Operator," 2025. [openai.com/index/introducing-operator/](https://openai.com/index/introducing-operator/)

[89] Google DeepMind, "Project Mariner," 2025. [deepmind.google/models/project-mariner/](https://deepmind.google/models/project-mariner/)

[90] Google, "A2A: A new era of agent interoperability," 2025. [developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/)

[91] Anthropic, "Agent Skills Specification," 2025. [github.com/anthropics/skills](https://github.com/anthropics/skills)

[92] Anthropic, "Effective Harnesses for Long-Running Agents," 2025. [anthropic.com/engineering/effective-harnesses-for-long-running-agents](https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents)

[93] Meta, "LlamaFirewall: An Open Source Guardrail System for Building Secure AI Agents," 2025. [arxiv.org/abs/2505.03574](https://arxiv.org/abs/2505.03574)

---

**Document Status**: Work in Progress
**Version**: Null
**Last Updated**: 2026-02-18
**Author**: Mario Garrido
        `;

        // Render the markdown
        document.getElementById('content').innerHTML = marked.parse(whitepaperMarkdown);

        // Add IDs to headings for navigation
        const addHeadingIds = () => {
            const content = document.getElementById('content');
            const headings = content.querySelectorAll('h2, h3, h4');

            const idMap = {
                '1. Introduction': 'introduction',
                '1.1 The Current State of Agents': 'current-state',
                '1.2 Evolution of Agentic Workflows': 'evolution',
                '2. Core Principles': 'principles',
                '2.1 Structure Reduces Problem Space': 'structure',
                '2.1.1 External Validation Signals': 'validation',
                '2.1.2 Tools Enable Hierarchical Reasoning': 'tools-principle',
                '2.2 Context Engineering': 'minimize-context',
                '2.2.1 Progressive Disclosure': 'progressive',
                '2.3 The Cost of Unconstrained Systems': 'unconstrained',
                'The Two Cost Functions': 'two-cost-functions',
                'Error Compounding in Multi-Step Tasks': 'error-compounding',
                'The Validation Cost Asymmetry': 'validation-asymmetry',
                'When Structure Doesn\'t Pay': 'when-structure-doesnt-pay',
                '2.4 The Flexibility-Reliability Tradeoff': 'tradeoff',
                '2.4.1 Designing for Harness Obsolescence': 'harness-obsolescence',
                '2.5 Practical Corollaries': 'corollaries',
                '2.5.1 Redundancy is a Feature': 'redundancy',
                '3. Taxonomy of Structured Agents': 'taxonomy',
                '3.1 The Agent Model': 'agent-model',
                'Invocation': 'invocation',
                'Agent': 'agent',
                'Context Window': 'context-window',
                'State': 'state',
                'Structure Classes': 'structure-classes',
                '3.2 Actions': 'actions',
                'Tools': 'tools-section',
                'Forced Tool Use': 'forced-tools',
                'Communication': 'communication',
                '3.3 Information Management': 'information',
                'Contexts': 'contexts',
                'Progressive Disclosure in Practice': 'progressive-disclosure',
                'Forced Contexts': 'forced-contexts',
                'Retrieval Mechanisms (RAG, GraphRAG, grep, etc.)': 'retrieval',
                '3.4 Composition': 'composition',
                'Subagents': 'subagents',
                'Delegation Challenges': 'delegation-challenges',
                'Skills': 'skills',
                '3.5 Planning': 'planning',
                'Why Planning Matters': 'planning-why',
                'Plan Representations': 'plan-representations',
                'Planning Strategies': 'planning-strategies',
                'Plan Execution': 'plan-execution',
                'Planning and Composition': 'planning-composition',
                '3.6 Error Handling & Recovery': 'error-handling',
                'Failure Modes': 'failure-modes',
                'Recovery Strategies': 'recovery-strategies',
                'Interruptions as Hierarchical Signals': 'interruptions',
                '3.7 Evaluation & Observability': 'evaluation',
                'What to Measure': 'what-to-measure',
                'Observability Primitives': 'observability',
                'Testing Agentic Systems': 'testing-agents',
                '3.8 Safety & Security': 'safety',
                'Prompt Injection': 'prompt-injection',
                'Adversarial Resilience': 'adversarial-resilience',
                'Tool Misuse': 'tool-misuse',
                'Structure as Implicit Alignment': 'implicit-alignment',
                '4. Open Questions & Future Directions': 'future',
                '5. Conclusion': 'conclusion',
                'Appendix: Terminology Reference': 'appendix',
                'References': 'references'
            };

            headings.forEach(heading => {
                const text = heading.textContent.trim();
                if (idMap[text]) {
                    heading.id = idMap[text];
                }
            });
        };

        // Make citation markers clickable and add IDs to reference entries
        const linkCitations = () => {
            const content = document.getElementById('content');
            const html = content.innerHTML;

            // Find the References heading
            const refsMatch = html.match(/<h2[^>]*>References<\/h2>/);
            if (!refsMatch) return;
            const refsIdx = html.indexOf(refsMatch[0]);

            // Split content at the References heading
            let beforeRefs = html.substring(0, refsIdx);
            let refsAndAfter = html.substring(refsIdx);

            // Make inline citation markers clickable (only before References section)
            beforeRefs = beforeRefs.replace(/\[(\d+)\]/g, '<a class="citation" href="#ref-$1">[$1]</a>');

            // Add IDs to reference entry paragraphs in the References section
            refsAndAfter = refsAndAfter.replace(/<p>\[(\d+)\]/g, '<p id="ref-$1">[$1]');

            content.innerHTML = beforeRefs + refsAndAfter;
        };

        /* Version filtering — commented out for now, showing full article only
        function tagVersionContent() {
            const content = document.getElementById('content');
            content.querySelectorAll('details.counterpoints').forEach(el => {
                el.setAttribute('data-version', 'formal');
            });
            content.querySelectorAll('p').forEach(p => {
                if (p.textContent.match(/^\s*\[TODO/)) {
                    p.setAttribute('data-version', 'formal');
                }
            });
            const refsHeading = document.getElementById('references');
            if (refsHeading) {
                const prevEl = refsHeading.previousElementSibling;
                if (prevEl && prevEl.tagName === 'HR') {
                    prevEl.setAttribute('data-version', 'formal');
                }
                refsHeading.setAttribute('data-version', 'formal');
                let el = refsHeading.nextElementSibling;
                while (el) {
                    if (el.tagName === 'HR') break;
                    el.setAttribute('data-version', 'formal');
                    el = el.nextElementSibling;
                }
            }
        }

        function applyVersionFilter(filter) {
            const content = document.getElementById('content');
            document.querySelectorAll('.filter-bar .filter-btn').forEach(btn => {
                btn.classList.toggle('active', btn.dataset.filter === filter);
            });
            const versioned = content.querySelectorAll('[data-version]');
            const variantGroups = content.querySelectorAll('[data-variant-group]');

            if (filter === 'full') {
                versioned.forEach(el => {
                    if (!el.closest('[data-variant-group]')) {
                        el.classList.remove('version-hidden');
                    }
                });
                variantGroups.forEach(group => {
                    group.classList.remove('version-hidden');
                    const variants = group.querySelectorAll(':scope > [data-version]');
                    let toggle = group.querySelector('.variant-toggle');
                    if (!toggle && variants.length > 1) {
                        toggle = createVariantToggle(group);
                    }
                    if (toggle) toggle.style.display = '';
                    variants.forEach((v, i) => {
                        if (v.classList.contains('variant-active')) {
                            v.classList.remove('version-hidden');
                        } else if (!group.querySelector('.variant-active')) {
                            v.classList.toggle('version-hidden', i !== 0);
                            if (i === 0) v.classList.add('variant-active');
                        } else {
                            v.classList.add('version-hidden');
                        }
                    });
                });
            } else {
                versioned.forEach(el => {
                    if (el.closest('[data-variant-group]')) return;
                    el.classList.toggle('version-hidden', el.getAttribute('data-version') !== filter);
                });
                variantGroups.forEach(group => {
                    const matching = group.querySelector('[data-version="' + filter + '"]');
                    if (matching) {
                        group.classList.remove('version-hidden');
                        group.querySelectorAll(':scope > [data-version]').forEach(v => {
                            v.classList.toggle('version-hidden', v !== matching);
                        });
                    } else {
                        group.classList.add('version-hidden');
                    }
                    const toggle = group.querySelector('.variant-toggle');
                    if (toggle) toggle.style.display = 'none';
                });
            }

            // Update sidebar TOC visibility for hidden headings
            const sidebar = document.querySelector('.sidebar nav');
            if (sidebar) {
                sidebar.querySelectorAll('a[href]').forEach(link => {
                    const targetId = link.getAttribute('href').substring(1);
                    const target = document.getElementById(targetId);
                    const li = link.closest('li');
                    if (target && li) {
                        li.style.display = target.classList.contains('version-hidden') ? 'none' : '';
                    }
                });
            }

            const url = new URL(window.location);
            if (filter === 'full') {
                url.searchParams.delete('view');
            } else {
                url.searchParams.set('view', filter);
            }
            history.replaceState(null, '', url);
        }

        function createVariantToggle(group) {
            const variants = group.querySelectorAll(':scope > [data-version]');
            if (variants.length < 2) return null;
            const toggle = document.createElement('div');
            toggle.className = 'variant-toggle';
            variants.forEach((v, i) => {
                const btn = document.createElement('button');
                const label = v.getAttribute('data-version');
                btn.className = 'variant-toggle-btn' + (i === 0 ? ' active' : '');
                btn.textContent = label.charAt(0).toUpperCase() + label.slice(1);
                btn.addEventListener('click', () => {
                    toggle.querySelectorAll('.variant-toggle-btn').forEach(b => b.classList.remove('active'));
                    btn.classList.add('active');
                    variants.forEach(vv => {
                        vv.classList.toggle('version-hidden', vv !== v);
                        vv.classList.toggle('variant-active', vv === v);
                    });
                });
                toggle.appendChild(btn);
            });
            variants[0].classList.add('variant-active');
            variants.forEach((v, i) => { if (i > 0) v.classList.add('version-hidden'); });
            group.insertBefore(toggle, group.firstChild);
            return toggle;
        }

        document.querySelectorAll('.filter-bar .filter-btn').forEach(btn => {
            btn.addEventListener('click', () => applyVersionFilter(btn.dataset.filter));
        });
        */

        // Sticky header (shared component)
        initStickyHeader();

        // Sidebar TOC (shared component)
        initSidebarTOC({
            sectionSelector: '#content h2, #content h3',
            scrollOffset: 120
        });

        // Theme toggle
        function getPreferredTheme() {
            const stored = localStorage.getItem('theme');
            if (stored) return stored;
            return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
        }

        function setTheme(theme) {
            document.documentElement.setAttribute('data-theme', theme);
            localStorage.setItem('theme', theme);
        }

        function toggleTheme() {
            const current = document.documentElement.getAttribute('data-theme');
            setTheme(current === 'dark' ? 'light' : 'dark');
        }


        // Initialize
        setTheme(getPreferredTheme());
        addHeadingIds();
        linkCitations();
        // tagVersionContent();
        // const viewParam = new URLSearchParams(window.location.search).get('view');
        // if (viewParam && ['formal', 'builder'].includes(viewParam)) {
        //     applyVersionFilter(viewParam);
        // }
        window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', e => {
            if (!localStorage.getItem('theme')) {
                setTheme(e.matches ? 'dark' : 'light');
            }
        });
    </script>
</body>
</html>
