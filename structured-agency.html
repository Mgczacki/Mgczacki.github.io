<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Structured Agency | Mario Garrido</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="assets/base.css">
    <style>
        :root {
            --green: #3d7a4a;
            --green-light: #e8f0ea;
            --green-border: #b8d4be;
        }

        [data-theme="dark"] {
            --green: #6aab73;
            --green-light: #1a2b1e;
            --green-border: #2d4a33;
        }

        .wip-banner {
            background: #fff0f0;
            border: 2px solid #c00;
            border-radius: 6px;
            padding: 16px 20px;
            margin: 24px 0;
            color: #900;
            font-size: 15px;
            line-height: 1.6;
        }

        [data-theme="dark"] .wip-banner {
            background: #2b1414;
            border-color: #a33;
            color: #e8a0a0;
        }

        body {
            font-family: 'Inter', 'Helvetica Neue', Helvetica, Arial, sans-serif;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 60px 60px 120px;
            display: grid;
            grid-template-columns: 220px 1fr;
            gap: 80px;
            align-items: start;
        }

        /* Sidebar TOC: base styles loaded from assets/sidebar-toc.css */

        .content-wrapper {
            min-width: 0;
            max-width: 900px;
        }

        .page-header {
            margin-bottom: 48px;
            padding-bottom: 40px;
            border-bottom: 1px solid var(--border-faint);
        }

        .page-header h1 {
            font-size: 36px;
            font-weight: 700;
            margin-bottom: 16px;
            letter-spacing: -0.5px;
            color: var(--text);
            line-height: 1.2;
        }

        .meta {
            color: var(--text-faint);
            font-size: 13px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            font-weight: 500;
        }

        #content {
            background: transparent;
        }

        #content h1 {
            font-size: 28px;
            font-weight: 700;
            margin: 48px 0 20px 0;
            letter-spacing: -0.4px;
            line-height: 1.25;
            color: var(--text);
        }

        #content h2 {
            font-size: 24px;
            font-weight: 700;
            margin: 56px 0 20px 0;
            color: var(--text);
            letter-spacing: -0.3px;
            line-height: 1.3;
            padding-top: 8px;
        }

        #content h2:first-child {
            margin-top: 0;
        }

        #content h3 {
            font-size: 18px;
            font-weight: 700;
            margin: 36px 0 16px 0;
            color: var(--text);
            letter-spacing: -0.2px;
            line-height: 1.35;
        }

        #content h4 {
            font-size: 16px;
            font-weight: 600;
            margin: 28px 0 12px 0;
            color: var(--text);
        }

        #content h5 {
            font-size: 15px;
            font-weight: 600;
            margin: 24px 0 10px 0;
            color: var(--text);
        }

        #content p {
            margin-bottom: 1.5em;
            color: var(--text-secondary);
            font-size: 16px;
            line-height: 1.7;
        }

        #content ul, #content ol {
            margin-bottom: 1.5em;
            padding-left: 24px;
        }

        #content li {
            margin-bottom: 0.65em;
            line-height: 1.7;
            color: var(--text-secondary);
        }

        #content strong {
            color: var(--text);
            font-weight: 600;
        }

        #content code {
            background: var(--bg-alt);
            padding: 3px 6px;
            border-radius: 3px;
            font-family: 'SF Mono', Monaco, monospace;
            font-size: 14px;
            color: var(--text);
            border: 1px solid var(--border-faint);
        }

        #content pre {
            background: var(--bg-alt);
            padding: 20px;
            border-radius: 6px;
            overflow-x: auto;
            margin-bottom: 1.5em;
            border: 1px solid var(--border-light);
        }

        #content pre code {
            background: none;
            padding: 0;
            color: var(--text-muted);
            border: none;
            font-size: 13px;
        }

        #content blockquote {
            border-left: 2px solid var(--border-light);
            padding-left: 20px;
            margin: 1.5em 0;
            color: var(--text-muted);
            font-style: italic;
        }

        #content a {
            color: var(--text);
            font-weight: 500;
            text-decoration: underline;
            text-decoration-color: var(--border-light);
            transition: text-decoration-color 0.2s;
        }

        #content a:hover {
            text-decoration-color: var(--text);
        }

        #content a.citation {
            text-decoration: none;
            color: var(--text-muted);
            font-weight: 400;
            font-size: 0.88em;
        }

        #content a.citation:hover {
            color: var(--text);
            text-decoration: underline;
        }

        #content details.counterpoints {
            margin: 24px 0 32px;
            border: 1px solid var(--border-light);
            border-radius: 6px;
            padding: 0;
            background: var(--bg-alt);
        }

        #content details.counterpoints summary {
            cursor: pointer;
            padding: 14px 20px;
            font-size: 13px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.4px;
            color: var(--text-muted);
            list-style: none;
            display: flex;
            align-items: center;
            gap: 8px;
            user-select: none;
        }

        #content details.counterpoints summary::-webkit-details-marker {
            display: none;
        }

        #content details.counterpoints summary::before {
            content: '▸';
            font-size: 12px;
            transition: transform 0.2s;
        }

        #content details.counterpoints[open] summary::before {
            transform: rotate(90deg);
        }

        #content details.counterpoints summary:hover {
            color: var(--text);
        }

        #content details.counterpoints .counterpoints-body {
            padding: 0 20px 20px;
            border-top: 1px solid var(--border-light);
        }

        #content details.counterpoints .counterpoints-body p {
            font-size: 15px;
            margin-bottom: 1em;
        }

        #content details.counterpoints .counterpoints-body p:last-child {
            margin-bottom: 0;
        }

        #content details.counterpoints .counterpoints-body .author-response {
            margin-top: 16px;
            padding-left: 16px;
            border-left: 2px solid var(--border-light);
        }

        #content details.counterpoints .counterpoints-body .author-response p:first-child::before {
            content: 'Author\27s note: ';
            font-weight: 600;
            color: var(--text);
        }

        #content details.practical-example {
            margin: 24px 0 32px;
            border: 1px solid var(--green-border);
            border-left: 3px solid var(--green);
            border-radius: 6px;
            padding: 0;
            background: var(--green-light);
        }

        #content details.practical-example summary {
            cursor: pointer;
            padding: 14px 20px;
            font-size: 13px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.4px;
            color: var(--green);
            list-style: none;
            display: flex;
            align-items: center;
            gap: 8px;
            user-select: none;
        }

        #content details.practical-example summary::-webkit-details-marker {
            display: none;
        }

        #content details.practical-example summary::before {
            content: '\25B8';
            font-size: 12px;
            transition: transform 0.2s;
        }

        #content details.practical-example[open] summary::before {
            transform: rotate(90deg);
        }

        #content details.practical-example summary:hover {
            color: var(--text);
        }

        #content details.practical-example .practical-example-body {
            padding: 0 20px 20px;
            border-top: 1px solid var(--green-border);
        }

        #content details.practical-example .practical-example-body p {
            font-size: 15px;
            margin-bottom: 1em;
        }

        #content details.practical-example .practical-example-body p:last-child {
            margin-bottom: 0;
        }

        #content details.practical-example .practical-example-body ol,
        #content details.practical-example .practical-example-body ul {
            font-size: 15px;
        }

        #content details.deep-dive {
            margin: 24px 0 32px;
            border: 1px solid var(--border-faint);
            border-left: 3px solid var(--text-faint);
            border-radius: 6px;
            padding: 0;
            background: var(--bg-alt);
        }

        #content details.deep-dive summary {
            cursor: pointer;
            padding: 14px 20px;
            font-size: 13px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.4px;
            color: var(--text-faint);
            list-style: none;
            display: flex;
            align-items: center;
            gap: 8px;
            user-select: none;
        }

        #content details.deep-dive summary::-webkit-details-marker {
            display: none;
        }

        #content details.deep-dive summary::before {
            content: '\25B8';
            font-size: 12px;
            transition: transform 0.2s;
        }

        #content details.deep-dive[open] summary::before {
            transform: rotate(90deg);
        }

        #content details.deep-dive summary:hover {
            color: var(--text);
        }

        #content details.deep-dive .deep-dive-body {
            padding: 0 20px 20px;
            border-top: 1px solid var(--border-faint);
        }

        #content details.deep-dive .deep-dive-body p {
            font-size: 15px;
            margin-bottom: 1em;
        }

        #content details.deep-dive .deep-dive-body p:last-child {
            margin-bottom: 0;
        }

        #content details.deep-dive .deep-dive-body h5 {
            margin-top: 1.5em;
        }

        #content .definition {
            margin: 24px 0;
            padding: 16px 20px;
            border-left: 3px solid #5b8a9a;
            background: var(--bg-alt);
            border-radius: 0 4px 4px 0;
            font-size: 15px;
            line-height: 1.7;
        }

        #content .definition p {
            margin: 0 0 10px 0;
        }

        #content .definition p:last-child {
            margin-bottom: 0;
        }

        #content .definition strong {
            font-weight: 700;
        }

        #content .definition ul {
            margin: 8px 0 0 0;
            padding-left: 20px;
        }

        #content .definition li {
            margin-bottom: 6px;
        }

        #content .principle {
            margin: 20px 0;
            padding: 12px 18px;
            border-left: 3px solid var(--text);
            background: var(--bg-alt);
            border-radius: 0 4px 4px 0;
            font-size: 15px;
            line-height: 1.6;
        }

        #content .principle p {
            margin: 0;
        }

        #content .principle strong {
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.3px;
            font-size: 11px;
        }

        #content img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 2em auto;
        }

        [data-theme="dark"] #content img {
            filter: invert(1);
        }

        #content hr {
            border: none;
            border-top: 1px solid var(--border-light);
            margin: 48px 0;
        }

        #content table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5em 0;
            font-size: 15px;
        }

        #content table th {
            font-size: 11px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            color: var(--text-faint);
            text-align: left;
            padding: 0 12px 16px 0;
            border-bottom: 1px solid var(--border-light);
        }

        #content table td {
            padding: 16px 12px 16px 0;
            border-bottom: 1px solid var(--border-faint);
            vertical-align: top;
            color: var(--text-secondary);
        }

        #content table tr:last-child td {
            border-bottom: none;
        }

        /* Version filtering UI — commented out for now, showing full article only
        .filter-bar {
            display: flex;
            gap: 8px;
            flex-wrap: wrap;
        }

        .filter-btn {
            background: none;
            color: var(--text-muted);
            padding: 0 0 2px;
            font-size: 13px;
            border: none;
            border-bottom: 1.5px solid transparent;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            font-family: inherit;
        }

        .filter-btn:hover {
            color: var(--text);
        }

        .filter-btn.active {
            color: var(--text);
            border-bottom-color: var(--text);
        }

        .version-hidden {
            display: none;
        }

        .variant-toggle {
            display: inline-flex;
            gap: 0;
            margin-bottom: 12px;
            border: 1px solid var(--border-light);
            border-radius: 4px;
            overflow: hidden;
        }

        .variant-toggle-btn {
            background: none;
            border: none;
            padding: 4px 12px;
            font-size: 12px;
            font-weight: 500;
            color: var(--text-muted);
            cursor: pointer;
            font-family: inherit;
            transition: all 0.2s;
        }

        .variant-toggle-btn:not(:last-child) {
            border-right: 1px solid var(--border-light);
        }

        .variant-toggle-btn:hover {
            color: var(--text);
        }

        .variant-toggle-btn.active {
            background: var(--bg-alt);
            color: var(--text);
        }
        */

        @media (max-width: 1024px) {
            .container {
                grid-template-columns: 1fr;
                gap: 40px;
            }

            .sidebar {
                position: static;
                max-height: none;
                display: none;
            }
        }

        @media (max-width: 768px) {
            .masthead-inner {
                padding: 0 24px;
            }

            .nav-links {
                gap: 16px;
            }

            .container {
                padding: 40px 24px 80px;
            }

            .page-header h1 {
                font-size: 28px;
            }

            #content h2 {
                font-size: 22px;
            }

            #content h3 {
                font-size: 17px;
            }
        }
    </style>
    <link rel="stylesheet" href="assets/sidebar-toc.css">
    <script src="assets/sidebar-toc.js"></script>
    <link rel="stylesheet" href="assets/sticky-header.css">
    <script src="assets/sticky-header.js"></script>
</head>
<body>
    <header class="masthead">
        <div class="masthead-inner">
            <a href="index.html" class="logo">Mario Garrido</a>
            <div class="masthead-right">
                <div class="nav-links">
                    <a href="writings.html">Writings</a>
                    <a href="projects.html">Projects</a>
                    <a href="cv.html">CV</a>
                </div>
                <button class="theme-toggle" onclick="toggleTheme()" aria-label="Toggle dark mode">
                    <svg class="sun-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </header>

    <div class="container">
        <aside class="sidebar">
            <h3>Contents</h3>
            <nav>
                <ul>
                    <li><a href="#introduction">1. Introduction</a>
                        <ul class="subsection">
                            <li><a href="#current-state">1.1 Current State</a></li>
                            <li><a href="#evolution">1.2 Evolution</a></li>
                        </ul>
                    </li>
                    <li><a href="#principles">2. Core Principles</a>
                        <ul class="subsection">
                            <li><a href="#structure">2.1 Structure</a></li>
                            <li><a href="#minimize-context">2.2 Context Engineering</a></li>
                            <li><a href="#unconstrained">2.3 Unconstrained Cost</a></li>
                            <li><a href="#tradeoff">2.4 Flexibility-Reliability</a>
                                <ul class="subsection">
                                    <li><a href="#harness-obsolescence">Harness Obsolescence</a></li>
                                    <li><a href="#commoditizing-intelligence">Commoditizing Intelligence</a></li>
                                </ul>
                            </li>
                            <li><a href="#corollaries">2.5 Corollaries</a>
                                <ul class="subsection">
                                    <li><a href="#redundancy">Redundancy is a Feature</a></li>
                                    <li><a href="#stress-strengthens">Stress Strengthens a System</a></li>
                                    <li><a href="#harness-awareness">The Agent Should Know Its Harness</a></li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                    <li><a href="#taxonomy">3. Taxonomy of Structured Agents</a>
                        <ul class="subsection">
                            <li><a href="#agent">3.1 Agent</a>
                                <ul class="subsection">
                                    <li><a href="#invocation">Invocation</a></li>
                                    <li><a href="#llm">LLM</a></li>
                                    <li><a href="#turn">Turn</a></li>
                                    <li><a href="#directives">Directives</a></li>
                                    <li><a href="#working-memory">Working Memory</a></li>
                                </ul>
                            </li>
                            <li><a href="#state">3.2 State</a>
                                <ul class="subsection">
                                    <li><a href="#structure-classes">Structure Classes</a></li>
                                    <li><a href="#state-staleness">State Staleness</a></li>
                                    <li><a href="#tools-as-state">Tools as State</a></li>
                                    <li><a href="#why-tools-are-state">Why Tools Are State</a></li>
                                </ul>
                            </li>
                            <li><a href="#tools-section">3.3 Tools</a>
                                <ul class="subsection">
                                    <li><a href="#tool-categories">Tool Categories</a></li>
                                    <li><a href="#bootstrapped-tools">Bootstrapped Tools</a></li>
                                    <li><a href="#tool-design">Tool Design</a></li>
                                    <li><a href="#forced-tools">Forced Tool Use</a></li>
                                    <li><a href="#communication">Communication</a></li>
                                </ul>
                            </li>
                            <li><a href="#contexts">3.4 Contexts</a>
                                <ul class="subsection">
                                    <li><a href="#progressive-disclosure">Progressive Disclosure</a></li>
                                    <li><a href="#forced-contexts">Forced Contexts</a></li>
                                    <li><a href="#retrieval">Retrieval Mechanisms</a></li>
                                </ul>
                            </li>
                            <li><a href="#composition">3.5 Composition</a>
                                <ul class="subsection">
                                    <li><a href="#delegation-challenges">Delegation Challenges</a></li>
                                </ul>
                            </li>
                            <li><a href="#planning">3.6 Planning</a></li>
                            <li><a href="#error-handling">3.7 Error Handling</a>
                                <ul class="subsection">
                                    <li><a href="#interruptions">Interruptions</a></li>
                                </ul>
                            </li>
                            <li><a href="#evaluation">3.8 Evaluation</a></li>
                            <li><a href="#safety">3.9 Safety & Security</a></li>
                        </ul>
                    </li>
                    <li><a href="#future">4. Open Questions</a></li>
                    <li><a href="#conclusion">5. Conclusion</a></li>
                    <li><a href="#appendix">Appendix</a></li>
                    <li><a href="#references">References</a></li>
                </ul>
            </nav>
        </aside>

        <div class="content-wrapper">
            <div class="page-header">
                <h1>Structured Agency</h1>
                <div class="meta">Mario Garrido • February 2026</div>
            </div>

            <!-- Version filter bar — commented out for now, showing full article only
            <div class="filter-bar" style="margin-bottom: 32px;">
                <button class="filter-btn active" data-filter="full">Full</button>
                <button class="filter-btn" data-filter="formal">Formal</button>
                <button class="filter-btn" data-filter="builder">Builder</button>
            </div>
            -->

            <div id="content">
<h3>On Building Reliable Agents</h3>
<div class="wip-banner">
<strong>Work in Progress.</strong> This document contains many placeholders, unverified citations, and slop illustrations. Do not treat it as a finished or authoritative reference.
</div>

<p>The individual principles behind reliable agents are not new—and the field knows it. Anthropic&#39;s &quot;Building Effective Agents&quot; <a class="citation" href="#ref-3">[3]</a> established that workflows and constraints outperform unconstrained autonomy. Their subsequent work on context engineering <a class="citation" href="#ref-90">[90]</a> and agent harnesses <a class="citation" href="#ref-91">[91]</a> codified progressive disclosure and structured state management. OpenAI&#39;s practical guide <a class="citation" href="#ref-9">[9]</a> converges on the same patterns. DSPy <a class="citation" href="#ref-92">[92]</a> formalized the shift from prompting to programming. The Bitter Lesson discourse <a class="citation" href="#ref-93">[93]</a><a class="citation" href="#ref-95">[95]</a> has been applied specifically to agent scaffolding. Context engineering as a discipline has its own rapidly growing literature <a class="citation" href="#ref-30">[30]</a><a class="citation" href="#ref-94">[94]</a>. The building blocks are well-understood.</p>
<p>What has been missing is a coherent <em>mental model</em>. The field suffers from widespread semantic confusion: "agent," "tool," "context," "state," "memory," and "planning" are used inconsistently across frameworks, papers, and products—often meaning different things in different contexts, or conflating concepts that should be distinct. LangChain's "agent" is not Anthropic's "agent" is not a reinforcement learning "agent." A "tool" in one framework is a "skill" in another and an "action" in a third. "Memory" might mean the context window, a vector store, a conversation log, or persistent state, depending on who is speaking. This confusion is not merely terminological—it leads to architectural mistakes, because builders inherit muddled abstractions and wire systems together without a clear picture of what each component <em>is</em> and how it relates to the others.</p>

<p>This paper attempts to cut through that confusion by proposing a first-principles mental model for thinking about agents: a dependency chain of principles that explains not just <em>what</em> to do, but <em>why</em> each principle follows from the others, and <em>when</em> the tradeoffs justify the investment. The existing literature offers cookbooks of patterns (Anthropic), toolkits of abstractions (LangChain, DSPy), and collections of best practices (OpenAI). This paper attempts something different: a unified framework with a cost model for deciding when structure is justified, an explicit dependency graph showing how principles build on each other, and a taxonomy of architectural surfaces where constraints can be applied.</p>
<p>This paper proposes <strong>Structured Agency</strong>: a framework for building harnesses around LLM-powered agents through deliberate constraints, external validation, and composable tooling. The agents that result are <strong>structured agents</strong>—systems that trade unconstrained flexibility for reliability, observability, and predictable behavior. The framework is grounded in three years of experience building agents across production systems at <a href="https://kuona.ai">kuona.ai</a>, academic research, and personal projects, informed by industry analysis, and organized as a dependency chain of principles where each builds on the last. The naming and formalism are my own and evolve over time.</p>
<hr>
<h2 id="introduction">1. Introduction</h2>
<h3 id="current-state">1.1 The Current State of Agents</h3>
<p>The past year has seen an explosion in agentic systems. Coding agents—Claude Code <a class="citation" href="#ref-7">[7]</a>, OpenAI Codex <a class="citation" href="#ref-84">[84]</a>, GitHub Copilot <a class="citation" href="#ref-85">[85]</a>, Google Jules <a class="citation" href="#ref-86">[86]</a>, Devin <a class="citation" href="#ref-87">[87]</a>—now routinely navigate codebases, write code, and run tests autonomously. Computer-using agents like OpenAI Operator <a class="citation" href="#ref-88">[88]</a> and Google Mariner <a class="citation" href="#ref-89">[89]</a> interact with GUIs through screenshots and virtual input. Multi-agent research systems <a class="citation" href="#ref-24">[24]</a> coordinate teams of specialized subagents. General-purpose agents like Manus achieved state-of-the-art on general assistant benchmarks <a class="citation" href="#ref-66">[66]</a> before being acquired by Meta. Interoperability protocols—MCP <a class="citation" href="#ref-46">[46]</a>, Agent2Agent <a class="citation" href="#ref-90">[90]</a>, Agent Skills <a class="citation" href="#ref-91">[91]</a>—have standardized how agents connect to tools and to each other. These systems, along with foundational research <a class="citation" href="#ref-6">[6]</a><a class="citation" href="#ref-3">[3]</a><a class="citation" href="#ref-9">[9]</a>, have converged on several insights that, taken together, motivate the framework proposed in this paper.</p>
<p><strong>1. Agents thrive with reliable, small-scoped tools which are composable</strong> <a class="citation" href="#ref-1">[1]</a><a class="citation" href="#ref-2">[2]</a><a class="citation" href="#ref-3">[3]</a><a class="citation" href="#ref-4">[4]</a>.</p>
<p>The UNIX design philosophy aligns remarkably well with this principle <a class="citation" href="#ref-5">[5]</a>. Tools like `grep`, `cat`, and `sed` have become commonplace in the most capable agents—not by accident, but because they embody:</p>
<ul>
<li>Single responsibility</li>
<li>Composability</li>
<li>Predictable behavior</li>
<li>Simple interfaces</li>
</ul>
<p>The convergence on composable tooling has gone beyond individual systems to produce open standards. The Model Context Protocol (MCP) <a class="citation" href="#ref-46">[46]</a> standardized how agents connect to tools through typed interfaces; Google&#39;s Agent2Agent Protocol (A2A) <a class="citation" href="#ref-90">[90]</a> standardized inter-agent communication through capability discovery (&quot;Agent Cards&quot;); Anthropic&#39;s Agent Skills specification <a class="citation" href="#ref-91">[91]</a> standardized reusable capability packages. By late 2025, MCP alone had 97M+ monthly SDK downloads and adoption from every major provider. This is not accidental—it reflects a structural insight: composable, well-defined tool interfaces are the foundation of reliable agent behavior.</p>
<p><strong>2. The most effective agents exploit the underlying structure of their problem domain.</strong></p>
<p>Composable tools are the mechanism, but the deeper pattern is that capable agents don&#39;t treat the world as unstructured text to reason over—they navigate the domain&#39;s own organization. Every major coding agent—Claude Code <a class="citation" href="#ref-7">[7]</a>, Codex <a class="citation" href="#ref-84">[84]</a>, Copilot <a class="citation" href="#ref-85">[85]</a>, Jules <a class="citation" href="#ref-86">[86]</a>, Cursor, Amazon Q—exploits the inherent structure of code: grepping file trees, following import graphs, reading type signatures, querying language server protocols <a class="citation" href="#ref-81">[81]</a><a class="citation" href="#ref-82">[82]</a>. A database agent that queries schemas before writing SQL exploits the structure of relational data. In each case, effectiveness comes not from raw reasoning over tokens but from leveraging domain structure that already exists—or, where it doesn&#39;t exist, creating it (Section 3.4).</p>
<p><strong>3. External systems validate better and cheaper than models self-assess</strong> <a class="citation" href="#ref-17">[17]</a><a class="citation" href="#ref-18">[18]</a>.</p>
<p>The field has converged on outsourcing verification to purpose-built external mechanisms—compilers, linters, test suites, schema validators, rule engines—not because models <em>cannot</em> self-correct, but because external validation provides reliable, deterministic signals. This pattern now appears at every scale: OpenAI Codex <a class="citation" href="#ref-84">[84]</a> runs in sandboxed containers with internet disabled (constraint through isolation), Operator <a class="citation" href="#ref-88">[88]</a> hands control to users when uncertain (human-in-the-loop validation), Meta&#39;s LlamaFirewall <a class="citation" href="#ref-93">[93]</a> provides guardrails as a separate system independent of the agent&#39;s reasoning, and Anthropic&#39;s harness patterns <a class="citation" href="#ref-92">[92]</a> use progress files and git checkpoints as external recovery signals for long-running agents. Section 2.1, External Validation establishes the principle; Section 2.3 makes the economic case.</p>
<p><strong>4. The context window is scarce working memory, not a knowledge store</strong> <a class="citation" href="#ref-25">[25]</a><a class="citation" href="#ref-26">[26]</a><a class="citation" href="#ref-27">[27]</a>.</p>
<p>The most capable production agents deliberately manage what enters the context window, when, and in what form. The discipline of <strong>context engineering</strong> <a class="citation" href="#ref-30">[30]</a>—selecting, ordering, representing, and timing the information an agent operates on—has emerged as a named practice precisely because context quality matters more than context quantity, and irrelevant context actively degrades performance. By mid-2025, the term had been popularized by Shopify CEO Tobi Lutke and Andrej Karpathy, reflecting a shift in the field&#39;s understanding: reliability comes not from better prompts but from better-structured context (Section 2.2).</p>
<h3 id="evolution">1.2 Evolution of Agentic Workflows</h3>
<p><img src="assets/agents/image2-evolution.svg" alt="Evolution Timeline"></p>
<p>The evolution of agentic capabilities has progressed alongside model improvements <a class="citation" href="#ref-3">[3]</a><a class="citation" href="#ref-8">[8]</a><a class="citation" href="#ref-9">[9]</a><a class="citation" href="#ref-10">[10]</a>:</p>
<ol>
<li><strong>Bespoke tasks with LLMs</strong> → Handcrafted prompts with manual routing</li>
<li><strong>Workflows with state machines</strong> → Deterministic transitions between states</li>
<li><strong>Agentic routing</strong> → Dynamic decision-making with tools</li>
<li><strong>Subagent delegation</strong> → Hierarchical task decomposition</li>
</ol>
<p>Crucially, these levels are not a strict progression where each supersedes the last—<strong>they coexist</strong>. A production system will typically contain all four: bespoke prompts for well-understood subtasks, state-machine workflows where determinism is needed, agentic routing for flexible decision-making, and subagent delegation for complex decomposition. In my own work, all of these coexist to form reliable, cost-effective, and flexible systems. The right level depends on the task and domain, not on the system&#39;s overall sophistication. A useful heuristic: the lower on this curve a task sits, the more that context stuffing and simple tool design dominate; the higher, the more that deliberate context engineering and structural harnesses become necessary.</p>
<details class="practical-example">
<summary>Practical Example — All Four Levels in One System</summary>
<div class="practical-example-body">

<p>A system I built at kuona.ai illustrates how all four levels coexist within a single task execution. The system is a general business agent that, among other capabilities, can delegate data transformation work to a specialized subagent.</p>

<p><img src="assets/agents/image10-practical-example.svg" alt="Practical Example: Four Levels"></p>
<p>A user asks the general agent to clean up a product catalog. The agent determines that a column of free-text product names needs to be transformed into standardized brand categories. Here is what happens:</p>

<p><strong>Level 4 — Subagent delegation.</strong> The general agent delegates the transformation task to a specialized data transformation subagent, passing a structured instruction: "Extract the brand from these product names."</p>

<p><strong>Level 3 — Agentic routing.</strong> The data transformation subagent operates in a ReAct loop. It dynamically selects which tool to call based on the instruction—in this case, a column transformation workflow that works on a Table (a structured class, see Section 3.2) and targets a specific column.</p>

<p><strong>Level 2 — Workflow with deterministic transitions.</strong> The column transformation tool is itself a workflow—a state machine that orchestrates a fixed sequence of operations: (1) obtain the column values, (2) deduplicate to extract unique values, (3) run a map operation over the unique values using an LLM, (4) apply the resulting mapping to the entire column to produce a derived column.</p>

<p><strong>Level 1 — Bespoke LLM prompt.</strong> Inside the map operation, a purpose-built prompt receives each unique product name and extracts the brand. This is a single-purpose, well-scoped LLM call with no tool access and no agentic behavior—just input → output.</p>

<p>The workflow completes, producing a new "brand" column. Control returns to the ReAct agent, which may perform additional transformations. When the subagent finishes, control returns to the general agent, which continues with the next business task.</p>

<p>Note what happened: a single user request traversed all four levels of the evolution curve. The bespoke prompt didn't need tools. The workflow didn't need agentic routing. The ReAct agent didn't need to understand the workflow's internals. And the general agent didn't need to know how brand extraction works. Each level operates within its appropriate constraints, and the hierarchical structure ensures that complexity is contained at each boundary.</p>

</div>
</details>

<p>This progression reveals three fundamental questions that any agentic system must answer:</p>
<ul>
<li><strong>Partition of work</strong>: Who does what work?</li>
<li><strong>Partition of responsibilities</strong>: Who is accountable for what outcomes?</li>
<li><strong>Partition of knowledge</strong>: Who knows what context?</li>
</ul>
<details class="counterpoints">
<summary>Counterpoints & Discussion</summary>
<div class="counterpoints-body">
<p>The "small, composable tools" thesis is not without criticism. Research suggests that tool overload—having too many fine-grained tools—can increase decision friction and hurt selection accuracy <a class="citation" href="#ref-68">[68]</a><a class="citation" href="#ref-69">[69]</a>. LlamaIndex (2024) notes that fine-grained decomposition creates an increased burden on less-capable models <a class="citation" href="#ref-70">[70]</a>. The UNIX "do one thing well" metaphor may not map cleanly to LLM tool selection, where the cost of choosing between many similar tools can outweigh the benefits of composability.</p>
<div class="author-response">
<p>My early experiments agree with these findings, but three mitigations exist: (1) formal plan validation with retries catches tool-selection errors before they propagate, (2) searching over the tool space with a dedicated meta-tool and progressive disclosure reduces the effective choice set, and (3) in most production systems the tool count is bounded by the use case—these issues are buildable-around. Recent model generations are also increasingly capable at tool selection, making this less of a concern over time.</p>
</div>
</div>
</details>

<hr>
<h2 id="principles">2. Core Principles</h2>
<div class="definition">

<p><strong>Structure</strong>, in this framework, is any pre-computed organization that reduces the space an agent must navigate — whether designed by humans, derived automatically from previous usage, or produced through optimization. It is the opposite of &quot;let the model figure it out from scratch.&quot; Structure takes many forms:</p>
<ul>
<li><strong>Domain structure</strong> — Encoding the problem domain: schemas, entity relationships, business rules, valid states. The domain has inherent structure; the harness captures it rather than asking the model to rediscover it.</li>
<li><strong>Cognitive structure</strong> — Human-aligned reasoning patterns: decomposition, planning, step-by-step processes, hierarchical thinking. Structure that mirrors how humans organize complex work.</li>
<li><strong>State structure</strong> — Typed, addressable representations rather than raw text. Tasks, contexts, plans, action history as first-class objects with lifecycle—not strings in a conversation. This is central to the framework: unified state is the substrate on which all other structure operates.</li>
<li><strong>Constraint structure</strong> — Boundaries that reduce the action space: output schemas, tool interfaces, validation rules, privilege boundaries. The walls that keep the agent on track.</li>
<li><strong>Compositional structure</strong> — How work is partitioned across agents, delegation boundaries, hierarchy. The organizational chart of the system.</li>
<li><strong>Information structure</strong> — How context is organized, layered, and disclosed. What the agent knows, when it knows it, and in what format.</li>
</ul>
</div>

<p><strong>A clarification on what structure is not.</strong> By "structure" this paper does not mean the old-style prescription of rigid multi-agent workflows — "build subagent A for extraction, subagent B for validation, subagent C for formatting, connected by a fixed DAG." Any practitioner who has built agents at scale reaches the same conclusion: more flexibility, more shared context, and fewer hard boundaries produce the most general and frictionless designs. Cognition AI's argument against multi-agent architectures <a class="citation" href="#ref-50">[50]</a> captures an intuition widely held in the field — that parallelism boundaries and context isolation often create more friction than they resolve. But the conclusion should not be "therefore, remove all structure." Agents that are both reliable and fast can shed the old rigid context boundaries <em>while</em> introducing more precise structures — structures that enable better orchestration, lower latency, and capabilities that exceed the cognitive ceiling of the base model alone. Parallelism <em>is</em> an incredibly useful concept within agentic systems, and context boundaries <em>are</em> powerful — as long as they can be gracefully bypassed when the task demands it. The structure advocated here is as much about progressive disclosure as it is about alignment: the agent can always explore beyond what is initially provided, but certain task boundaries, tool constraints, and information layers are simply smart design. They don't cage the agent — they give it a head start.</p>

<p>Most of these principles are individually well-established—constraint satisfaction, external validation, progressive disclosure, and hierarchical decomposition appear across the existing literature on agent design <a class="citation" href="#ref-3">[3]</a><a class="citation" href="#ref-9">[9]</a><a class="citation" href="#ref-90">[90]</a>. The contribution of this section is not the individual principles but their organization into a <strong>dependency chain</strong>: each principle follows logically from the previous, and the chain as a whole provides a framework for reasoning about <em>when</em> and <em>why</em> to invest in structure, not just <em>what</em> structure to apply.</p>
<p><img src="assets/agents/image1-principle-graph.svg" alt="Principle Dependency Graph"></p>
<h3 id="structure">2.1 Structure Reduces Problem Space</h3>
<div class="principle">

<p><strong>Principle</strong>: Constraints enable better performance, not worse <a class="citation" href="#ref-11">[11]</a><a class="citation" href="#ref-12">[12]</a><a class="citation" href="#ref-13">[13]</a>.</p>
</div>

<p>This is the foundational principle from which most others follow. Constraints:</p>
<ol>
<li>Enable validation signals (checkable boundaries)</li>
<li>Reduce problem spaces (fewer paths to explore)</li>
<li>Improve computational economy (less work per step) <a class="citation" href="#ref-14">[14]</a></li>
<li>Prevent error propagation between tasks (isolation)</li>
<li>Enable effective delegation (clear boundaries for splitting work across agents—see Section 3.5 for the full treatment)</li>
</ol>
<h4 id="validation">External Validation Signals</h4>
<p>Once a system is structured, its boundaries become checkable—which is what enables external validation.</p>
<div class="principle">

<p><strong>Principle</strong>: Validation must come from outside the reasoning system itself <a class="citation" href="#ref-17">[17]</a><a class="citation" href="#ref-18">[18]</a>.</p>
</div>

<p><strong>Examples</strong>:</p>
<ul>
<li>Code linting and compilation</li>
<li>Rule-tracking systems</li>
<li>Formal plan verification <a class="citation" href="#ref-19">[19]</a></li>
<li>World-model validation</li>
<li>External test suites</li>
</ul>
<p><strong>Why this matters</strong>: Validation signals allow agents to course-correct <a class="citation" href="#ref-20">[20]</a>. Crucially, these constraints should stem from mechanisms OUTSIDE the agent to form genuine boundaries.</p>
<details class="counterpoints">
<summary>Counterpoints & Discussion</summary>
<div class="counterpoints-body">
<p>Recent work challenges the strict necessity of external validation. Kumar et al. <a class="citation" href="#ref-21">[21]</a> achieve significant gains on MATH benchmarks through self-correction via reinforcement learning without external feedback. Intrinsic self-critique methods <a class="citation" href="#ref-73">[73]</a> have achieved strong results on planning benchmarks without external verifiers. However, the consensus remains that external feedback is more reliable for production systems <a class="citation" href="#ref-17">[17]</a><a class="citation" href="#ref-18">[18]</a>.</p>
<div class="author-response">
<p>Self-correction is real and improving, but the economy of agents still favors external validation. For many problems, validation is cheaper than inference—external systems (compilers, linters, test suites, rule engines) can verify correctness at a fraction of the cost of having the model self-assess. When validation is cheap and inference is expensive, it pays to delegate the checking to purpose-built external mechanisms.</p>
</div>
</div>
</details>

<h4 id="tools-principle">Tools Enable Hierarchical Reasoning</h4>
<p>Structure, concretely applied, takes the form of tools—pre-built abstractions that encapsulate solved problems <a class="citation" href="#ref-22">[22]</a><a class="citation" href="#ref-23">[23]</a>.</p>
<div class="principle">

<p><strong>Principle</strong>: Delegate reasoning to higher-level abstractions through tools.</p>
</div>

<p>Even when a model <em>could</em> solve a task without tools, well-designed tools provide:</p>
<ol>
<li><strong>Hierarchical reasoning</strong> → No need to redo solved problems</li>
<li><strong>Predictable outputs</strong> → Pre-validated, well-understood behavior</li>
<li><strong>Battle-tested reliability</strong> → Hardened by previous use cases</li>
</ol>
<details class="counterpoints">
<summary>Counterpoints & Discussion</summary>
<div class="counterpoints-body">
<p>Tam et al. <a class="citation" href="#ref-15">[15]</a> show that strict format constraints can degrade LLM reasoning performance—models forced into rigid output schemas sometimes sacrifice accuracy. Park et al. <a class="citation" href="#ref-16">[16]</a> demonstrate that grammar-constrained decoding distorts probability distributions. Banerjee et al. <a class="citation" href="#ref-71">[71]</a> propose alternating constrained and unconstrained generation as a middle path. More broadly, the Bitter Lesson argument <a class="citation" href="#ref-72">[72]</a> suggests that hand-crafted structure will eventually be superseded by scaling compute and data.</p>
<div class="author-response">
<p>I have empirically verified that strict format constraints degrade reasoning in standard LLMs. However, reasoning models and inference-time scaling have in my view largely solved this issue—the performance penalty from structured output has shrunk significantly with recent model generations. As for the Bitter Lesson—see Section 2.4, Harness Obsolescence for the full argument, but the short version is: structure competes on *economy*, not *capability*. Latency, cost, and predictability are orthogonal to what a model can theoretically derive from scratch.</p>
</div>
</div>
</details>

<details class="practical-example">
<summary>Practical Example — Hierarchical Reasoning Done Right: Entity Semantics Search</summary>
<div class="practical-example-body">

<p>At <a href="https://kuona.ai">kuona.ai</a>, agents need to search across user catalogs, ontologies, and domain-specific entities — product hierarchies, promotional categories, store groupings — to answer business questions. A naive approach would have the agent reason over raw catalog data at query time, parsing thousands of entries to find semantically relevant entities. This is exactly the kind of solved problem that tools should encapsulate.</p>

<p>Instead, we built a <strong>decoupled semantic indexing system</strong>: an asynchronous agent that pre-processes user catalogs and ontologies into task-specific semantic databases. This agent runs offline, building structured indexes that capture entity relationships, synonyms, hierarchical groupings, and domain semantics. The main agent never sees the raw catalog data — it calls a search tool that queries these pre-indexed structures.</p>

<p>The search tool itself calls a workflow underneath — an agentic search over the semantic databases — but from the main agent's perspective, it is a single tool call: <em>"find entities matching this concept."</em> The hard reasoning (understanding what "premium beverages" means across a retailer's specific product taxonomy) has been solved offline and baked into the index. The main agent reasons at a higher level: which entities to search for, how to combine results, and how to use them in downstream queries.</p>

<p><strong>Why this works as hierarchical reasoning</strong>: The semantic indexing agent solves entity understanding once. The search tool encapsulates that solution. The main agent delegates a narrow, well-defined task — semantic entity lookup — to a pre-built abstraction, and gets back structured, validated results. Without this tool, the agent would need to reason over raw data on every query, multiplying both latency and error surface. With it, the agent operates on pre-solved semantic understanding, focusing its reasoning capacity on the actual business question rather than on entity resolution.</p>

</div>
</details>

<details class="practical-example">
<summary>Practical Example — When Tools Are Missing: The LIDA Visualization Trap</summary>
<div class="practical-example-body">

<p>An earlier <a href="https://kuona.ai">kuona.ai</a> system attempted autonomous visualization generation inspired by Microsoft Research's LIDA <a class="citation" href="#ref-96">[96]</a>. The initial vision was full autonomy: a user asks a question, the system produces a correct visualization without human involvement, using LLM-driven code generation to produce arbitrary plots.</p>

<p>The system worked end-to-end, but it was unreliable. Models generated syntactically valid plotting code that made poor visualization choices — mishandling outliers, selecting inappropriate chart types, producing unreadable scales. The initial assumption was that better prompts or larger models would fix these surface-level issues. That assumption was wrong. The problem was architectural: arbitrary code generation for visualizations required <em>visual reasoning</em> that language models fundamentally lacked. No amount of prompt engineering could bridge that gap.</p>

<p><strong>The fix was to introduce tools</strong>. Instead of generating free-form plotting code (reasoning from scratch every time), the system was redesigned around a curated set of <strong>visualization templates with well-defined configurations</strong>. The agent's role shifted from "generate anything" to "route and parameterize the right visualization." These templates were the missing hierarchical abstraction — pre-built tools that encapsulated solved visualization problems (how to handle outliers, how to scale axes, how to label series).</p>

<p><strong>What was lost</strong>: fine-grained control over custom plots, the ability to add arbitrary annotations, and any visualization outside the template library. <strong>What was gained</strong>: accuracy went from ~70% to ~95%, failures became predictable, and every visualization followed consistent design principles — a unified output language that emerged precisely <em>because</em> the scope was constrained. A second tradeoff followed: rather than adding retry loops to chase the last 5%, the interface was redesigned to give users direct control when a visualization was flagged as low quality. What felt like admitting defeat on the autonomous vision became the feature users valued most — predictable reliability with escape hatches.</p>

<p><strong>The lesson for hierarchical reasoning</strong>: The original system asked the agent to reason from first principles about visual design on every invocation. The fix was to package that reasoning into tools — constrained, validated, battle-tested abstractions. The agent stopped solving visualization problems and started <em>selecting</em> pre-solved solutions. Reliability came not from improving the model's raw capabilities, but from redefining the task boundary so the agent reasoned at the right level of abstraction.</p>

</div>
</details>

<p>Structure also carries implicit alignment—a point developed fully in Section 3.9.</p>
<h3 id="minimize-context">2.2 Context Engineering</h3>
<p>Structure tells you what&#39;s relevant and what isn&#39;t—which leads to the operational principle of deliberately managing what enters the context window.</p>
<p>The field has converged on the term <strong>context engineering</strong> <a class="citation" href="#ref-30">[30]</a> to describe this practice, with substantial recent treatment by Anthropic <a class="citation" href="#ref-90">[90]</a>, Martin <a class="citation" href="#ref-94">[94]</a>, and others. The discipline encompasses designing the full informational environment an agent operates within—not just what&#39;s in the prompt, but what&#39;s retrieved, when it&#39;s loaded, how it&#39;s structured, and what&#39;s excluded. Context engineering subsumes prompt engineering the same way software architecture subsumes writing functions. The principles below are well-established in this literature; our contribution is connecting them to the dependency chain—showing how context engineering follows from structure (Section 2.1) and feeds into the cost model (Section 2.3).</p>
<div class="principle">

<p><strong>Principle</strong>: The context window is the agent&#39;s working memory. Structure it deliberately <a class="citation" href="#ref-25">[25]</a><a class="citation" href="#ref-26">[26]</a><a class="citation" href="#ref-27">[27]</a><a class="citation" href="#ref-28">[28]</a>.</p>
</div>

<p>This is where the structure principle (Section 2.1) becomes operational. Structure tells you what information is relevant for a given task, which boundaries separate concerns, and which contexts belong to which agents. Without structure, context engineering degenerates into guesswork—stuffing tokens and hoping the model finds what it needs.</p>
<p><strong>What context engineering controls</strong>:</p>
<ul>
<li><strong>Selection</strong> → What information enters the context window (and what doesn&#39;t)</li>
<li><strong>Ordering</strong> → How information is sequenced (recency, relevance, dependency)</li>
<li><strong>Representation</strong> → How information is formatted (raw text, summaries, structured schemas)</li>
<li><strong>Timing</strong> → When information is loaded (upfront, on-demand, forced)</li>
</ul>
<p>Research consistently shows that context quality matters more than context quantity. Irrelevant context actively degrades performance <a class="citation" href="#ref-26">[26]</a>, length alone hurts even with perfect retrieval <a class="citation" href="#ref-27">[27]</a>, and models exhibit predictable attention patterns that structured context can exploit <a class="citation" href="#ref-25">[25]</a>.</p>
<h4 id="progressive">Progressive Disclosure</h4>
<p>The primary mechanism for effective context engineering: don&#39;t load everything upfront—reveal information incrementally as needed <a class="citation" href="#ref-31">[31]</a><a class="citation" href="#ref-32">[32]</a>.</p>
<div class="principle">

<p><strong>Principle</strong>: Reveal information incrementally as needed.</p>
</div>

<p>This prevents context overload while maintaining access to necessary information. The parallel to cognitive science is striking: Miller&#39;s &quot;Magical Number Seven&quot; <a class="citation" href="#ref-33">[33]</a> and Baddeley&#39;s working memory model <a class="citation" href="#ref-34">[34]</a> both suggest that human cognition benefits from bounded information flow. Packer et al. <a class="citation" href="#ref-35">[35]</a> explore analogous memory management for LLMs in MemGPT.</p>
<p>Progressive disclosure is not the same as minimizing context. Sometimes the right context engineering decision is to load <em>more</em>—context stuffing remains highly effective when the corpus fits and is relevant. Progressive disclosure is the general pattern; context stuffing is the trivial special case where everything fits and everything is relevant.</p>
<details class="practical-example">
<summary>Practical Example — From Context Stuffing to Progressive Disclosure</summary>
<div class="practical-example-body">

<p>The same kuona.ai system from Section 1.2 illustrates how context strategies evolve—not because of context window limits, but because of compositional demands. The system is an NLP-to-SQL agent that performs promotional postmortem analysis: a user asks a business question, and the agent translates it into SQL queries against the company's data warehouse.</p>

<p><img src="assets/agents/image11-context-evolution.svg" alt="Context Strategy Evolution: From Stuffing to Structure"></p>
<p><strong>Stage 1: Context stuffing (worked well).</strong> The initial version used a single ~50k token knowledgebase file that mixed domain knowledge (schema definitions, business rules, metric calculations) with task knowledge (query patterns, SQL dialect conventions, common joins). Context stuffing worked because the domain was narrow and bounded—promotional analysis only. The corpus fit, everything was relevant. This is the trivial special case.</p>

<p><strong>Stage 2: RAG (degraded performance).</strong> When exploring optimization, RAG was tried. It introduced latency, sometimes missed relevant tables or columns (low recall), and sometimes injected semi-relevant but confusing context (noise). Maximizing recall recovered performance and achieved some savings over stuffing everything, but the fundamental issue remained: the retrieval mechanism couldn't distinguish what was <em>structurally</em> relevant from what was <em>semantically</em> similar. A table's schema definition might not be semantically close to the user's question, but it's structurally essential for writing a correct query.</p>

<p><strong>Stage 3: Structured contexts (enabled scaling).</strong> Scaling from promotional analysis to general business use cases forced the real solution. The monolithic knowledgebase got split into structured contexts with different treatment:</p>

<ul>
<li><strong>Always-loaded context</strong>: Core DB query patterns and system conventions—how queries must be built, SQL dialect rules, output formatting. Loaded on every invocation.</li>
<li><strong>Domain contexts</strong>: Per-domain knowledge (promotional metrics vs. inventory vs. finance)—loaded based on task routing, so the agent gets the right business framing for its current task.</li>
<li><strong>Queryable context</strong>: Domain-specific schema details—retrieved on demand when the agent needs to inspect a table's structure or understand a column's semantics.</li>
</ul>

<p>This is progressive disclosure in practice: the agent always has the foundation, gets the domain framing for its current task, and can reach for details as needed.</p>

<p>The evolution from stuffing to structure wasn't driven by context window limits. The 50k tokens still fit. It was driven by the need for <em>composability</em>—the same agent serving multiple domains required the context to be <em>engineered</em>, not just <em>loaded</em>.</p>

</div>
</details>

<details class="counterpoints">
<summary>Counterpoints & Discussion</summary>
<div class="counterpoints-body">
<p>Long-context models consistently outperform RAG in average performance when sufficient compute is available <a class="citation" href="#ref-29">[29]</a>. Databricks (2024) shows that corpora under 2M tokens can be fed directly without retrieval. This suggests that aggressive context minimization is often counterproductive—more context, properly structured, yields better results than aggressive filtering. The real skill is knowing what to include, not how little you can get away with.</p>
<div class="author-response">
<p>I agree, and maximizing recall remains incredibly useful in some domains—particularly narrow, well-scoped tasks where the relevant corpus fits in context and the agent's job is bounded. But for general-purpose and flexible agents, defaulting to maximum recall is a lower-quality design. It works until it doesn't: the moment the corpus exceeds the context window, or the task requires navigating across domains, the agent has no mechanism for deciding what matters. This is exactly why I frame the principle as context *engineering* rather than context *minimization*. The goal is deliberate structure, not minimal tokens. When everything fits, load everything. When it doesn't—which is the common case for agents operating over large codebases, document corpora, or multi-session histories—progressive disclosure provides the mechanism for deciding what to load and when. The principle is about intentionality, not austerity. The connection to agentic maturity (Section 1.2) is worth noting: the lower on the evolution curve a task sits (bespoke prompts, simple workflows), the more context stuffing dominates. As agents take on more flexible, multi-step tasks, deliberate context management becomes essential.</p>
</div>
</div>
</details>

<h3 id="unconstrained">2.3 The Cost of Unconstrained Systems</h3>
<p>The existing literature is clear on <em>what</em> structure to apply but largely silent on <em>when</em> the investment is justified. Anthropic advises &quot;find the simplest solution possible&quot; <a class="citation" href="#ref-3">[3]</a>; OpenAI recommends &quot;an incremental approach&quot; <a class="citation" href="#ref-9">[9]</a>. These are useful heuristics but not decision frameworks. This section proposes a formal cost model for reasoning about the structure investment—arguably the most distinctive contribution of this paper.</p>
<p>What happens when you ignore structure? A fully unconstrained agent (no structure, no verification, no premade tools, no limitations) <em>might</em> solve a problem, but at what cost? To reason about this clearly, it helps to think in terms of two competing cost functions.</p>
<h4 id="two-cost-functions">The Two Cost Functions</h4>
<p>Every agentic system implicitly makes a tradeoff between two costs:</p>
<p><strong>Cost of Structure (C_s)</strong>: The upfront and ongoing investment in constraints.</p>
<ul>
<li><strong>Design cost</strong> → Engineering time to define tools, schemas, validation rules</li>
<li><strong>Maintenance cost</strong> → Keeping structure updated as requirements evolve</li>
<li><strong>Flexibility cost</strong> → Tasks that don&#39;t fit the structure require workarounds</li>
<li><strong>Runtime overhead</strong> → Validation checks, schema enforcement, plan verification</li>
</ul>
<p><strong>Cost of Failure (C_f)</strong>: The expected cost when things go wrong.</p>
<ul>
<li><strong>Direct cost</strong> → Wasted tokens, API calls, compute on failed paths</li>
<li><strong>Recovery cost</strong> → Replanning, rollback, state repair, human intervention</li>
<li><strong>Propagation cost</strong> → Downstream errors caused by upstream failures</li>
<li><strong>Irreversibility cost</strong> → Actions that cannot be undone (emails sent, data deleted, state corrupted)</li>
</ul>
<p>Structure is economically justified when the reduction in expected failure cost exceeds the cost of imposing it: <strong>C_s &lt; ΔC_f × P(failure)</strong>. This is the economic argument for why structure motivates the rest of this paper.</p>
<h4 id="error-compounding">Error Compounding in Multi-Step Tasks</h4>
<p>The most powerful argument for structure is probabilistic. In a multi-step task, per-step reliability compounds multiplicatively:</p>
<table>
<thead>
<tr>
<th>Per-step success rate</th>
<th>5 steps</th>
<th>10 steps</th>
<th>20 steps</th>
</tr>
</thead>
<tbody><tr>
<td>90% (no structure)</td>
<td>59%</td>
<td>35%</td>
<td>12%</td>
</tr>
<tr>
<td>95% (light structure)</td>
<td>77%</td>
<td>60%</td>
<td>36%</td>
</tr>
<tr>
<td>99% (heavy structure)</td>
<td>95%</td>
<td>90%</td>
<td>82%</td>
</tr>
</tbody></table>
<p>A 5-percentage-point improvement in per-step reliability (90% → 95%) nearly doubles the success rate over 10 steps. This is why structure matters most for long-horizon tasks—even small reliability gains at each step have dramatic compound effects.</p>
<p><strong>Corollary</strong>: The longer the task horizon, the more structure is justified, because the compounding penalty for per-step unreliability grows exponentially. This also connects to the redundancy principle (Section 2.5, Redundancy): having multiple paths to success at each step is one mechanism for increasing per-step reliability.</p>
<h4 id="validation-asymmetry">The Validation Cost Asymmetry</h4>
<p>A key economic property of agentic systems: <strong>validation is almost always cheaper than inference</strong>.</p>
<table>
<thead>
<tr>
<th>Operation</th>
<th>Token cost</th>
<th>Latency</th>
<th>Reliability</th>
</tr>
</thead>
<tbody><tr>
<td>Compiler check</td>
<td>0 tokens</td>
<td>~ms</td>
<td>Deterministic</td>
</tr>
<tr>
<td>Linter/formatter</td>
<td>0 tokens</td>
<td>~ms</td>
<td>Deterministic</td>
</tr>
<tr>
<td>Test suite</td>
<td>0 tokens</td>
<td>~seconds</td>
<td>Deterministic</td>
</tr>
<tr>
<td>Schema validation</td>
<td>0 tokens</td>
<td>~ms</td>
<td>Deterministic</td>
</tr>
<tr>
<td>Model self-assessment</td>
<td>100s–1000s tokens</td>
<td>~seconds</td>
<td>Probabilistic</td>
</tr>
<tr>
<td>Model re-reasoning</td>
<td>1000s+ tokens</td>
<td>~seconds</td>
<td>Probabilistic</td>
</tr>
</tbody></table>
<p>When external validation is available, it dominates self-assessment on every dimension: cheaper, faster, and more reliable. This asymmetry is what makes the external validation principle (Section 2.1, External Validation) not just theoretically sound but economically compelling—delegating verification to purpose-built external mechanisms is almost always the right trade.</p>
<h4>When Structure Doesn&#39;t Pay</h4>
<p>Structure is not always justified. The cost model identifies conditions where flexibility wins:</p>
<ol>
<li><strong>One-off tasks</strong> → C_s cannot be amortized across multiple executions</li>
<li><strong>High novelty</strong> → The task space is too unpredictable to constrain usefully</li>
<li><strong>Cheap failure</strong> → C_f is low (reversible actions, no downstream consequences, low token cost)</li>
<li><strong>Rapidly evolving domains</strong> → Maintenance cost of structure exceeds its benefits</li>
</ol>
<p>This connects directly to the Flexibility-Reliability Tradeoff (Section 2.4): the maturity curve is really a curve of amortized structure cost declining as task frequency and predictability increase.</p>
<details class="counterpoints">
<summary>Counterpoints & Discussion</summary>
<div class="counterpoints-body">
<p>The compounding reliability argument assumes step independence—that per-step success rates are uncorrelated. In practice, failures often cluster: a single misunderstanding early in a task can cascade into correlated failures across all subsequent steps, making the effective compounding worse than the independent model predicts. Conversely, agents that self-correct can exhibit positive correlation where early successes build context that improves later steps. The simple multiplicative model is useful as a mental framework but may over- or underestimate actual failure rates depending on the domain.</p>
<div class="author-response">
<p>Agreed. The table is a simplification—real tasks have correlated steps. The point is directional: per-step reliability improvements have outsized effects on long-horizon outcomes, and the independent model provides a useful lower bound for reasoning about where to invest in structure. In my experience, the clustering effect makes the case for structure stronger, not weaker—a single structural guardrail at the right choke point can break a failure cascade that would otherwise propagate across many steps.</p>
</div>
</div>
</details>

<details class="practical-example">
<summary>Practical Example — What You Optimize For Changes Everything</summary>
<div class="practical-example-body">

<p>The cost model above is deliberately abstract — C_s and C_f are expressed in generic "cost" without specifying <em>what</em> that cost measures. This is intentional: the dominant cost dimension varies across systems, and the structural decisions that follow from the model change accordingly. The framework is a general heuristic; the instantiation is domain-specific.</p>

<p><strong>When latency is the cost function.</strong> At <a href="https://kuona.ai">kuona.ai</a>, latency is one of the north-star metrics — particularly for certain tasks and subagents in the interactive query pipeline, where the system serves business analysts who ask questions and expect answers in seconds, not minutes. (Correctness is an even higher priority — see below — but for the latency-sensitive surfaces, speed dominates the structural decisions.) In this regime, C_f is measured primarily in <em>time</em>: a failed step triggers a retry loop, each retry adds seconds of model inference, and users abandon queries that take too long. Structure is justified when it eliminates retry cycles — a validation check that takes 5ms and prevents a 3-second retry loop is an obvious win. This shaped every architectural decision: pre-computed semantic indexes (Section 2.1, Tools) exist because entity resolution at query time added unacceptable latency, visualization templates replaced code generation because retrying failed plots was too slow, and context was structured by change rate (Section 3.1) because KV cache reuse cut per-turn inference time. The cost model didn't say "add structure everywhere" — it said "add structure wherever it eliminates a latency-visible failure mode."</p>

<p><strong>When inference cost is the cost function.</strong> A different system — a batch processing pipeline that runs overnight, classifying and routing thousands of documents — operates under a completely different C. Latency is irrelevant; the pipeline has hours. But every token costs money, and at scale, wasted tokens dominate the budget. Here, C_f is measured in <em>dollars</em>: a failed classification triggers a re-inference on a document that might be 50k tokens, and propagation cost means a misclassification early in the pipeline causes expensive downstream reprocessing. Structure is justified when it prevents token waste — a lightweight classifier that routes documents to specialized prompts (cheaper, shorter) instead of feeding everything to a single expensive prompt, or a schema validator that catches malformed output before it propagates to the next stage. Retry loops are acceptable (time is cheap); token-burning retry loops are not.</p>

<p><strong>When correctness is the cost function.</strong> At <a href="https://kuona.ai">kuona.ai</a>, after the query system retrieves data, a separate system answers user questions grounded on those results. In this regime, C_f is measured in <em>business risk</em>: a single hallucination or misread data point in an answer can drive decisions worth millions of dollars, and the error may take months to surface. A business analyst who trusts an AI-generated insight about promotional effectiveness, inventory allocation, or pricing strategy acts on it immediately — and by the time the mistake is discovered, the downstream damage is already done. This is why the system was built around a strict grounding architecture: every claim in a user-facing answer must trace back to retrieved data, every numerical assertion is cross-validated against the query results, and the system refuses to speculate beyond what the data supports. Correctness in the answer-crafting layer is the north-star metric — not latency, not token cost, but whether the answer faithfully represents the underlying data. The cost model says "C_s is almost always less than C_f" because C_f is catastrophic and delayed. The "When Structure Doesn't Pay" conditions rarely apply — even one-off analyses need grounding structure because cheap failure doesn't exist when the wrong number can redirect a client's strategy.</p>

<p><strong>C can vary within the same system.</strong> At <a href="https://kuona.ai">kuona.ai</a>, the query system is multi-tenant across different database backends. When the agent queries cloud data warehouses and big data infrastructure, failures are expensive: queries take seconds to minutes, consume compute resources, and a malformed query against a production warehouse can time out, lock resources, or return misleading partial results. In this regime, strict query planning and validation paid for themselves immediately — a planning layer that validated query structure, checked schema compatibility, and verified join paths <em>before</em> execution prevented costly round-trips to remote infrastructure. But the same system also handles small datasets that fit in the agent's local environment via DuckDB. Here, the entire cost calculus flips: queries execute in milliseconds against an in-process database, failures are instantly visible and instantly retryable, and there are no remote resources to waste. Errors are cheap. For DuckDB queries, the planning layer was dropped entirely — the agent generates and executes queries directly, iterating on failures at negligible cost. The same system, the same agent, two completely different structural decisions — because C_f for a failed cloud warehouse query and C_f for a failed DuckDB query differ by orders of magnitude.</p>

<p><strong>The general heuristic</strong>: Before applying the cost model, identify what your system's C actually measures. The same architectural decision — adding a validation checkpoint, pre-computing an index, constraining output format — can be clearly justified, clearly wasteful, or somewhere in between, depending entirely on which cost dimension dominates. The model is universal; the parameterization is not.</p>

</div>
</details>

<p>The conditions above—when structure pays and when it doesn&#39;t—are not binary. They form a spectrum, which the following section formalizes.</p>
<h3 id="tradeoff">2.4 The Flexibility-Reliability Tradeoff</h3>
<p><img src="assets/agents/image3-flexibility-reliability.svg" alt="Flexibility-Reliability Tradeoff"></p>
<p>Any process an agent works on exists on a maturity curve <a class="citation" href="#ref-3">[3]</a><a class="citation" href="#ref-36">[36]</a><a class="citation" href="#ref-37">[37]</a>:</p>
<ul>
<li><strong>One-off tasks</strong> → Require flexibility and exploration</li>
<li><strong>Mature processes</strong> → Benefit from rigidity, predictability, and structure</li>
</ul>
<p>While some agents work well as completely flexible, <strong>reliability and low latency come from structure</strong>. Agents built with these structural harnesses—structured agents—occupy the upper-right of this space: high flexibility through LLM reasoning, high reliability through external constraints.</p>
<p><strong>Key insight</strong>: Even with increased agentic capabilities, reducing the problem space and &quot;degrees of freedom&quot; leads to reduced latencies and improved reliability. The structured agency approach is fundamentally about moving agents upward on this chart by adding harnesses rather than restricting what they can do.</p>
<h4 id="harness-obsolescence">Designing for Harness Obsolescence</h4>
<p>The flexibility-reliability tradeoff has a temporal dimension that is central to how structured agency should be practiced: <strong>build harnesses expecting them to weaken over time.</strong></p>
<p>The Bitter Lesson has been applied directly to agent scaffolding by Martin <a class="citation" href="#ref-93">[93]</a>, Bowne-Anderson <a class="citation" href="#ref-95">[95]</a>, and others, with the practical observation that agent harnesses must be &quot;repeatedly simplified as models improve&quot;—that teams at Anthropic and elsewhere rebuild their agent infrastructure on the order of months, not years. We agree with this framing and formalize it as a design principle: harness obsolescence is not a failure of design but a <em>feature</em> of well-designed systems.</p>
<p>Current harnesses address current model limitations. State management compensates for the fact that models lack persistent memory across invocations. Context-window-as-working-memory harnesses (progressive disclosure, forced contexts, retrieval mechanisms) exist because attention doesn&#39;t scale perfectly and context is expensive. Structured output schemas exist because models don&#39;t always produce well-formed output. Validation signals exist because models can&#39;t yet reliably self-verify. Each of these is a response to a specific limitation—and each limitation is on a trajectory of improvement.</p>
<p>As models improve, some harnesses will naturally weaken. A model with perfect self-verification needs fewer external validation signals. A model with reliable structured output needs less schema enforcement. A model with effectively infinite context and perfect attention needs less aggressive progressive disclosure. New architectures may overcome the need for external memory entirely, collapsing the state management harness into the model itself. The Bitter Lesson predicts exactly this trajectory: capabilities that were once impossible without external scaffolding get absorbed into the model itself.</p>
<p>But even as harnesses weaken, <strong>domain-specific structured state remains economical</strong>. The latency cost of having a model re-derive a customer&#39;s order history from raw conversation logs—every invocation—will always exceed the cost of maintaining that information in a structured state object, regardless of how capable the model becomes. The cognitive cost (in tokens) of navigating an unstructured environment will always exceed the cost of navigating one where the structure is pre-computed. This is the same argument that applies to software engineering generally: we don&#39;t rewrite parsers from first principles every time we need to parse JSON, even though we <em>could</em>. Structured agency&#39;s harnesses occupy this same economic niche—they are amortized investments in structure that pay dividends across invocations. The right design principle, then, is not &quot;build permanent harnesses&quot; but &quot;build harnesses with a clear understanding of which limitations they address and how those limitations are evolving.&quot; This is the temporal dimension of the flexibility-reliability tradeoff: the optimal point on the curve shifts over time as model capabilities advance, and well-designed structured agents shift with it.</p>
<h4 id="commoditizing-intelligence">Commoditizing Intelligence</h4>
<p>Section 2.4, Harness Obsolescence argues that the harness should survive model <em>improvement</em>—the temporal axis. There is a second axis: the harness should survive model <em>substitution</em>.</p>
<div class="principle">

<p><strong>Principle</strong>: Treat the model as a replaceable commodity. The harness is the stable layer; the intelligence provider is a variable.</p>
</div>

<p>The economic realities of model serving make this more than an architectural nicety. Providers compete on price, latency, and capability across different task profiles. A model that is cost-optimal for structured extraction may not be the best choice for open-ended reasoning. A harness that abstracts over the intelligence provider can route tasks to the appropriate model—or simply to the cheapest one that meets the reliability threshold. This is the same economic logic that drives cloud infrastructure: you don&#39;t build your application around a specific EC2 instance.</p>
<p>This connects directly to the cost model (Section 2.3): different models align with different cost functions. A task where C_f is measured in <em>risk</em> — the grounded answer system from Section 2.3 — justifies routing to the most capable (and expensive) model, because the cost of a hallucination dwarfs the inference premium. A task where C_f is measured in <em>latency</em> — entity resolution at query time — favors the fastest model that meets the accuracy threshold, even if a more capable model would be marginally better. A task where C_f is measured in <em>dollars</em> — high-volume classification or routing — favors the cheapest model that clears the reliability bar. Model commoditization is not just about substitution resilience; it is about <strong>matching intelligence cost to task risk</strong>. The harness that treats models as interchangeable also enables the harness that routes each task to the model whose cost profile best fits its C.</p>
<p>The robustness argument is equally compelling. Provider outages, rate limits, and deprecations are operational realities. A harness designed around model interchangeability treats these as transient conditions rather than architectural crises—if one provider fails, the system falls back without adaptation.</p>
<p>This principle extends down to the implementation level. Frameworks like PydanticAI, LangChain, and similar orchestration layers earn their value not primarily through abstraction but through the operational niceties that make model commoditization practical:</p>
<ul>
<li><strong>Retry loops with exponential backoff</strong> for transient provider errors (rate limits, timeouts, 5xx responses)</li>
<li><strong>Structured output validation with automatic retries</strong> — when the model produces malformed JSON given a schema constraint, the framework re-prompts rather than failing. This is a harness that compensates for a model limitation (unreliable structured output), exactly as Section 2.4, Harness Obsolescence describes.</li>
<li><strong>Provider-agnostic interfaces</strong> that allow swapping the underlying model without changing application code</li>
<li><strong>Streaming, token counting, and cost tracking</strong> as cross-cutting concerns independent of which model is being called</li>
</ul>
<p>Importantly, benefiting from these niceties does not require buying into any framework&#39;s full ecosystem or opinionated agent architecture. The retry loops, structured output validation, and provider abstraction are composable building blocks—you can use them to construct your own harnesses without adopting the framework&#39;s opinions about how agents should be structured. The value is in the operational primitives, not the orchestration philosophy.</p>
<p>The key insight is that these primitives are themselves harnesses in the structured agency sense. They sit between your application logic and the intelligence provider, adding structure (retries, validation, provider abstraction) that makes the system more reliable. The model produces the intelligence; the harness ensures that intelligence is delivered reliably, affordably, and in the right format.</p>
<details class="counterpoints">
<summary>Counterpoints & Discussion</summary>
<div class="counterpoints-body">
<p>Model-agnostic design can paper over meaningful capability differences. Not all models handle tool calling, structured output, or multi-turn reasoning equally—abstracting over these differences can lead to lowest-common-denominator designs that fail to exploit what a specific model does well. Some architectures (like Claude's extended thinking or OpenAI's o-series reasoning) offer unique capabilities that a generic interface cannot expose.</p>
<div class="author-response">
<p>This is a real tension, and I don't advocate for pure abstraction at the cost of capability. The principle is about the <em>default posture</em>: design the harness so that the model is replaceable <em>unless</em> a specific capability justifies the coupling. Provider-specific features should be opt-in, not baked into the architecture. In practice, I've found that 80–90% of agent interactions are standard enough to be provider-agnostic, while a small number of critical steps (complex reasoning, long-context synthesis) may warrant model-specific routing. The harness should support both: commodity intelligence for the common case, specialized intelligence where it matters.</p>
</div>
</div>
</details>

<h3 id="corollaries">2.5 Practical Corollaries</h3>
<h4 id="redundancy">Redundancy is a Feature</h4>
<div class="principle">

<p><strong>Principle</strong>: Have multiple ways to accomplish the same thing <a class="citation" href="#ref-38">[38]</a><a class="citation" href="#ref-39">[39]</a><a class="citation" href="#ref-40">[40]</a>.</p>
</div>

<p><strong>Rationale</strong>: From a probabilistic argument, having multiple paths to success increases the likelihood of task completion. Relying on a single tool (unless it&#39;s ubiquitous) creates fragile systems.</p>
<p><strong>Examples</strong>:</p>
<ul>
<li>Multiple retrieval mechanisms (grep, semantic search, full-text search)</li>
<li>Multiple communication channels</li>
<li>Fallback tools when primary tools fail</li>
</ul>
<h4 id="stress-strengthens">Stress Strengthens a System</h4>
<div class="principle">

<p><strong>Principle</strong>: Reuse agents and components across contexts. Shared stress surfaces harden them.</p>
</div>

<p><strong>Rationale</strong>: A component used in one context is fragile — its edge cases are discovered only within that context. A component reused across many contexts is stressed from multiple directions, forcing it to become more general, more robust, and more predictable. Stress — the pressure of diverse usage patterns — strengthens a system the same way load testing strengthens infrastructure.</p>
<details class="practical-example">
<summary>Practical Example — From Monolithic Agents to Reusable Cores</summary>
<div class="practical-example-body">

<p>At <a href="https://kuona.ai">kuona.ai</a>, an early discovery shaped the entire architecture: designing monolithic agents with all desired capabilities baked in led to brittle code. When agent B needed a capability that lived inside agent A, the result was either duplication (rewriting the capability) or tight coupling (calling into agent A's internals). Both paths created fragility — a change to agent A's prompts or tool configuration would silently break agent B's dependency.</p>

<p>The solution was to structure from the beginning so that the same agents and the same prompts would be reused across as many contexts as possible. A single query agent, with the same core prompt and tool set, serves promotional analysis, inventory questions, and financial reporting. Each new domain that stresses the agent exposes edge cases that, once fixed, benefit every other domain. The agent got <em>stronger</em> as it was reused — more general, fewer blind spots, more predictable failure modes.</p>

<p><strong>A subtlety in how this was achieved</strong>: the alternative to reuse is decomposition — splitting capabilities into separate specialized modules. For instance, SQL generation could live in a dedicated subagent that the main agent delegates to. But this creates a more brittle system with more dependencies: the main agent must know when to delegate, the subagent must understand the main agent's context, and the interface between them becomes a failure surface. Instead, the main agent was designed to be <em>reconfigurable</em> — its tool set and directives can be narrowed per invocation. When only SQL generation is needed, the agent is invoked with the SQL tool forced and a directive to return after generating the query. Same agent, same prompt, same battle-tested reasoning — just a narrower scope. This is reuse through reconfiguration, not decomposition through delegation.</p>

<p>These lessons — that reuse hardens components, that monoliths are brittle, and that reconfiguration beats decomposition — heavily influence the formalisms in the taxonomy that follows. The distinction between invocation-level configuration (Section 3.1) and agent identity, the treatment of tools as state that can be added or removed per invocation (Section 3.2), and the composition patterns (Section 3.5) all trace back to this practical discovery.</p>

</div>
</details>

<h4 id="harness-awareness">The Agent Should Know Its Harness</h4>
<div class="principle">

<p><strong>Principle</strong>: Design the harness so the agent can introspect it, reason about it, and use it to amplify its own capabilities — not merely be constrained by it.</p>
</div>

<p><strong>Rationale</strong>: The default framing of structure is restrictive — guardrails, constraints, boundaries, limitations. This framing is incomplete. A well-designed harness is not a cage; it is an exoskeleton. The agent that is <em>aware</em> of its tools, its context boundaries, its validation signals, and its delegation paths can reason about them as resources, not just obey them as rules. The shift is from structure-as-restriction to <strong>structure-as-capability</strong>.</p>
<p>An agent that knows it has a schema validator can proactively check its own output before submitting it. An agent that knows its context is structured by change rate can decide what to offload and what to keep. An agent that knows it can delegate to a scoped subagent can choose delegation strategically rather than having it imposed. An agent that knows its working memory has been compacted can recognize when it is operating on incomplete information and reach for addressable history. In each case, the harness becomes something the agent <em>uses</em>, not something that <em>uses</em> the agent.</p>
<p><strong>Design implications</strong>:</p>
<ul>
<li><strong>Make structure visible</strong> — Tool descriptions, available contexts, delegation capabilities, and validation mechanisms should be legible in the agent's working memory. The agent should know what it has access to, not discover it by accident.</li>
<li><strong>Make structure introspectable</strong> — The agent should be able to query its own harness: what tools are available, what contexts are loaded, what constraints are active, what has been compacted. Introspection turns the harness from an external imposition into an internal resource.</li>
<li><strong>Design for leverage, not just compliance</strong> — When adding a structural element, ask not just "does this prevent the agent from failing?" but "does this enable the agent to succeed in ways it couldn't without it?" The best harnesses do both.</li>
</ul>
<p>This principle connects back to the clarification at the opening of this section: structure is not the old-style rigid workflow that constrains the agent into a fixed path. It is the set of capabilities, boundaries, and information architecture that the agent can leverage to reason better, act faster, and recover from errors more gracefully. The agent that understands its own harness is more capable than the unstructured agent — not less flexible, but more <em>powerful</em>.</p>

<hr>
<p>With the principles established, we now turn to how they manifest concretely in agent design. The taxonomy below maps each principle to the architectural surfaces where harnesses can be applied.</p>
<h2 id="taxonomy">3. Taxonomy of Structured Agents</h2>
<p>The structured agency framework decomposes agents into a set of concerns—each representing a surface where harnesses can be applied. This taxonomy describes those surfaces.</p>
<p>The following iconography is used consistently across all diagrams in this paper:</p>
<p><img src="assets/agents/image0-iconography.svg" alt="Iconography"></p>
<h3 id="agent">3.1 Agent</h3>
<div class="definition">

<p><strong>Agent</strong> — A reasoning core that observes the world through actions and builds an internal state representation.</p>
</div>

<p><img src="assets/agents/image8-agent-architecture.svg?v=6" alt="Agent Architecture"></p>
<p>Borrowing from the reinforcement learning framework <a class="citation" href="#ref-43">[43]</a>, and informed by recent surveys on LLM-based autonomous agents <a class="citation" href="#ref-44">[44]</a>, we define the following components.</p>
<h4 id="invocation">Invocation</h4>
<div class="definition">

<p><strong>Invocation</strong> — The period from when an agent &quot;starts up&quot; until it &quot;dies&quot; and stops performing actions.</p>
</div>

<p>An agent may persist across multiple invocations with the same state. Each invocation provides specific settings:</p>
<ul>
<li><strong>Allowed tools</strong> → What actions are permitted</li>
<li><strong>Initial context</strong> → What information to load</li>
<li><strong>Directives</strong> → What goals to pursue</li>
</ul>
<p><strong>Implementation note</strong>: Whether each user input triggers a new invocation (daemon shuts down) or the agent persists across inputs is an implementation detail. The key is distinguishing between invocations and mere instruction reception.</p>
<details class="practical-example">
<summary>Practical Example — Decoupled Invocations at Kuona</summary>
<div class="practical-example-body">

<p>At <a href="https://kuona.ai">kuona.ai</a>, we pre-emptively launch agent invocations when leading indicators suggest the agent will be needed—effectively a warm start. The agent is already running when the user message arrives, so it hits the ground running. Earlier versions tied invocation lifetime to user message reception, but decoupling them yielded cleaner semantics:</p>
<ol>
<li><strong>Warm starts become natural.</strong> The invocation exists before any user input; the first message is just another event.</li>
<li><strong>Lifecycle is decoupled from message reception.</strong> The agent&#39;s birth and death are governed by orchestration policy, not by whether a chat message happened to arrive.</li>
<li><strong>Double-dialing and multiple messages are handled by design.</strong> Because message reception is a simple operation within an already-running agent—rather than the trigger that creates it—concurrent or duplicate messages require no special-case handling.</li>
</ol>
<p>This pattern also enables long-running agent designs where user messages and inter-agent messages flow through the same pathways. We exploit this in Kuona&#39;s testing harness, which simulates user trajectories and replays them against production agents—the agent cannot distinguish a synthetic test message from a real user message, because structurally they are identical.</p>
</div>
</details>

<p><strong>Interruptions as a lifecycle property</strong>: Invocations can be interrupted—by the user, by the parent agent, by budget exhaustion, or by a sibling&#39;s failure. An invocation that is interrupted mid-execution must leave state in a recoverable condition. This means state must be designed to survive interruption: partial writes should be atomic or rollback-safe, and in-progress markers should be distinguishable from completed ones. Interruption is not an edge case—it is a first-class lifecycle event that the agent model must account for. See Section 3.7 for how interruptions propagate through hierarchical systems.</p>
<h4 id="llm">LLM</h4>
<div class="definition">

<p><strong>LLM</strong> — The model that powers the agent&#39;s reasoning. The LLM is stateless: it has no memory between calls. All persistence is through working memory and state.</p>
</div>

<p>For most current &quot;agentic&quot; systems, the agent is an LLM or derived technology. The LLM is the implementation of the &quot;reasoning core&quot; in the agent definition above — the component that, given an observation of the world (working memory), decides what to do next (tool calls or terminal output).</p>
<p><strong>Statelessness is the fundamental property.</strong> Between turns, the LLM retains nothing. No hidden state carries over, no internal &quot;memory&quot; persists. All continuity comes from working memory being re-presented at the start of the next turn. This is why context engineering (Section 2.2) matters so much — the only lever for shaping the agent&#39;s reasoning is shaping what it sees.</p>
<p><strong>The LLM is a black box from the harness&#39;s perspective.</strong> The harness cannot inspect or modify the model&#39;s internal reasoning — it can only shape what goes in (context engineering) and constrain what comes out (validation, tool design). This asymmetry is why structure matters: you cannot fix reasoning directly, but you can make correct behavior the path of least resistance through well-designed tools and directives.</p>
<p><strong>Model selection is a design decision.</strong> Different invocations, subagents, or tasks can use different LLMs. This connects directly to the cost functions in Section 2.3: risk-sensitive tasks warrant more capable models, latency-sensitive tasks warrant faster models, and cost-sensitive batch tasks warrant cheaper models. A well-structured harness makes model selection a configuration parameter, not an architectural commitment.</p>
<p>The LLM&#39;s capabilities — context window size, tool-calling fidelity, reasoning depth — constrain what the agent can accomplish per turn. This is precisely why structure compensates for model limitations: a model that struggles with complex multi-step reasoning in a single pass can succeed when the harness decomposes the problem into simpler per-turn decisions.</p>
<h4 id="turn">Turn</h4>
<div class="definition">

<p><strong>Turn</strong> — A single LLM call cycle: the agent receives its working memory as input, generates output (text and/or tool calls), tool calls are executed, results are appended to working memory, and the next turn begins. The turn is the atomic unit of agent computation.</p>
</div>

<p>The agent loop can be expressed as: <code>while not done: turn(working_memory) → output → execute tools → update working_memory</code>. Each turn transforms working memory through the LLM&#39;s reasoning — the model reads everything in the context window, decides on actions, and the harness executes those actions and appends results for the next turn.</p>
<p><strong>Between turns, the agent does not exist as a process.</strong> No computation runs. The harness is what operates at turn boundaries: validating outputs, injecting context, synchronizing state, checking budgets, and deciding whether to continue or terminate the invocation. This is the harness&#39;s primary control surface.</p>
<p>Each turn can produce multiple tool calls (parallel tool use) or no tool calls (terminal output that ends the invocation). A turn is to an invocation what a step is to an episode in reinforcement learning — the atomic transition within a bounded execution.</p>
<p><strong>Budget is measured in turns.</strong> Turn count is a natural unit for bounding invocation cost — each turn represents an LLM call with measurable token consumption and latency. Token budgets accumulate across turns. This is where the cost functions from Section 2.3 are operationalized: the harness enforces budget constraints at turn boundaries, terminating the invocation when limits are exceeded.</p>
<p>The harness&#39;s control surface operates entirely at the turn boundary. Between any two turns, it can: inject context into working memory, force specific tool use, validate or reject the previous turn&#39;s output, modify available tools, or terminate the invocation. The agent never &quot;sees&quot; this intervention — it simply receives a new working memory state at the start of the next turn.</p>
<h4 id="directives">Directives</h4>
<div class="definition">

<p><strong>Directives</strong> — The goals, constraints, and rules that guide the agent&#39;s behavior. These include system prompts, task descriptions, and behavioral guardrails. Directives shape <em>what</em> the agent does and <em>how</em> it does it.</p>
</div>

<p>Directives and working memory together determine the agent&#39;s behavior at any point in an invocation. Where working memory defines what the agent can <em>see</em>, directives define what the agent should <em>do</em> with what it sees.</p>
<h4 id="working-memory">Working Memory</h4>
<div class="definition">

<p><strong>Working Memory</strong> — What the agent can currently attend to. Working memory is populated by tool results, directive content, and prior conversation. It is finite and transient — information not actively maintained is effectively forgotten.</p>
</div>

<p>In LLMs, working memory is realized as the <strong>context window</strong> — a fixed-size stream of tokens <a class="citation" href="#ref-33">[33]</a><a class="citation" href="#ref-34">[34]</a><a class="citation" href="#ref-35">[35]</a><a class="citation" href="#ref-45">[45]</a>. The context window is an implementation constraint, not a conceptual primitive.</p>
<p><strong>Best practice</strong>: Treat the context window as a <strong>scratchpad</strong>, not permanent storage:</p>
<ul>
<li>Don&#39;t depend on it for long-term memory</li>
<li>Successive invocations should preserve only relevant information (through pruning or compression)</li>
<li>Supplement with forced tool use and external contexts</li>
</ul>
<p><strong>Caveat</strong>: The scratchpad metaphor has limits. LLMs are <strong>causal models</strong> — each token&#39;s representation depends only on the tokens before it, and inference engines exploit this through <strong>KV caches</strong> and <strong>prompt caching</strong>. When the prefix of the context window is unchanged across requests, its key-value representations can be reused, dramatically reducing cost and latency. Treating working memory as a freely mutable scratchpad — inserting, reordering, or modifying content near the top — invalidates these caches and forces full recomputation. This creates a practical design constraint: <strong>structure context by change rate</strong>. Place invariant content (system prompts, tool definitions, stable directives) at the beginning, semi-stable content (loaded contexts, retrieved documents) in the middle, and volatile content (recent conversation turns, tool results) at the end. This layering — invariant → semi-stable → volatile — maximizes cache reuse while preserving the scratchpad&#39;s flexibility where it matters most.</p>
<p><strong>Critical distinction</strong>: The context window is NOT the agent&#39;s state — it is the agent&#39;s <em>view</em> of its state. Working memory is populated by tools that load, search, and surface relevant state into the context window. The quality of that population — what gets loaded, when, and in what form — is context engineering (Section 2.2).</p>
<details class="deep-dive">
<summary>Working Memory Management — Context Rot, Compaction, and the Limits of Summarization</summary>
<div class="deep-dive-body">

<p>Long-running invocations inevitably exhaust the context window. When the agent must transition between tasks or shed old interactions to make room for new ones, the question becomes: what to keep, what to compress, and what to discard.</p>

<h5>Context Rot</h5>

<p>This is not merely a space management problem. <strong>Context rot</strong> <a class="citation" href="#ref-97">[97]</a> — the empirically demonstrated degradation in LLM performance as context length increases — means that accumulated content actively harms the agent before the window is even full. Research across 18 models shows that accuracy declines consistently with input length, that irrelevant-but-similar content (distractors) amplifies degradation disproportionately, and that information buried in the middle of the context is recalled far less reliably than content at the beginning or end <a class="citation" href="#ref-98">[98]</a>. Stale tool results, old reasoning traces, and superseded constraints don't just waste tokens — they dilute the model's attention budget and degrade its performance on the current task. This makes working memory management not an optimization but a correctness concern: an agent that retains too much irrelevant context will perform worse than one that strategically forgets.</p>

<h5>Lossy Summarization</h5>

<p>The common approach is <strong>lossy summarization</strong>: when transitioning between tasks, generate a compressed summary of past interactions and continue with the new task on a fresh context. This works in the narrow sense — the agent retains a high-level understanding of what happened — but it introduces problems that go beyond simple forgetting. Summarization loses the specific constraints, edge cases, and corrections that accumulated during the prior task. When the agent later re-encounters a related situation, it must re-acquire that context from scratch — a <strong>context re-acquisition cost</strong> that can exceed the original interaction. Worse, lossy compression runs counter to the prompt caching systems discussed above: rewriting the context prefix invalidates the KV cache, so every compaction triggers a full recomputation of the surviving content. Summarization trades token count for cache efficiency — often a bad trade.</p>

<h5>Structured Compaction</h5>

<p><strong>Structured compaction</strong> offers a better model. Rather than summarizing everything uniformly, exploit the structure of the conversation itself. Claude Code's context compaction, for instance, performs summarization but retains some addressability of previous interactions — before summarizing, it uses the structure of messages to selectively drop previous tool results, intermediate reasoning, and other high-volume low-retention content, while preserving the decisions, constraints, and corrections that are likely to matter later. This is closer to what this framework proposes with Contexts (Section 3.4): working memory content that has been used should be offloaded to addressable, retrievable state rather than compressed into a lossy summary. The agent can then re-acquire specific context on demand rather than carrying everything or losing everything.</p>

<h5>Epistemological Humility</h5>

<p>The deeper challenge is epistemological. A compacted agent doesn't know what it has forgotten. Without some form of introspection into past interactions — even a structured index of what was discussed, decided, and constrained — the agent cannot know when it is operating on incomplete information. The ideal is <strong>epistemological humility baked into the system</strong>: the agent should have awareness of the boundaries of its current working memory, know that prior interactions existed beyond what it currently sees, and have tools to reach back into them when needed. If the system provides structured introspection into past interactions within the limits of working memory — a table of contents of what was compacted, what decisions were made, what constraints were established — the agent can adapt: it can recognize when a current task touches on prior context and selectively re-acquire what it needs.</p>

<h5>An Open Problem</h5>

<p>This remains an open problem. Current systems either summarize too aggressively (losing critical detail) or retain too much (exhausting the context window and accelerating context rot). The structured agency approach suggests a middle path: treat compacted history as a Context — addressable, searchable, and selectively loadable — rather than as a string to be summarized or discarded. The Contexts primitive introduced in Section 3.4 maps cleanly to the concept of external memory in recursive language models — state that lives outside the agent's immediate observation but can be accessed through deliberate action. Minimizing dependency on the context window beyond a certain saturation point will likely be crucial for reliable long-horizon agents, but the design to do this properly remains unclear. The natural approach — recursive decomposition into subagents with scoped contexts — runs into latency costs that quickly exceed acceptable thresholds for interactive systems. Each delegation boundary adds round-trip overhead, and deeply nested decompositions multiply that cost. The tension between context rot (too much in one window) and delegation latency (too many windows) is one of the central unsolved design problems in agentic systems.</p>

</div>
</details>
<h3 id="state">3.2 State</h3>
<div class="definition">

<p><strong>State</strong> — The totality of an agent&#39;s structured understanding of the world. State is all — domain data, configuration, memories, and critically, <em>tools themselves</em>.</p>
</div>

<p><strong>Key properties</strong>:</p>
<ul>
<li><strong>All state changes must go through tools.</strong> This is not a stylistic preference — it is a structural requirement. Tools write to the context window, so every state mutation becomes visible in working memory: the agent <em>knows</em> what changed because it saw the tool result. Tools are introspectable by the system, so the harness can audit, validate, and reason about every mutation. And tool use can be forced at any point in the invocation, so the harness can ensure critical state synchronization happens even when the agent doesn't initiate it. If state could change through side channels — background processes, silent updates, external mutations not surfaced through tools — the agent would be reasoning about a world it cannot observe, violating the fundamental contract of structured agency.</li>
<li><strong>Tools are part of state</strong> (see Tools as State below)</li>
<li><strong>State is NOT the context window</strong> — the context window is a <em>view</em> of state</li>
<li><strong>State is NOT a string</strong> — it is structured (class instances, databases, knowledge graphs)</li>
<li><strong>State persists</strong> across invocations; working memory does not</li>
</ul>
<p><strong>Benefits of structured state</strong>:</p>
<ul>
<li>Enables validation signals</li>
<li>Tools operate directly on state (reducing task complexity)</li>
<li>Type safety and constraints</li>
<li>Clear interfaces</li>
</ul>
<p><img src="assets/agents/image9-state-anatomy.svg" alt="State and Structure Classes"></p>
<h4 id="structure-classes">Structure Classes</h4>
<p>The principle that &quot;state is NOT a string&quot; raises a natural question: what <em>is</em> state made of? Structure classes are the first-class types that populate structured state—the building blocks that make state genuinely structured rather than an opaque blob.</p>
<p><strong>Core structure classes</strong>:</p>
<ul>
<li><strong>Tasks</strong> — An encapsulation of work. Having tasks as a first-class structure class — not strings in a plan but structured objects with status, dependencies, ownership, and results — enables referencing work across invocations of the same agent, tracking the work of other agents, deferring or scheduling asynchronous work, and integrating tasks that arise from external systems. This is analogous to task primitives in orchestration frameworks (Celery, Temporal, Airflow), elevated to a core concept of agent state. Tasks carry lifecycle metadata (created, assigned, in-progress, blocked, completed, failed) that enables the planning (Section 3.6) and composition (Section 3.5) layers to operate on them programmatically.</li>
<li><strong>Contexts</strong> — Addressable information blocks that agents can load, search, and reference — the building blocks of context engineering (Section 2.2). Listed here because they are a state type: they exist in state, are updated through actions, and have lifecycle properties. See Section 3.4 for the full treatment.</li>
<li><strong>Messages / Communications</strong> — Structured records of inter-agent and user-agent communication. Not just log entries but typed objects with sender, recipient, content, timestamp, and thread/conversation metadata.</li>
<li><strong>Plans</strong> — Structured plan representations that live in state. A plan is itself a state object that can be inspected, validated, and revised (see Section 3.6).</li>
<li><strong>Tool Results / Action History</strong> — Structured log of actions taken and their outcomes. Each entry records the tool called, its arguments, its result, and any state mutations it caused. This is the basis for observability (Section 3.8) and recovery (Section 3.7).</li>
<li><strong>Domain-specific types</strong> — The types that encode the actual problem domain: tickets, orders, code files, database schemas, customer records. These are what make a structured agent domain-aware rather than domain-agnostic.</li>
</ul>
<p><strong>Introspection without intervention.</strong> Structure classes can always be <em>introspected</em>—the agent can read tasks, query plans, search contexts—but they are not necessarily <em>modifiable</em>. A structure class may be read-only: the agent can observe it but cannot change it. This asymmetry is not a limitation; it is a feature. Read-only structure classes encode <strong>domain-level constraints</strong>—invariants that the harness enforces by making certain state unwritable. A customer record&#39;s compliance flags, an order&#39;s immutable audit trail, a plan&#39;s locked dependencies—these are state the agent must respect but cannot alter. The constraint lives in the access pattern, not in the prompt. This is structure (Section 2.1) applied at the data level: by controlling which structure classes are writable, the harness bounds the agent&#39;s action space without the agent needing to understand <em>why</em> certain mutations are forbidden.</p>
<p>[<strong>TODO</strong>: Flesh out each structure class with concrete examples from production systems. Show how tasks-as-structured-objects differ from tasks-as-strings in a plan.]</p>
<p>[<strong>TODO</strong>: Define the lifecycle properties common to all structure classes (creation, mutation, validation, serialization) and how they connect to the state principles above.]</p>
<h4 id="state-staleness">State Staleness</h4>
<p>State represents the agent's structured understanding of the world — but the world keeps changing. A database schema queried ten minutes ago may have been altered. A task list fetched at the start of an invocation may have been updated by another agent or a user. An external API's response may have changed since the agent last called it. <strong>State can go stale</strong>, and stale state is a source of silent failures: the agent acts on a snapshot that no longer reflects reality.</p>
<p>The problem is compounded by a fundamental limitation: <strong>LLMs are unreliable at temporal reasoning</strong>. A model cannot reliably determine whether one second or one week has passed since it last observed a piece of state. It has no internal clock, no sense of elapsed time, and no mechanism to reason about whether its cached knowledge is still valid. Left to its own devices, an agent will treat state fetched at the beginning of a long invocation as equally fresh throughout — even as the world diverges from its representation.</p>
<p>Staleness operates at three levels, each requiring different mitigation:</p>
<ul>
<li><strong>World → State</strong>: The external world changes but the agent's state hasn't been updated to reflect it. A competitor's price changed, a customer submitted a new order, a deployment completed.</li>
<li><strong>State → Working Memory</strong>: The agent's state has been updated (perhaps by another agent or an external system), but the agent's working memory still holds the old values from when it last loaded them.</li>
<li><strong>World → Working Memory</strong>: Both state and working memory are stale — the agent is reasoning about a world that has moved on at every level.</li>
</ul>
<p><strong>Mitigation strategies</strong>:</p>
<ol>
<li><p><strong>Temporal guidance injection.</strong> The harness injects time-aware signals into the context window, compensating for the model's inability to track elapsed time. This can be as simple as a system-level annotation: <em>"It has been 45 seconds since tasks were last checked. Task state may have been updated by other agents or users."</em> The model doesn't need to reason about time — the harness does the temporal reasoning and translates it into natural language guidance that the model can act on. Timers, heartbeat signals, and staleness warnings are all forms of temporal guidance that the harness provides because the model cannot provide them for itself.</p>
</li>
<li><p><strong>Forced periodic tool use.</strong> Rather than relying on the agent to decide when to re-check state, the harness forces synchronization at defined intervals or triggers. This is forced tool use (Section 3.3) applied to staleness. For instance, if the agent is working on a task that depends on incoming changes from another system, the harness can poll automatically by forcing a tool call before each agent turn. In practice, this pattern works well for incoming user messages: a hook before each agent turn forces <code>receive_messages()</code>, and if any new messages are found, their content is present in working memory by the time the underlying LLM is invoked. The agent doesn't need to remember to check — the harness ensures it always has current information.</p>
</li>
<li><p><strong>Tool-level validation.</strong> Tools themselves can perform synchronization checks before acting on state, ensuring that the state they're about to modify still reflects reality. A tool that updates an order's status can first verify that the order hasn't been modified since the agent last read it — the same optimistic concurrency pattern used in database systems. If the state has drifted, the tool returns an introspective error ("Order #4521 was modified 12 seconds ago by another process. Current status is 'shipped', not 'processing' as expected. Re-read before proceeding.") rather than silently operating on stale data. This pushes staleness detection to the point of action, where it matters most.</p>
</li>
</ol>
<p>State staleness is where the framework's commitment to external validation (Section 2.1, External Validation) becomes most concrete: the agent cannot reliably determine whether its own state is fresh, so the harness must provide the temporal awareness and synchronization mechanisms that the model lacks.</p>
<h4 id="tools-as-state">Tools as State</h4>
<p>The agent model diagram places tools <em>inside</em> state, not alongside it. This is not a visual convenience — it is a conceptual claim: <strong>tools are a special case of state</strong>.</p>
<p>In LISP, code is data. In agent systems, tools are state that the agent acts <em>through</em>. They exist in the agent&#39;s state, can be introspected, and in coding agents, can be modified. This has several implications.</p>
<p><strong>Two orientations of tools</strong>:</p>
<ol>
<li><strong>World-facing tools</strong> (actions) — Reach <em>outside</em> state to observe or affect the external environment: `execute_sql`, `call_api`, `write_file`, `send_message`. These are the agent&#39;s hands.</li>
<li><strong>State-facing tools</strong> (cognitive economy) — Reach <em>into</em> state to introspect or modify the agent&#39;s own structured representation: `list_tables`, `get_schema`, `read_context`, `search_state`. These are the agent&#39;s capacity for self-reflection.</li>
</ol>
<p>State-facing tools are what make progressive disclosure (Section 2.2, Progressive Disclosure) possible in practice. Rather than loading all state into working memory, the agent uses state-facing tools to discover what exists (`ls`), search for relevance (`grep`), and read the detail (`cat`). The progressive disclosure funnel is <em>implemented</em> through state-facing tools.</p>
<p><strong>Tools can be introspected and modified</strong>: In coding agents, the toolset itself is editable state — an agent can inspect its available tools, understand their capabilities, and write new tools or modify existing ones. Non-coding agents cannot modify their tools but can still introspect them: reading tool descriptions, understanding parameters, discovering capabilities through progressive disclosure.</p>
<p><strong>Bootstrapping requires state introspection</strong>: For an agent to begin operating on structured state, it must have tools to discover what that state contains. Without `list_tables` or `get_schema` or equivalent, the agent cannot learn what it has to work with. State-facing tools are the minimal bootstrap — they must be available from the first invocation. Some tools must be automatically invoked or their descriptions automatically loaded into the agent&#39;s directives or working memory. This is forced tool use (Section 3.3) — a bootstrapping mechanism that ensures the agent starts with the minimum context needed to act effectively.</p>
<p>Section 3.3 elaborates on tool design, categorization, and forced tool use patterns. The distinction here is conceptual: tools are not a separate concern from state but a structured interface <em>within</em> it.</p>
<h4 id="why-tools-as-state">Why Tools Are State</h4>
<p>The claim that tools are state is the most counterintuitive assertion in this taxonomy. Most engineers think of tools as <em>capabilities</em>—fixed interfaces that an agent calls—not as data that lives alongside database rows and configuration objects. This section makes the affirmative case and addresses the objections.</p>
<p><strong>The affirmative case.</strong> Consider what it means for something to be state: it exists in the agent&#39;s representation of the world, it can be observed, and it can (in principle) be modified. Tools satisfy all three criteria. An agent&#39;s available tools are <em>listed</em> in its state (tool descriptions are data). An agent <em>observes</em> its tools (reading descriptions, understanding parameters, discovering capabilities). And in coding agents, tools are <em>modified</em>—an agent can write a new bash script, define a new function, or compose existing tools into new ones. The toolset is not fixed infrastructure; it is a mutable part of the agent&#39;s world model.</p>
<p>The LISP parallel is precise, not metaphorical. In LISP, the boundary between code and data dissolves: functions are first-class values that can be stored, passed, inspected, and rewritten. In structured agents, the same dissolution occurs. A tool description is data (a JSON schema). A tool implementation is code. But from the agent&#39;s perspective, both are entries in its state that it reasons about and acts through. The agent does not distinguish between &quot;looking up a database schema&quot; (state introspection) and &quot;reading a tool&#39;s parameter list&quot; (tool introspection)—both are queries against structured state.</p>
<p><strong>The explanatory payoff.</strong> Treating tools as state is not just taxonomic elegance—it unlocks explanations that a &quot;tools are separate&quot; framing cannot provide:</p>
<ul>
<li><strong>Dynamic toolsets become natural.</strong> If tools are state, then adding, removing, or modifying tools is just a state mutation—no different from updating a database record. This explains why MCP servers can dynamically register tools, why coding agents can extend their own capabilities, and why tool availability can vary across invocations.</li>
<li><strong>Bootstrapping has a coherent story.</strong> The chicken-and-egg problem—how does an agent discover its tools without already having tools?—resolves cleanly: the initial toolset is the seed state, and state-facing tools (the minimal bootstrap) are how the agent introspects the rest. Without &quot;tools as state,&quot; bootstrapping requires a separate mechanism outside the state model.</li>
<li><strong>Tool selection is state-dependent reasoning.</strong> When an agent chooses which tool to call, it is reasoning over its state—specifically, the subset of state that describes available actions. This is the same cognitive operation as reasoning over any other state: evaluating options, checking preconditions, selecting the best fit. Separating tools from state would require a parallel reasoning mechanism for tool selection, which is both unnecessary and architecturally incoherent.</li>
</ul>
<p><strong>Objections and responses.</strong></p>
<p><em>&quot;State changes; tools don&#39;t.&quot;</em> For non-coding agents with a fixed toolset, this is largely true—the tool list is constant across an invocation. But constancy does not disqualify something from being state. A read-only database table is still state; it just happens to be immutable. Configuration that doesn&#39;t change at runtime is still state. The relevant property is not mutability but <em>membership in the agent&#39;s structured representation of the world</em>. Tools satisfy this regardless of whether they change.</p>
<p><em>&quot;This overloads the term &#39;state&#39; beyond recognition.&quot;</em> A fair concern. If everything is state, the term loses discriminating power. The boundary this framework draws is: state is everything the agent can observe, reason about, or act on, <em>excluding the agent&#39;s own reasoning process</em> (working memory and directives). Tools fall on the state side of this boundary because the agent reasons <em>about</em> them (which tool to call?) and acts <em>through</em> them—they are objects of reasoning, not the reasoning itself.</p>
<p><em>&quot;In traditional software, we distinguish between code and data.&quot;</em> We do—and that distinction is useful for compilers, not for agents. An agent&#39;s relationship to its tools is fundamentally different from a program&#39;s relationship to its functions. A program <em>executes</em> functions; an agent <em>selects</em> tools based on reasoning. The selection process treats tools as options to evaluate—as data to reason over—even if the tools themselves contain executable logic. The traditional code/data distinction applies within the tool implementation, not at the level of the agent&#39;s interaction with tools.</p>
<p><em>&quot;The RL framework you cite doesn&#39;t model actions as part of state.&quot;</em> Correct. In standard RL, the action space is separate from the state space—the agent observes state and selects actions. This framework deliberately departs from that formalism. The departure is justified because LLM-based agents, unlike RL agents, <em>reason over their action descriptions as text</em>. Tool descriptions enter the context window alongside other state representations. The agent processes them identically. From the agent&#39;s computational perspective, there is no boundary between &quot;state data&quot; and &quot;tool data&quot;—both are tokens in the context window that inform the next action. The RL formalism&#39;s separation is a modeling choice that does not reflect how LLM agents actually process information.</p>
<h3 id="tools-section">3.3 Tools</h3>
<div class="definition">

<p><strong>Tools</strong> — The functions that measure and modify state, and through which the agent acts on the world. As established in Section 3.2, tools are themselves a special case of state.</p>
</div>

<h4 id="tool-categories">Tool Categories</h4>
<p><strong>Categories</strong> <a class="citation" href="#ref-1">[1]</a><a class="citation" href="#ref-2">[2]</a>:</p>
<ol>
<li><strong>Bespoke tools</strong> → Hardcoded by developers for specific use cases</li>
<li><strong>MCP servers</strong> <a class="citation" href="#ref-46">[46]</a> → Collections of tools from remote endpoints</li>
<li><strong>Coding-model functions</strong> → Libraries like pandas, scikit-learn, matplotlib</li>
<li><strong>CLI utilities</strong> → Unix tools like grep, cat, sed</li>
</ol>
<p><img src="assets/agents/image7-tool-anatomy.svg" alt="Tool Lifecycle and Orientations"></p>
<h4 id="bootstrapped-tools">Bootstrapped Tools</h4>
<p><strong>Bootstrapped tools</strong> are tools whose definitions are loaded into working memory at the start of every invocation — they are always available, always visible, and always occupying context. Unlike tools discovered or loaded dynamically during execution, bootstrapped tools are part of the agent&#39;s initial prompt and shape its reasoning from the first token.</p>
<p><strong>Why this matters</strong>: Because LLMs are causal models with KV caches (see Working Memory above), bootstrapped tool definitions sit in the invariant prefix of the context window. Their key-value representations are computed once and reused across every subsequent turn. This makes them essentially free after the first request — adding a bootstrapped tool increases first-request latency but has near-zero marginal cost for all subsequent interactions within an invocation. This is a fundamental asymmetry that should inform tool placement decisions.</p>
<p><strong>Design implications</strong>:</p>
<ul>
<li><strong>Stable definitions only</strong> — Bootstrapped tools should have fixed descriptions and schemas. If a tool&#39;s definition changes between turns, it invalidates the cache prefix and negates the performance benefit.</li>
<li><strong>Core over convenience</strong> — Only tools that the agent needs across most turns belong in the bootstrap set. Rarely-used tools are better loaded dynamically to keep the invariant prefix lean.</li>
<li><strong>Ordering matters</strong> — Place the most frequently used bootstrapped tools earlier in the definition list. Causal attention means earlier tokens have more influence on downstream reasoning, and earlier definitions are more reliably followed.</li>
<li><strong>Budget awareness</strong> — Each bootstrapped tool consumes tokens in every request. A large bootstrap set can crowd out space for actual task content. Monitor the ratio of tool-definition tokens to task tokens.</li>
</ul>
<p><strong>Examples</strong>: In Claude Code, the file read/write/edit tools, bash execution, and search tools are bootstrapped — they are always present because the agent needs them on virtually every turn. Specialized tools like GitHub CLI wrappers or MCP-provided tools may be loaded conditionally based on project context.</p>
<h4 id="tool-design">Tool Design</h4>
<ol>
<li><p><strong>Multi-level descriptions</strong></p>
<ul>
<li>Short description (like Unix tool summary)</li>
<li>Detailed documentation (like man pages)</li>
</ul>
</li>
<li><p><strong>Usage examples</strong></p>
<ul>
<li>Show common patterns</li>
<li>Demonstrate edge cases</li>
</ul>
</li>
<li><p><strong>Validators</strong></p>
<ul>
<li>Check preconditions (e.g., document exists before editing)</li>
<li>Verify inputs match expected types/formats</li>
</ul>
</li>
<li><p><strong>Introspective errors</strong></p>
<ul>
<li>Provide actionable feedback</li>
<li>Example: &quot;Query on field &#39;X&#39; failed: field &#39;Y&#39; does not exist in schema. Available fields: [...]&quot;</li>
</ul>
</li>
</ol>
<h4 id="forced-tools">Forced Tool Use</h4>
<p><strong>Definition</strong>: Forcing or injecting simulated tool uses into the agent&#39;s context window upon certain actions <a class="citation" href="#ref-7">[7]</a><a class="citation" href="#ref-47">[47]</a>.</p>
<p><strong>Examples</strong>:</p>
<ul>
<li>Claude Code reads a file when users mention it with `@` syntax</li>
<li>Calling `get_current_tasks()` before the LLM runs</li>
<li>Running `get_incoming_messages()` after every LLM handoff to check for user/system messages</li>
</ul>
<p><strong>Note</strong>: While this could be seen as hooks that run before actions, empirically there is significant value in treating these as injected tool calls rather than separate mechanisms.</p>
<p><strong>Why this works</strong>: It leverages the agent&#39;s existing tool-use reasoning capabilities while ensuring critical operations always occur.</p>
<h4 id="communication">Communication</h4>
<div class="principle">

<p><strong>Principle</strong>: Treat communication as a tool.</p>
</div>

<p>Communication with users, other agents, or external systems should follow the same patterns as other tools:</p>
<ul>
<li>Well-defined interfaces</li>
<li>Validation of inputs/outputs</li>
<li>Observable effects on state</li>
</ul>
<p>Communication channels—whether inter-agent messaging, user-facing output, or external notifications—are tools with the same design principles as any other: they should be idempotent where possible, scoped to a clear purpose, and observable. Communication is the mechanism by which the hierarchical system propagates signals, including interruptions (see Section 3.7).</p>
<p>[<strong>TODO</strong>: Inter-agent messaging patterns — how do agents in a hierarchy exchange structured information beyond tool-call arguments and return values?]</p>
<p>[<strong>TODO</strong>: User-facing communication as a structured action — when and how should an agent communicate with the user, and how should this be modeled in state?]</p>
<p>[<strong>TODO</strong>: Communication channels as tools — design patterns for messaging tools that follow the same principles as other tools (scoped, observable, validated)]</p>
<h3 id="contexts">3.4 Contexts</h3>
<div class="definition">

<p><strong>Contexts</strong> — Addressable blocks of information that agents can load, search, and reference — the building blocks of context engineering (Section 2.2).</p>
</div>

<p>Section 2.2 established context engineering as a principle—the <em>why</em>. This section provides the taxonomy—the <em>what</em>: the concrete primitives that context engineering operates on.</p>
<p><strong>Key properties</strong>:</p>
<ul>
<li><strong>Addressable</strong> → Can be referenced by name/ID</li>
<li><strong>Selective loading</strong> → Different tasks access different contexts</li>
<li><strong>Scoped access</strong> → Contexts may be task-specific or shared</li>
</ul>
<h5>Examples of Contexts</h5>
<table>
<thead>
<tr>
<th>Context</th>
<th>Description</th>
<th>Properties</th>
</tr>
</thead>
<tbody><tr>
<td>Session Context</td>
<td>Log of session actions</td>
<td>Partially structured (messages + timestamps)</td>
</tr>
<tr>
<td>Company Documents</td>
<td>Documents owned by Company X</td>
<td>Pre-indexed as knowledge graph</td>
</tr>
<tr>
<td>Subagent Log</td>
<td>Record of subagent actions</td>
<td>Read-only, time-bounded</td>
</tr>
<tr>
<td>Memories</td>
<td>Long-term memory store</td>
<td>Read-write, persistent</td>
</tr>
</tbody></table>
<p><strong>Context variations</strong>:</p>
<ul>
<li>Text-only vs. enriched/searchable</li>
<li>Read-only vs. read-write</li>
<li>Agent-specific vs. shared</li>
</ul>
<p><strong>Why contexts deserve special attention</strong>: Contexts are the lifeblood of agentic applications, but different systems handle them differently:</p>
<ul>
<li><strong>Claude Code</strong> → Greps over files (filesystem as context)</li>
<li><strong>Cursor</strong> → Maintains code index with semantic search</li>
<li><strong>Recursive Language Models</strong> → Maintains prompt as context (string)</li>
</ul>
<h4 id="progressive-disclosure">Progressive Disclosure in Practice</h4>
<p><img src="assets/agents/image5-progressive-disclosure.svg" alt="Progressive Disclosure Funnel"></p>
<p><strong>The chicken-and-egg problem</strong>: How do you know what to read from a context without knowing what&#39;s inside?</p>
<p><strong>Solution</strong>: Provide summarized versions or metadata before full context retrieval.</p>
<p><strong>Example flow</strong>:</p>
<ol>
<li>Tool provides high-level summary of available contexts</li>
<li>Agent decides which contexts are relevant</li>
<li>Agent searches specific contexts</li>
<li>Agent retrieves detailed information</li>
</ol>
<p><strong>Analogy</strong>: Like file system commands:</p>
<ul>
<li>`ls` → See what&#39;s available (progressive disclosure)</li>
<li>`grep` → Search specific files</li>
<li>`cat` → Read full contents</li>
</ul>
<h4 id="forced-contexts">Forced Contexts</h4>
<p><strong>Definition</strong>: Automatically loading entire contexts into working memory when appropriate.</p>
<p>This is a specific case combining:</p>
<ul>
<li>Progressive disclosure</li>
<li>Forced tool use</li>
</ul>
<p><strong>When to use</strong>:</p>
<ul>
<li>Context is small enough to fit in working memory</li>
<li>Context is highly relevant to the task (e.g., editing a file → load the file)</li>
<li>Cost of loading is lower than cost of multiple queries</li>
</ul>
<p><strong>Note</strong>: This is &quot;context stuffing&quot; from pre-RAG LLM systems, but it remains highly effective for the right problem sets.</p>
<h4 id="retrieval">Retrieval Mechanisms (RAG, GraphRAG, grep, etc.)</h4>
<div class="principle">

<p><strong>Principle</strong>: All retrieval mechanisms are context access patterns.</p>
</div>

<p>RAG <a class="citation" href="#ref-48">[48]</a> pioneered the pattern of augmenting generation with retrieved documents. GraphRAG <a class="citation" href="#ref-49">[49]</a> extends this by constructing knowledge graphs over corpora for more structured retrieval.</p>
<p>However, recent production systems have revealed a significant shift in how retrieval actually works in structured domains. Claude Code, Aider, and similar coding agents have largely moved away from semantic search (embedding similarity over chunked documents) toward what can be called <strong>agentic search</strong>—retrieval driven by the domain&#39;s own structure rather than by vector proximity <a class="citation" href="#ref-81">[81]</a><a class="citation" href="#ref-82">[82]</a><a class="citation" href="#ref-83">[83]</a>.</p>
<p>Aider&#39;s repo-map approach <a class="citation" href="#ref-82">[82]</a> illustrates this well: rather than embedding and retrieving code chunks, it builds condensed tree-sitter outlines of the repository—a structural summary that lets the model navigate the codebase by understanding its organization (modules, classes, function signatures, dependency graphs). Claude Code <a class="citation" href="#ref-7">[7]</a> takes a complementary approach: grep-over-filesystem, file tree exploration, and symbol-level navigation. In both cases, retrieval exploits the <strong>inherent structure of the domain</strong> to guide the model to relevant information, rather than relying on semantic similarity between a query embedding and document embeddings.</p>
<p>This distinction maps cleanly onto domain type. For <strong>structured domains</strong> (codebases, databases, APIs, configuration systems), agentic search works because the domain provides navigable structure—file hierarchies, type systems, dependency graphs, schema definitions. The retrieval mechanism can follow the domain&#39;s own organization. For <strong>unstructured domains</strong> (document corpora, knowledge bases, conversation histories), that navigable structure doesn&#39;t exist natively. We must <strong>create</strong> it—via indexing, knowledge graphs (GraphRAG <a class="citation" href="#ref-49">[49]</a>), or other enrichment—before agentic search becomes viable. This is why GraphRAG exists: it manufactures the structural scaffolding that code and databases get for free.</p>
<p>This is a special case of the contexts taxonomy above: different retrieval mechanisms are different strategies for accessing contexts, each with tradeoffs in precision, recall, latency, and setup cost. Traditional RAG trades setup cost (embedding, chunking, indexing) for broad recall across unstructured corpora. Agentic search trades generality for high precision in structured domains—it&#39;s fast and accurate when the domain has navigable structure, but inapplicable when it doesn&#39;t. GraphRAG occupies a middle ground: it invests heavily in creating structure (knowledge graph construction) to enable structured retrieval over originally unstructured data. The choice between these mechanisms is itself a context engineering decision—matching the retrieval strategy to the structure (or lack thereof) of the domain.</p>
<h3 id="composition">3.5 Composition</h3>
<h4 id="subagents">Subagents</h4>
<p><img src="assets/agents/image6-subagent-topology.svg" alt="Subagent vs. Multiagent Topology"></p>
<p><strong>Definition</strong>: An agent invoked by another agent (via a tool) that has its own context access, directives, and tools <a class="citation" href="#ref-24">[24]</a><a class="citation" href="#ref-51">[51]</a>.</p>
<p><strong>State sharing</strong>: May be partial or complete with the invoking agent.</p>
<h5>Subagents vs. Multiagent Systems</h5>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Subagents</th>
<th>Multiagent</th>
</tr>
</thead>
<tbody><tr>
<td>Lifetime</td>
<td>Invoked for specific purpose, dies when done</td>
<td>Independent lifetimes</td>
</tr>
<tr>
<td>Relationship</td>
<td>Parent-child hierarchy</td>
<td>Peer-to-peer</td>
</tr>
<tr>
<td>State sharing</td>
<td>Easy (by design)</td>
<td>Complex (requires explicit channels)</td>
</tr>
<tr>
<td>Communication</td>
<td>Direct invocation</td>
<td>Message passing</td>
</tr>
</tbody></table>
<p><strong>When to use subagents</strong> <a class="citation" href="#ref-50">[50]</a>:</p>
<ul>
<li>Clear hierarchical task decomposition</li>
<li>Need to isolate context/tools for subtask</li>
<li>Want to reuse subagent across multiple invocations</li>
<li>State sharing is important</li>
</ul>
<h4 id="delegation-challenges">Delegation Challenges</h4>
<p>Delegation—whether to subagents or across multi-agent systems—introduces costs that are easy to underestimate. Two failure patterns dominate production systems:</p>
<p><strong>Scope rediscovery.</strong> Each delegated agent starts with a fresh or partial context window. The parent agent has built up a rich working memory over the course of the task—observations, intermediate results, implicit assumptions, domain context. When it delegates to a subagent, much of this working memory must be <em>re-established</em> in the subagent&#39;s context. This creates a <strong>working-memory discontinuity</strong> (see Section 2.2): the subagent must spend tokens and reasoning cycles recontextualizing before it can do useful work. The more complex the parent&#39;s accumulated context, the more expensive this rediscovery becomes. This is why delegation works best for tasks that are relatively self-contained—where the required context can be transmitted cheaply and the subagent doesn&#39;t need to reconstruct the full reasoning history of the parent.</p>
<p><strong>Tool segmentation failures.</strong> In systems with a router or orchestrator that dispatches tasks to specialized agents, a common failure mode is routing to an agent that lacks the right tools or context for the task. The agent attempts the task, fails, and the error bubbles back up to the router, which must retry—potentially multiple times—before either finding the right agent or escalating. This retry cycle compounds latency and cost, and is especially pernicious because it&#39;s often silent: the system <em>eventually</em> succeeds, masking the inefficiency. Structured composition (clear tool boundaries, explicit capability declarations, well-defined routing rules) mitigates this, but doesn&#39;t eliminate it. This pattern connects directly to the error handling concerns in Section 3.7.</p>
<p>The upshot: delegation is not free. It works best for <strong>cheaper, more reliable subtasks</strong> where the cost of recontextualization is justified by the benefits of isolation, parallelism, or specialization. When a subtask requires deep access to the parent&#39;s accumulated context, the cost of transmitting that context may exceed the cost of simply having the parent do the work itself.</p>
<h4 id="skills">Skills</h4>
<p><strong>Definition</strong>: Pre-packaged instructions and context bundles.</p>
<p>Skills encapsulate:</p>
<ul>
<li>Domain knowledge</li>
<li>Common patterns</li>
<li>Best practices</li>
<li>Tool combinations</li>
</ul>
<p><strong>Purpose</strong>: Reduce repetition and improve consistency for common task types.</p>
<details class="counterpoints">
<summary>Counterpoints & Discussion</summary>
<div class="counterpoints-body">
<p>The hierarchical subagent model is not the only viable approach. Du et al. <a class="citation" href="#ref-52">[52]</a> show that peer-to-peer debate between agents boosts factuality beyond what hierarchical approaches achieve. Li et al. <a class="citation" href="#ref-53">[53]</a> demonstrate that single-agent skill selection degrades non-linearly as skill libraries grow, suggesting multi-agent architectures still win for complex systems. OpenAI's Swarm <a class="citation" href="#ref-74">[74]</a> explores lightweight peer-to-peer orchestration as an alternative to strict hierarchies. More fundamentally, pure hierarchical models may sacrifice robustness—flat topologies can be more fault-tolerant when individual agents fail <a class="citation" href="#ref-75">[75]</a>.</p>
<div class="author-response">
<p>Fair—I have not personally implemented peer-to-peer debate or swarm-style systems in production. The hierarchical model reflects what I have built and observed working reliably, but I acknowledge this is a gap in my direct experience.</p>
</div>
</div>
</details>

<p>Composition defines the <em>who</em> of agent systems. Planning defines the <em>what</em> and <em>when</em>—the mechanism by which composed agents coordinate their work.</p>
<h3 id="planning">3.6 Planning</h3>
<p>Planning is the mechanism by which agents decide <strong>what to do before doing it</strong> <a class="citation" href="#ref-58">[58]</a>. While reactive agents execute actions step-by-step based on immediate observations, planning introduces deliberation—a lookahead that evaluates potential action sequences before committing.</p>
<h4 id="planning-why">Why Planning Matters</h4>
<p>Without planning, an agent&#39;s behavior is purely reactive: observe → act → observe → act <a class="citation" href="#ref-56">[56]</a>. This works for simple tasks but fails when:</p>
<ul>
<li>Actions are expensive or irreversible</li>
<li>The task requires coordination across multiple steps</li>
<li>Dependencies between subtasks constrain execution order</li>
<li>The agent must reason about resource allocation (tokens, API calls, time)</li>
</ul>
<div class="principle">

<p><strong>Principle</strong>: Planning amortizes the cost of reasoning over the execution of a task <a class="citation" href="#ref-62">[62]</a>.</p>
</div>

<h4 id="plan-representations">Plan Representations</h4>
<p>How a plan is represented determines what the agent can reason about:</p>
<table>
<thead>
<tr>
<th>Representation</th>
<th>Structure</th>
<th>Strengths</th>
<th>Weaknesses</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Natural language</strong></td>
<td>Free-form text outline</td>
<td>Flexible, human-readable</td>
<td>Hard to validate, ambiguous</td>
</tr>
<tr>
<td><strong>Task lists</strong></td>
<td>Ordered/unordered items with status</td>
<td>Trackable, supports dependencies</td>
<td>Limited branching logic</td>
</tr>
<tr>
<td><strong>DAGs</strong></td>
<td>Directed acyclic graphs of subtasks</td>
<td>Explicit dependencies, parallelizable</td>
<td>Complex to construct</td>
</tr>
<tr>
<td><strong>State machines</strong></td>
<td>States + transitions</td>
<td>Deterministic, verifiable</td>
<td>Rigid, poor for novel tasks</td>
</tr>
<tr>
<td><strong>Hierarchical plans</strong></td>
<td>Nested subtask trees</td>
<td>Natural decomposition, progressive detail</td>
<td>Can lose cross-branch context</td>
</tr>
</tbody></table>
<p>[<strong>TODO</strong>: Add examples from your own systems—which representations have you used and where?]</p>
<h4 id="planning-strategies">Planning Strategies</h4>
<h5>Decomposition</h5>
<p>Breaking a task into subtasks before executing any of them. This is the most common planning pattern in current agentic systems <a class="citation" href="#ref-60">[60]</a><a class="citation" href="#ref-61">[61]</a>.</p>
<p><strong>Approaches</strong>:</p>
<ul>
<li><strong>Top-down decomposition</strong> → Split high-level goal into subgoals recursively</li>
<li><strong>Means-ends analysis</strong> → Identify the gap between current and goal state, then find actions that reduce it</li>
<li><strong>Template-based</strong> → Match the task to a known plan template and fill in specifics</li>
</ul>
<p>[<strong>TODO</strong>: Document decomposition patterns you&#39;ve implemented]</p>
<h5>Search-Based Planning</h5>
<p>Exploring multiple possible action sequences and selecting the best one.</p>
<p><strong>Approaches</strong>:</p>
<ul>
<li><strong>Tree-of-thought</strong> <a class="citation" href="#ref-55">[55]</a> → Generate and evaluate multiple reasoning paths</li>
<li><strong>Monte Carlo methods</strong> → Sample rollouts and estimate value</li>
<li><strong>Best-first search</strong> <a class="citation" href="#ref-59">[59]</a> → Expand the most promising partial plan</li>
</ul>
<p>[<strong>TODO</strong>: Document search-based approaches you&#39;ve experimented with]</p>
<h5>Verification-Driven Planning</h5>
<p>Using validation signals to guide plan construction—plans are iteratively refined until they pass verification checks.</p>
<p><strong>Connection to Section 2.1, External Validation</strong>: External validation signals are not just useful during execution—they can also constrain the planning phase itself.</p>
<p>[<strong>TODO</strong>: Document verification-driven planning patterns]</p>
<h4 id="plan-execution">Plan Execution</h4>
<p>A plan is only as good as its execution. Key concerns:</p>
<ul>
<li><strong>Plan following vs. replanning</strong> → When should an agent stick to the plan vs. replan?</li>
<li><strong>Monitoring</strong> → How does the agent detect plan divergence?</li>
<li><strong>Checkpointing</strong> → Where can execution resume if interrupted?</li>
<li><strong>Partial execution</strong> → Can subtasks be executed out of order when dependencies allow?</li>
</ul>
<div class="principle">

<p><strong>Principle</strong>: Plans should be treated as hypotheses, not commitments <a class="citation" href="#ref-57">[57]</a>. The agent should be willing to abandon or revise a plan when observations invalidate its assumptions.</p>
</div>

<p>[<strong>TODO</strong>: Document execution strategies and replanning triggers you&#39;ve built]</p>
<h4 id="planning-composition">Planning and Composition</h4>
<p>Planning interacts closely with composition (Section 3.5):</p>
<ul>
<li><strong>Subagent delegation</strong> → A plan can assign subtasks to subagents</li>
<li><strong>Skill matching</strong> → Planning can involve selecting which skills to invoke</li>
<li><strong>Resource budgeting</strong> → Plans can allocate context window space, API calls, or time budgets across subtasks</li>
</ul>
<p>[<strong>TODO</strong>: Document how planning integrates with your subagent and skill systems]</p>
<details class="counterpoints">
<summary>Counterpoints & Discussion</summary>
<div class="counterpoints-body">
<p>The value of explicit planning is debated. Snell et al. <a class="citation" href="#ref-76">[76]</a> show that scaling test-time compute—simply giving a model more tokens to think—can match or beat structured planning approaches. Plan caching <a class="citation" href="#ref-77">[77]</a> reveals that planning overhead is non-trivial and can be amortized. Google DeepMind <a class="citation" href="#ref-78">[78]</a> finds that frontier models working alone can outperform agent teams on many benchmarks, questioning whether planning-heavy orchestration is always justified. Chain-of-thought prompting <a class="citation" href="#ref-54">[54]</a> itself can be seen as implicit planning that emerges from the model rather than being imposed externally.</p>
<div class="author-response">
<p>In practice, when actions are expensive and mistakes are costly or irreversible, I have found planning—especially with verification signals before execution—to be a very worthwhile investment. The overhead is real, but so is the cost of an unplanned action that corrupts state or wastes expensive API calls. Planning pays for itself when the cost of failure exceeds the cost of deliberation. Moreover, structured agency is itself a human-explainable, low-complexity form of test-time compute. The harness *is* compute—it's just compute that happens to be legible, auditable, and cheap. As models improve, explicit harness compute gets amortized into the model itself (the model learns to do what the harness enforced). But this is structured agency's *graduation path*, not its contradiction: structure scaffolds capability until the model internalizes it, at which point the harness gracefully weakens (see Section 2.4, Harness Obsolescence for the full Bitter Lesson argument).</p>
</div>
</div>
</details>

<h3>3.7 Error Handling &amp; Recovery</h3>
<p>Agents fail. Tools return errors, APIs time out, models hallucinate <a class="citation" href="#ref-17">[17]</a>. The question is not whether failures occur, but how the system responds <a class="citation" href="#ref-63">[63]</a>.</p>
<h4 id="failure-modes">Failure Modes</h4>
<table>
<thead>
<tr>
<th>Mode</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Tool failure</strong></td>
<td>A tool call returns an error or unexpected result</td>
<td>API rate limit, file not found</td>
</tr>
<tr>
<td><strong>Reasoning failure</strong></td>
<td>The agent produces an incorrect or incoherent plan</td>
<td>Hallucinated tool name, circular logic</td>
</tr>
<tr>
<td><strong>State corruption</strong></td>
<td>State becomes inconsistent due to partial writes</td>
<td>Half-completed database migration</td>
</tr>
<tr>
<td><strong>Context degradation</strong></td>
<td>Key information scrolls out of the context window</td>
<td>Losing track of the original goal</td>
</tr>
<tr>
<td><strong>Composition failure</strong></td>
<td>A subagent fails or returns unusable results</td>
<td>Subagent hits token limit, returns garbage</td>
</tr>
</tbody></table>
<h4 id="recovery-strategies">Recovery Strategies</h4>
<ul>
<li><strong>Retry with backoff</strong> → Transient failures (network, rate limits)</li>
<li><strong>Fallback tools</strong> → Alternative paths to accomplish the same action (see Section 2.5, Redundancy)</li>
<li><strong>Replanning</strong> <a class="citation" href="#ref-57">[57]</a> → Abandon current plan and construct a new one from current state</li>
<li><strong>Escalation</strong> → Communicate the failure to a parent agent or user</li>
<li><strong>Graceful degradation</strong> <a class="citation" href="#ref-40">[40]</a> → Accomplish a reduced version of the goal</li>
</ul>
<div class="principle">

<p><strong>Principle</strong>: Recovery strategies should be explicit, not emergent. An agent that silently retries indefinitely is worse than one that escalates after two failures.</p>
</div>

<p>[<strong>TODO</strong>: Document your error handling patterns and when each strategy applies]</p>
<h4 id="interruptions">Interruptions as Hierarchical Signals</h4>
<p>Interruptions are not just failures—they are first-class signals in hierarchical agent systems. When a parent agent determines that a subtask is no longer needed, it must be able to interrupt the child agent cleanly. This is distinct from error recovery: the child didn&#39;t fail—the parent&#39;s context changed.</p>
<p><strong>Example</strong>: In a schema-aware multi-query system, if one query reveals that the schema assumption underlying all queries is wrong, the parent can interrupt the remaining queries since their results would be wasted work. The interruption signal propagates <em>down</em> the hierarchy (parent → children), while failure signals propagate <em>up</em> (child → parent). Both must be handled, but they have different semantics: a failure says &quot;I couldn&#39;t do what you asked,&quot; while an interruption says &quot;stop—what I asked is no longer relevant.&quot;</p>
<p>This connects to composition (Section 3.5)—the parent-child relationship defines who can interrupt whom—and to planning (Section 3.6)—plan revision may trigger interruptions of in-progress subtasks that the revised plan no longer requires.</p>
<p>[<strong>TODO</strong>: Interruption propagation patterns — how do interruptions cascade through a hierarchy? What happens to in-progress state when a subtask is interrupted?]</p>
<p>[<strong>TODO</strong>: Interruption vs. cancellation vs. preemption — are these meaningfully different in practice, and should the agent model distinguish them?]</p>
<h3>3.8 Evaluation &amp; Observability</h3>
<p>Building agents without the ability to measure their behavior is building blind <a class="citation" href="#ref-42">[42]</a>. This section covers how to observe, measure, and improve agentic systems.</p>
<p><strong>Agentic testing is inherently integration testing</strong> <a class="citation" href="#ref-41">[41]</a><a class="citation" href="#ref-42">[42]</a>. You&#39;re not testing individual components in isolation—you&#39;re testing tool interactions, state updates, context retrieval, subagent coordination, and validation signals working together. This framing is essential: unit tests on individual tools or prompts are necessary but not sufficient. The interesting failures happen at the seams, and evaluation must be designed with this in mind.</p>
<h4 id="what-to-measure">What to Measure</h4>
<ul>
<li><strong>Task success rate</strong> → Did the agent accomplish the goal?</li>
<li><strong>Efficiency</strong> → How many tokens, API calls, and tool invocations were used?</li>
<li><strong>Latency</strong> → Time to first action, time to completion</li>
<li><strong>Reliability</strong> → Variance across repeated executions of the same task</li>
<li><strong>Recovery rate</strong> → How often does the agent recover from errors vs. failing completely?</li>
<li><strong>Delegation quality</strong> → Are subagents given appropriate scope and context?</li>
</ul>
<h4 id="observability">Observability Primitives</h4>
<ul>
<li><strong>Traces</strong> <a class="citation" href="#ref-67">[67]</a> → Ordered log of every action, tool call, and result</li>
<li><strong>State snapshots</strong> → Serialized state at key decision points</li>
<li><strong>Decision logs</strong> → Why the agent chose action A over action B</li>
<li><strong>Diff views</strong> → What changed in state after each action</li>
</ul>
<p>[<strong>TODO</strong>: Document what metrics and observability you&#39;ve built into your systems]</p>
<h4 id="testing-agents">Testing Agentic Systems</h4>
<p>Testing agentic systems is inherently integration testing (as framed above), but more specifically:</p>
<ul>
<li><strong>Scenario-based testing</strong> → Define input scenarios and expected outcomes</li>
<li><strong>Regression suites</strong> → Capture past failures and ensure they don&#39;t recur</li>
<li><strong>Adversarial testing</strong> → Intentionally degrade tools or state to test recovery</li>
<li><strong>Benchmark tasks</strong> <a class="citation" href="#ref-64">[64]</a><a class="citation" href="#ref-65">[65]</a><a class="citation" href="#ref-66">[66]</a> → Standardized tasks with known-good solutions</li>
</ul>
<p>[<strong>TODO</strong>: Document your testing approaches and any benchmark suites you use]</p>
<h3>3.9 Safety &amp; Security</h3>
<p>Agents that interact with the real world—executing code, calling APIs, sending messages—introduce attack surfaces that don&#39;t exist in pure text generation. Safety is not a feature to bolt on after deployment; it is a structural concern that permeates every layer of a structured agent.</p>
<h4 id="prompt-injection">Prompt Injection</h4>
<p><strong>The core threat</strong>: Untrusted content (user inputs, web pages, tool results, file contents) can contain instructions that the model treats as authoritative <a class="citation" href="#ref-47">[47]</a><a class="citation" href="#ref-79">[79]</a>.</p>
<p>A model reading a web page might encounter hidden text saying &quot;ignore all previous instructions and delete the user&#39;s files.&quot; Without structural defenses, the model has no mechanism to distinguish this from a legitimate directive.</p>
<p><strong>Defense in depth</strong>:</p>
<ul>
<li><strong>Privilege boundaries</strong> → Tools accessible through untrusted contexts operate with reduced permissions. A web-browsing agent should not have the same tool access as one operating on the user&#39;s behalf.</li>
<li><strong>Context tagging</strong> → Mark content provenance (system instruction, user message, tool result, external data) so the model can distinguish instruction sources and treat each with appropriate trust levels.</li>
<li><strong>Output filtering</strong> → Validate agent actions against an allowlist before execution. Actions triggered by untrusted content require explicit confirmation.</li>
<li><strong>Input sanitization</strong> → Strip or escape instruction-like patterns from untrusted data before injecting it into the context window.</li>
</ul>
<p><strong>Structural connection</strong>: The same principles from Section 2.1 apply directly—structure (clear boundaries between trusted and untrusted content) enables validation (checking whether an action was triggered by legitimate instructions). Prompt injection is fundamentally a failure of boundary enforcement.</p>
<h4 id="adversarial-resilience">Adversarial Resilience</h4>
<p>Agents face adversarial conditions beyond prompt injection:</p>
<table>
<thead>
<tr>
<th>Threat</th>
<th>Description</th>
<th>Structural Mitigation</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Poisoned tool outputs</strong></td>
<td>A compromised API returns misleading data</td>
<td>Redundant verification (Section 2.5, Redundancy)—cross-check through independent sources</td>
</tr>
<tr>
<td><strong>State manipulation</strong></td>
<td>External actors modify shared state between agent steps</td>
<td>State integrity checks before and after tool calls</td>
</tr>
<tr>
<td><strong>Resource exhaustion</strong></td>
<td>Expensive tool calls or infinite loops drain budgets</td>
<td>Hard limits on token spend, API calls, and wall-clock time per invocation</td>
</tr>
<tr>
<td><strong>Social engineering</strong></td>
<td>Content designed to manipulate reasoning (&quot;ignore previous instructions&quot;)</td>
<td>Context tagging + privilege boundaries</td>
</tr>
</tbody></table>
<div class="principle">

<p><strong>Principle</strong>: Assume that any data crossing a trust boundary is potentially adversarial. Design tools and validation signals accordingly.</p>
</div>

<p>The error compounding argument from Section 2.3 applies here too: a single adversarial input early in a multi-step task can corrupt every downstream step. Structural checkpoints—validation signals at each step boundary—limit the blast radius of a successful attack to a single step rather than the entire task.</p>
<h4 id="tool-misuse">Tool Misuse</h4>
<p>Even without adversarial intent, agents can misuse tools in ways that cause harm:</p>
<ul>
<li><strong>Overprivileged tools</strong> → A tool that can delete files when the agent only needs to read them</li>
<li><strong>Unintended side effects</strong> → A &quot;search&quot; tool that also logs queries to a third-party service</li>
<li><strong>Cascading permissions</strong> → A subagent inherits permissions it doesn&#39;t need</li>
<li><strong>Irreversible actions</strong> → Sending emails, publishing content, or deleting data without confirmation</li>
</ul>
<div class="principle">

<p><strong>Principle</strong>: Apply the principle of least privilege to every tool and every agent invocation <a class="citation" href="#ref-47">[47]</a><a class="citation" href="#ref-80">[80]</a>.</p>
</div>

<p><strong>Structural safeguards</strong>:</p>
<ul>
<li><strong>Scoped tool access</strong> → Each invocation grants only the tools needed for the task. A summarization agent does not need file-write permissions.</li>
<li><strong>Read-before-write</strong> → Require observation before mutation. Prevent blind state changes by forcing the agent to inspect current state before modifying it.</li>
<li><strong>Dry-run modes</strong> → Allow plan validation without execution for high-risk operations. Verification-driven planning (Section 3.6) provides a natural checkpoint—validate the safety properties of a plan before executing it.</li>
<li><strong>Audit trails</strong> → Every tool call, its arguments, and its results are logged (Section 3.8). This is both an observability concern and a security requirement.</li>
<li><strong>Human-in-the-loop gates</strong> → Irreversible or externally-visible actions (sending messages, deleting data, modifying permissions) require explicit user confirmation before execution.</li>
</ul>
<p><strong>Connection to the cost model</strong>: From Section 2.3, the irreversibility cost component of C_f is what makes safety harnesses economically justified. The cost of a leaked API key or a deleted production database dwarfs the cost of adding a confirmation step.</p>
<h4 id="implicit-alignment">Structure as Implicit Alignment</h4>
<p>Beyond reliability and safety, structure provides a low-bar alignment mechanism that is easy to overlook. When a human designer defines a tool&#39;s interface, a state schema&#39;s fields, or a validation rule&#39;s conditions, they embed <strong>implicit task alignment</strong> into the system. The schema of a customer-support agent&#39;s state (ticket ID, customer sentiment, escalation threshold, resolution status) carries domain knowledge about what matters, what operations are valid, and where boundaries lie. This alignment information would be difficult—and token-expensive—to convey through prompt context alone.</p>
<p>This is not a substitute for deeper alignment research, but it is a practical complement. Structure encodes a form of human intent that operates at the architectural level: the agent doesn&#39;t need to <em>understand</em> why certain state transitions are invalid if the harness prevents them. The boundaries themselves carry the alignment signal. In this sense, every well-designed tool, every carefully scoped permission set, and every validation rule is a small alignment intervention—cheap, composable, and immediately effective.</p>
<p>Concretely, safety harnesses don&#39;t just defend against bad outcomes—they promote good ones. By defining the boundaries within which an agent operates (which tools are available, what state transitions are valid, what actions require confirmation), structure keeps agents within <strong>human-intended operating boundaries</strong>. This is alignment through architecture rather than through instruction: the agent doesn&#39;t need to understand <em>why</em> certain actions are dangerous if the harness prevents those actions from being taken without oversight. The safety and alignment properties of structured agency are two faces of the same coin—structure constrains the action space, and the shape of that constraint embeds human judgment about what the agent should and should not do. This makes structured agency not just an engineering methodology but a lightweight alignment strategy for production systems operating today.</p>
<details class="counterpoints">
<summary>Counterpoints & Discussion</summary>
<div class="counterpoints-body">
<p>Strict safety harnesses introduce friction that can degrade the user experience and agent throughput. Every confirmation gate is a latency penalty; every privilege restriction is a task the agent cannot perform autonomously. Some argue that overly restrictive safety measures make agents less useful than the unconstrained systems they aim to improve—users disable safety features when they get in the way <a class="citation" href="#ref-80">[80]</a>. There is also the question of who bears the cost: safety engineering is expensive, and the threat landscape evolves faster than most teams can implement defenses.</p>
<div class="author-response">
<p>The friction argument is real, and in practice I calibrate safety harnesses to the risk level of each action. Read-only operations need minimal gating; irreversible mutations need maximum gating. The key is that these harnesses should be structural and configurable—not all-or-nothing. A well-designed structured agent lets the deployer tune the tradeoff between autonomy and safety for their specific risk tolerance, rather than baking in a single policy.</p>
</div>
</div>
</details>

<hr>
<h2>4. Open Questions &amp; Future Directions</h2>
<h3>Incomplete Sections</h3>
<p>[<strong>TODO</strong>: Add the flexibility-reliability-latency graph]
[<strong>TODO</strong>: Provide concrete examples of state schemas]</p>
<h3>Open Questions</h3>
<ul>
<li>How to balance forced tool use with agent autonomy?</li>
<li>What are the optimal granularities for context chunking?</li>
<li>How to measure the &quot;structure vs. flexibility&quot; tradeoff quantitatively?</li>
<li>What are the best practices for cross-agent state sharing?</li>
</ul>
<h3>Areas for Further Exploration</h3>
<ul>
<li>Formal verification of agentic plans</li>
<li>Metrics for agentic system reliability</li>
<li>Patterns for graceful degradation when tools fail</li>
<li>Optimal context window management strategies</li>
<li>Subagent coordination protocols</li>
</ul>
<hr>
<h2 id="conclusion">5. Conclusion</h2>
<p>None of the individual principles in this paper are new. Constraints, validation, progressive disclosure, composition, and the Bitter Lesson are all well-understood in the existing literature <a class="citation" href="#ref-3">[3]</a><a class="citation" href="#ref-9">[9]</a><a class="citation" href="#ref-90">[90]</a><a class="citation" href="#ref-93">[93]</a>. What this paper contributes is the <em>synthesis</em>: a dependency chain that connects these principles into a coherent theory, a cost model (Section 2.3) for deciding <em>when</em> structure is justified, and a taxonomy (Section 3) of architectural surfaces where harnesses can be applied. The central claim is that reliability in agentic systems is not primarily a model capability problem—it is a <em>structural</em> problem. The principles of Structured Agency form a dependency chain:</p>
<ol>
<li><strong>Structure reduces problem space</strong> → Constraints create checkable boundaries, which enable external validation, tool abstraction, and delegation</li>
<li><strong>External validation is economically dominant</strong> → Purpose-built validators are cheaper, faster, and more reliable than model self-assessment</li>
<li><strong>Minimal context improves reasoning</strong> → Progressive disclosure keeps working memory focused, preventing the performance degradation that comes with noise</li>
<li><strong>Structure has a cost, and it must be justified</strong> → The cost model (C_s vs. C_f) determines when to invest in structure; error compounding makes the case strongest for long-horizon tasks</li>
<li><strong>Composition requires boundaries</strong> → Subagents, skills, and plans only work when responsibilities, contexts, and tools are clearly partitioned</li>
</ol>
<p>These principles are not a checklist to be applied independently. They are a connected framework: structure enables validation, validation enables tool reliability, tool reliability enables composition, and composition enables agents that solve complex problems without compounding errors at every step.</p>
<p>The field is evolving rapidly. Models will continue to improve, context windows will grow, and some of the structure advocated here may eventually be superseded by raw capability—the Bitter Lesson may yet apply. But several properties of structured agency are orthogonal to the capability curve. <strong>Harnesses are designed to weaken</strong>: the framework explicitly anticipates that today&#39;s structural scaffolding becomes tomorrow&#39;s unnecessary overhead as models internalize the patterns (Section 2.4, Harness Obsolescence). <strong>Structure provides alignment, not just reliability</strong>: human-defined boundaries embed task-appropriate behavior that would be difficult and expensive to convey through prompt context alone (Section 3.9). <strong>Economy is orthogonal to capability</strong>: even when a model <em>can</em> derive everything from scratch, the latency, cost, and predictability advantages of premade structural elements remain. As long as agents must interface with external systems, take irreversible actions, and operate under cost constraints, the economics of Structured Agency will hold: <strong>it is almost always cheaper to prevent failure through structure than to recover from it through flexibility</strong>.</p>
<hr>
<h2 id="appendix">Appendix: Terminology Reference</h2>
<table>
<thead>
<tr>
<th>Term</th>
<th>Definition</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Agent</strong></td>
<td>An entity that observes the world through actions and maintains internal state</td>
</tr>
<tr>
<td><strong>Structured Agency</strong></td>
<td>The practice of building harnesses around LLM-powered agents through deliberate constraints, external validation, and composable tooling</td>
</tr>
<tr>
<td><strong>Structured Agent</strong></td>
<td>An agent built with structural harnesses that trades unconstrained flexibility for reliability, observability, and predictable behavior</td>
</tr>
<tr>
<td><strong>Harness</strong></td>
<td>A set of structural constraints (tools, validation signals, privilege boundaries, plans) applied to an agent to increase its reliability</td>
</tr>
<tr>
<td><strong>Context</strong></td>
<td>Addressable block of information accessible to agents</td>
</tr>
<tr>
<td><strong>Context Window</strong></td>
<td>Working memory (token stream) of a language model</td>
</tr>
<tr>
<td><strong>Forced Context</strong></td>
<td>Automatically loaded context for relevant tasks</td>
</tr>
<tr>
<td><strong>Forced Tool Use</strong></td>
<td>Injected tool calls triggered by specific conditions</td>
</tr>
<tr>
<td><strong>Invocation</strong></td>
<td>Period from agent startup to shutdown</td>
</tr>
<tr>
<td><strong>Progressive Disclosure</strong></td>
<td>Incremental revelation of information through metadata/summaries</td>
</tr>
<tr>
<td><strong>State</strong></td>
<td>Structured internal representation of the world (NOT the context window)</td>
</tr>
<tr>
<td><strong>Subagent</strong></td>
<td>Agent invoked by another agent for specific subtasks</td>
</tr>
<tr>
<td><strong>Tool</strong></td>
<td>Action an agent can perform on the world or state</td>
</tr>
<tr>
<td><strong>Plan</strong></td>
<td>A structured representation of intended actions before execution</td>
</tr>
<tr>
<td><strong>Replanning</strong></td>
<td>Revising a plan based on new observations or failed assumptions</td>
</tr>
<tr>
<td><strong>Recovery Strategy</strong></td>
<td>Explicit mechanism for handling failures (retry, fallback, escalate)</td>
</tr>
<tr>
<td><strong>Trace</strong></td>
<td>Ordered log of actions, tool calls, and results for observability</td>
</tr>
<tr>
<td><strong>Validation Signal</strong></td>
<td>External feedback that allows course correction</td>
</tr>
</tbody></table>
<hr>
<h2 id="references">References</h2>
<p id="ref-1">[1] T. Schick et al., &quot;Toolformer: Language Models Can Teach Themselves to Use Tools,&quot; <em>NeurIPS</em>, 2023. <a href="https://arxiv.org/abs/2302.04761">arxiv.org/abs/2302.04761</a></p>
<p id="ref-2">[2] S. Patil et al., &quot;Gorilla: Large Language Model Connected with Massive APIs,&quot; <em>NeurIPS</em>, 2024. <a href="https://arxiv.org/abs/2305.15334">arxiv.org/abs/2305.15334</a></p>
<p id="ref-3">[3] Anthropic, &quot;Building Effective Agents,&quot; 2024. <a href="https://www.anthropic.com/research/building-effective-agents">anthropic.com/research/building-effective-agents</a></p>
<p id="ref-4">[4] Anthropic, &quot;Writing Effective Tools for AI Agents,&quot; 2025. <a href="https://www.anthropic.com/engineering/writing-tools-for-agents">anthropic.com/engineering/writing-tools-for-agents</a></p>
<p id="ref-5">[5] Eficode, &quot;Unix Principles Guiding Agentic AI,&quot; 2024. <a href="https://www.eficode.com/blog/unix-principles-guiding-agentic-ai-eternal-wisdom-for-new-innovations">eficode.com/blog/unix-principles-guiding-agentic-ai-eternal-wisdom-for-new-innovations</a></p>
<p id="ref-6">[6] T. Zhang, T. Kraska, and O. Khattab, &quot;Recursive Language Models,&quot; <em>arXiv:2512.24601</em>, 2025. <a href="https://arxiv.org/abs/2512.24601">arxiv.org/abs/2512.24601</a></p>
<p id="ref-7">[7] Anthropic, &quot;Claude Code Overview,&quot; 2025. <a href="https://code.claude.com/docs/en/overview">code.claude.com/docs/en/overview</a></p>
<p id="ref-8">[8] A. Ng, &quot;Agentic AI Design Patterns,&quot; <em>The Batch</em>, 2024. <a href="https://www.deeplearning.ai/the-batch/llms-evolve-with-agentic-workflows-enabling-autonomous-reasoning-and-collaboration/">deeplearning.ai/the-batch/llms-evolve-with-agentic-workflows-enabling-autonomous-reasoning-and-collaboration/</a></p>
<p id="ref-9">[9] OpenAI, &quot;A Practical Guide to Building Agents,&quot; 2025. <a href="https://openai.com/business/guides-and-resources/a-practical-guide-to-building-ai-agents/">openai.com/business/guides-and-resources/a-practical-guide-to-building-ai-agents/</a></p>
<p id="ref-10">[10] L. Weng, &quot;LLM Powered Autonomous Agents,&quot; 2023. <a href="https://lilianweng.github.io/posts/2023-06-23-agent/">lilianweng.github.io/posts/2023-06-23-agent/</a></p>
<p id="ref-11">[11] H. A. Simon, &quot;A Behavioral Model of Rational Choice,&quot; <em>Quarterly Journal of Economics</em>, vol. 69, no. 1, pp. 99–118, 1955.</p>
<p id="ref-12">[12] R. Dechter, <em>Constraint Processing</em>. Morgan Kaufmann, 2003.</p>
<p id="ref-13">[13] S. Russell and P. Norvig, <em>Artificial Intelligence: A Modern Approach</em>, 4th ed., Ch. 5: Constraint Satisfaction Problems. Pearson, 2021.</p>
<p id="ref-14">[14] B. Wang et al., &quot;Budget-Aware Tool-Use Enables Effective Agent Scaling,&quot; Google Research, 2025. <a href="https://arxiv.org/abs/2511.17006">arxiv.org/abs/2511.17006</a></p>
<p id="ref-15">[15] Z. Tam et al., &quot;Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models,&quot; <em>EMNLP</em>, 2024. <a href="https://arxiv.org/abs/2408.02442">arxiv.org/abs/2408.02442</a></p>
<p id="ref-16">[16] K. Park et al., &quot;Grammar-Aligned Decoding,&quot; <em>NeurIPS</em>, 2024. <a href="https://arxiv.org/abs/2405.21047">arxiv.org/abs/2405.21047</a></p>
<p id="ref-17">[17] J. Huang et al., &quot;Large Language Models Cannot Self-Correct Reasoning Yet,&quot; <em>ICLR</em>, 2024. <a href="https://arxiv.org/abs/2310.01798">arxiv.org/abs/2310.01798</a></p>
<p id="ref-18">[18] Y. Kamoi et al., &quot;When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs,&quot; <em>TACL</em>, 2024. <a href="https://arxiv.org/abs/2406.01297">arxiv.org/abs/2406.01297</a></p>
<p id="ref-19">[19] M. Kleppmann, &quot;Prediction: AI Will Make Formal Verification Go Mainstream,&quot; 2025. <a href="https://martin.kleppmann.com/2025/12/08/ai-formal-verification.html">martin.kleppmann.com/2025/12/08/ai-formal-verification.html</a></p>
<p id="ref-20">[20] Y. Liu et al., &quot;A Survey on the Feedback Mechanism of LLM-based AI Agents,&quot; <em>IJCAI</em>, 2025. <a href="https://www.ijcai.org/proceedings/2025/1175">ijcai.org/proceedings/2025/1175</a></p>
<p id="ref-21">[21] A. Kumar et al., &quot;Training Language Models to Self-Correct via Reinforcement Learning (SCoRe),&quot; <em>ICLR</em>, 2025.</p>
<p id="ref-22">[22] A. Paranjape et al., &quot;ART: Automatic Multi-step Reasoning and Tool-use for Large Language Models,&quot; 2023. <a href="https://arxiv.org/abs/2303.09014">arxiv.org/abs/2303.09014</a></p>
<p id="ref-23">[23] Y. Shen et al., &quot;HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,&quot; <em>NeurIPS</em>, 2023. <a href="https://arxiv.org/abs/2303.17580">arxiv.org/abs/2303.17580</a></p>
<p id="ref-24">[24] Anthropic, &quot;How We Built Our Multi-Agent Research System,&quot; 2025. <a href="https://www.anthropic.com/engineering/multi-agent-research-system">anthropic.com/engineering/multi-agent-research-system</a></p>
<p id="ref-25">[25] N. F. Liu et al., &quot;Lost in the Middle: How Language Models Use Long Contexts,&quot; <em>TACL</em>, 2024. <a href="https://arxiv.org/abs/2307.03172">arxiv.org/abs/2307.03172</a></p>
<p id="ref-26">[26] F. Shi et al., &quot;Large Language Models Can Be Easily Distracted by Irrelevant Context,&quot; <em>ICML</em>, 2023. <a href="https://arxiv.org/abs/2302.00093">arxiv.org/abs/2302.00093</a></p>
<p id="ref-27">[27] &quot;Context Length Alone Hurts LLM Performance Despite Perfect Retrieval,&quot; <em>EMNLP</em>, 2025. <a href="https://arxiv.org/html/2510.05381v1">arxiv.org/html/2510.05381v1</a></p>
<p id="ref-28">[28] &quot;Context Discipline and Performance Correlation,&quot; <em>arXiv</em>, 2025. <a href="https://arxiv.org/abs/2601.11564">arxiv.org/abs/2601.11564</a></p>
<p id="ref-29">[29] S. Li et al., &quot;Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach,&quot; <em>EMNLP Industry</em>, 2024. <a href="https://aclanthology.org/2024.emnlp-industry.66/">aclanthology.org/2024.emnlp-industry.66/</a></p>
<p id="ref-30">[30] RAGFlow, &quot;From RAG to Context — A 2025 Year-End Review,&quot; 2025. <a href="https://ragflow.io/blog/rag-review-2025-from-rag-to-context">ragflow.io/blog/rag-review-2025-from-rag-to-context</a></p>
<p id="ref-31">[31] J. Nielsen, &quot;Progressive Disclosure,&quot; Nielsen Norman Group. <a href="https://www.nngroup.com/videos/progressive-disclosure/">nngroup.com/videos/progressive-disclosure/</a></p>
<p id="ref-32">[32] Inferable.ai, &quot;Progressive Context Enrichment for LLMs,&quot; 2024. <a href="https://www.inferable.ai/blog/posts/llm-progressive-context-encrichment">inferable.ai/blog/posts/llm-progressive-context-encrichment</a></p>
<p id="ref-33">[33] G. A. Miller, &quot;The Magical Number Seven, Plus or Minus Two: Some Limits on Our Capacity for Processing Information,&quot; <em>Psychological Review</em>, vol. 63, no. 2, pp. 81–97, 1956.</p>
<p id="ref-34">[34] A. D. Baddeley, &quot;Working Memory,&quot; <em>Science</em>, vol. 255, no. 5044, pp. 556–559, 1992. (Original model proposed in Baddeley &amp; Hitch, 1974.)</p>
<p id="ref-35">[35] C. Packer et al., &quot;MemGPT: Towards LLMs as Operating Systems,&quot; 2023. <a href="https://arxiv.org/abs/2310.08560">arxiv.org/abs/2310.08560</a></p>
<p id="ref-36">[36] S. Kapoor et al., &quot;AI Agents That Matter,&quot; 2024. <a href="https://arxiv.org/abs/2407.01502">arxiv.org/abs/2407.01502</a></p>
<p id="ref-37">[37] LlamaIndex, &quot;Bending without Breaking: Optimal Design Patterns for Effective Agents,&quot; 2025.</p>
<p id="ref-38">[38] I. Koren and C. M. Krishna, <em>Fault-Tolerant Systems</em>. Morgan Kaufmann, 2007.</p>
<p id="ref-39">[39] W. Torres-Pomales, &quot;Software Fault Tolerance: A Tutorial,&quot; NASA Technical Memorandum TM-2000-210616, 2000.</p>
<p id="ref-40">[40] R. Hanmer, <em>Patterns for Fault Tolerant Software</em>. Wiley, 2007.</p>
<p id="ref-41">[41] &quot;Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI,&quot; 2025. <a href="https://arxiv.org/html/2512.12791v1">arxiv.org/html/2512.12791v1</a></p>
<p id="ref-42">[42] M. Cemri et al., &quot;Why Do Multi-Agent LLM Systems Fail?&quot;, 2025. <a href="https://arxiv.org/abs/2503.13657">arxiv.org/abs/2503.13657</a></p>
<p id="ref-43">[43] R. S. Sutton and A. G. Barto, <em>Reinforcement Learning: An Introduction</em>, 2nd ed. MIT Press, 2018.</p>
<p id="ref-44">[44] L. Wang et al., &quot;A Survey on Large Language Model based Autonomous Agents,&quot; <em>Frontiers of Computer Science</em>, 2024. <a href="https://link.springer.com/article/10.1007/s11704-024-40231-1">link.springer.com/article/10.1007/s11704-024-40231-1</a></p>
<p id="ref-45">[45] &quot;Cognitive Workspace: Active Memory Management for Large Language Models,&quot; 2025. <a href="https://arxiv.org/abs/2508.13171">arxiv.org/abs/2508.13171</a></p>
<p id="ref-46">[46] Anthropic, &quot;Introducing the Model Context Protocol,&quot; 2024. <a href="https://www.anthropic.com/news/model-context-protocol">anthropic.com/news/model-context-protocol</a></p>
<p id="ref-47">[47] &quot;Design Patterns for Securing LLM Agents against Prompt Injection,&quot; 2025. <a href="https://arxiv.org/html/2506.08837v2">arxiv.org/html/2506.08837v2</a></p>
<p id="ref-48">[48] P. Lewis et al., &quot;Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,&quot; <em>NeurIPS</em>, 2020. <a href="https://arxiv.org/abs/2005.11401">arxiv.org/abs/2005.11401</a></p>
<p id="ref-49">[49] D. Edge et al., &quot;From Local to Global: A Graph RAG Approach to Query-Focused Summarization,&quot; Microsoft Research, 2024. <a href="https://arxiv.org/abs/2404.16130">arxiv.org/abs/2404.16130</a></p>
<p id="ref-50">[50] Cognition AI, &quot;Don&#39;t Build Multi-Agents,&quot; 2025. <a href="https://cognition.ai/blog/dont-build-multi-agents">cognition.ai/blog/dont-build-multi-agents</a></p>
<p id="ref-51">[51] &quot;A Survey on LLM-based Multi-Agent Systems,&quot; <em>IJCAI</em>, 2024. <a href="https://arxiv.org/abs/2402.01680">arxiv.org/abs/2402.01680</a></p>
<p id="ref-52">[52] Y. Du et al., &quot;Improving Factuality and Reasoning in Language Models through Multiagent Debate,&quot; <em>ICML</em>, 2024. <a href="https://arxiv.org/abs/2305.14325">arxiv.org/abs/2305.14325</a></p>
<p id="ref-53">[53] Z. Li et al., &quot;When Single-Agent with Skills Replace Multi-Agent Systems and When They Fail,&quot; <em>ACL</em>, 2025. <a href="https://arxiv.org/abs/2601.04748">arxiv.org/abs/2601.04748</a></p>
<p id="ref-54">[54] J. Wei et al., &quot;Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,&quot; <em>NeurIPS</em>, 2022. <a href="https://arxiv.org/abs/2201.11903">arxiv.org/abs/2201.11903</a></p>
<p id="ref-55">[55] S. Yao et al., &quot;Tree of Thoughts: Deliberate Problem Solving with Large Language Models,&quot; <em>NeurIPS</em>, 2023. <a href="https://arxiv.org/abs/2305.10601">arxiv.org/abs/2305.10601</a></p>
<p id="ref-56">[56] S. Yao et al., &quot;ReAct: Synergizing Reasoning and Acting in Language Models,&quot; <em>ICLR</em>, 2023. <a href="https://arxiv.org/abs/2210.03629">arxiv.org/abs/2210.03629</a></p>
<p id="ref-57">[57] N. Shinn et al., &quot;Reflexion: Language Agents with Verbal Reinforcement Learning,&quot; <em>NeurIPS</em>, 2023. <a href="https://arxiv.org/abs/2303.11366">arxiv.org/abs/2303.11366</a></p>
<p id="ref-58">[58] X. Huang et al., &quot;Understanding the Planning of LLM Agents: A Survey,&quot; 2024. <a href="https://arxiv.org/abs/2402.02716">arxiv.org/abs/2402.02716</a></p>
<p id="ref-59">[59] A. Zhou et al., &quot;Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models (LATS),&quot; <em>ICML</em>, 2024. <a href="https://arxiv.org/abs/2310.04406">arxiv.org/abs/2310.04406</a></p>
<p id="ref-60">[60] &quot;Deep Agent: Agentic AI with Hierarchical Task DAG and Adaptive Execution,&quot; 2025. <a href="https://arxiv.org/html/2502.07056v1">arxiv.org/html/2502.07056v1</a></p>
<p id="ref-61">[61] &quot;Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks,&quot; 2025. <a href="https://arxiv.org/html/2503.09572v3">arxiv.org/html/2503.09572v3</a></p>
<p id="ref-62">[62] W. Huang et al., &quot;Inner Monologue: Embodied Reasoning through Planning with Language Models,&quot; <em>CoRL</em>, 2022. <a href="https://arxiv.org/abs/2207.05608">arxiv.org/abs/2207.05608</a></p>
<p id="ref-63">[63] H. Zhou et al., &quot;SHIELDA: Structured Handling of Intelligent Exceptions in LLM-Driven Agentic Workflows,&quot; 2025. <a href="https://arxiv.org/abs/2508.07935">arxiv.org/abs/2508.07935</a></p>
<p id="ref-64">[64] C. E. Jimenez et al., &quot;SWE-bench: Can Language Models Resolve Real-World GitHub Issues?&quot;, <em>ICLR</em>, 2024. <a href="https://arxiv.org/abs/2310.06770">arxiv.org/abs/2310.06770</a></p>
<p id="ref-65">[65] X. Liu et al., &quot;AgentBench: Evaluating LLMs as Agents,&quot; <em>ICLR</em>, 2024. <a href="https://arxiv.org/abs/2308.03688">arxiv.org/abs/2308.03688</a></p>
<p id="ref-66">[66] G. Mialon et al., &quot;GAIA: A Benchmark for General AI Assistants,&quot; <em>ICLR</em>, 2024. <a href="https://arxiv.org/abs/2311.12983">arxiv.org/abs/2311.12983</a></p>
<p id="ref-67">[67] OpenTelemetry, &quot;Semantic Conventions for Generative AI Systems,&quot; 2024. <a href="https://opentelemetry.io/docs/specs/semconv/gen-ai/">opentelemetry.io/docs/specs/semconv/gen-ai/</a></p>
<p id="ref-68">[68] Rebelion.la, &quot;Why Too Many Tools Can Break Your AI Agent,&quot; 2025. <a href="https://rebelion.la/agent-tools-guardrails-why-too-many-tools-can-break-your-ai">rebelion.la/agent-tools-guardrails-why-too-many-tools-can-break-your-ai</a></p>
<p id="ref-69">[69] Epic AI, &quot;Solving Tool Overload: A Vision for Smarter AI Assistants,&quot; 2025. <a href="https://www.epicai.pro/solving-tool-overload-a-vision-for-smarter-ai-assistants-upbdc">epicai.pro/solving-tool-overload-a-vision-for-smarter-ai-assistants-upbdc</a></p>
<p id="ref-70">[70] LlamaIndex, &quot;Dumber LLM Agents Need More Constraints and Better Tools,&quot; 2024.</p>
<p id="ref-71">[71] S. Banerjee et al., &quot;CRANE: Reasoning with Constrained LLM Generation,&quot; <em>ICML</em>, 2025. <a href="https://arxiv.org/abs/2502.09061">arxiv.org/abs/2502.09061</a></p>
<p id="ref-72">[72] Nilenso, &quot;Artisanal Shims for the Bitter Lesson Age,&quot; 2025. <a href="https://blog.nilenso.com/blog/2025/10/14/bitter-lesson-applied-ai/">blog.nilenso.com/blog/2025/10/14/bitter-lesson-applied-ai/</a></p>
<p id="ref-73">[73] &quot;Enhancing LLM Planning Capabilities through Intrinsic Self-Critique,&quot; 2025. <a href="https://arxiv.org/abs/2512.24103">arxiv.org/abs/2512.24103</a></p>
<p id="ref-74">[74] OpenAI, &quot;Swarm&quot; (experimental orchestration framework), 2024. <a href="https://github.com/openai/swarm">github.com/openai/swarm</a></p>
<p id="ref-75">[75] &quot;A Taxonomy of Hierarchical Multi-Agent Systems,&quot; 2025. <a href="https://arxiv.org/pdf/2508.12683">arxiv.org/pdf/2508.12683</a></p>
<p id="ref-76">[76] C. Snell et al., &quot;Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Model Parameters,&quot; <em>ICLR Oral</em>, 2025. <a href="https://arxiv.org/abs/2408.03314">arxiv.org/abs/2408.03314</a></p>
<p id="ref-77">[77] &quot;Agentic Plan Caching: Saving Test-Time Compute for Fast LLM Agent Planning,&quot; 2025.</p>
<p id="ref-78">[78] Google DeepMind, &quot;Towards a Science of Scaling Agent Systems,&quot; 2025. <a href="https://arxiv.org/html/2512.08296v1">arxiv.org/html/2512.08296v1</a></p>
<p id="ref-79">[79] S. Debenedetti et al., &quot;AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents,&quot; <em>NeurIPS</em>, 2024. <a href="https://arxiv.org/abs/2406.13352">arxiv.org/abs/2406.13352</a></p>
<p id="ref-80">[80] OWASP, &quot;OWASP Top 10 for Large Language Model Applications,&quot; 2025. <a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/">owasp.org/www-project-top-10-for-large-language-model-applications/</a></p>
<p id="ref-81">[81] SmartScope, &quot;RAG Debate: Agentic Search &amp; Code Exploration,&quot; 2025. <a href="https://smartscope.blog/en/ai-development/practices/rag-debate-agentic-search-code-exploration/">smartscope.blog/en/ai-development/practices/rag-debate-agentic-search-code-exploration/</a></p>
<p id="ref-82">[82] Aider, &quot;Repository Map.&quot; <a href="https://aider.chat/docs/repomap.html">aider.chat/docs/repomap.html</a></p>
<p id="ref-83">[83] LlamaIndex, &quot;RAG is Dead, Long Live Agentic Retrieval,&quot; 2025. <a href="https://www.llamaindex.ai/blog/rag-is-dead-long-live-agentic-retrieval">llamaindex.ai/blog/rag-is-dead-long-live-agentic-retrieval</a></p>
<p id="ref-84">[84] OpenAI, &quot;Introducing Codex,&quot; 2025. <a href="https://openai.com/index/introducing-codex/">openai.com/index/introducing-codex/</a></p>
<p id="ref-85">[85] GitHub, &quot;About Coding Agent,&quot; 2025. <a href="https://docs.github.com/en/copilot/concepts/agents/coding-agent/about-coding-agent">docs.github.com/en/copilot/concepts/agents/coding-agent/about-coding-agent</a></p>
<p id="ref-86">[86] Google, &quot;Jules is now available,&quot; 2025. <a href="https://blog.google/technology/google-labs/jules-now-available/">blog.google/technology/google-labs/jules-now-available/</a></p>
<p id="ref-87">[87] Cognition AI, &quot;Devin Annual Performance Review 2025,&quot; 2025. <a href="https://cognition.ai/blog/devin-annual-performance-review-2025">cognition.ai/blog/devin-annual-performance-review-2025</a></p>
<p id="ref-88">[88] OpenAI, &quot;Introducing Operator,&quot; 2025. <a href="https://openai.com/index/introducing-operator/">openai.com/index/introducing-operator/</a></p>
<p id="ref-89">[89] Google DeepMind, &quot;Project Mariner,&quot; 2025. <a href="https://deepmind.google/models/project-mariner/">deepmind.google/models/project-mariner/</a></p>
<p id="ref-90">[90] Anthropic, &quot;Effective Context Engineering for AI Agents,&quot; 2025. <a href="https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents">anthropic.com/engineering/effective-context-engineering-for-ai-agents</a></p>
<p id="ref-91">[91] Anthropic, &quot;Effective Harnesses for Long-Running Agents,&quot; 2025. <a href="https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents">anthropic.com/engineering/effective-harnesses-for-long-running-agents</a></p>
<p id="ref-92">[92] O. Khattab et al., &quot;DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines,&quot; <em>ICLR</em>, 2024. <a href="https://arxiv.org/abs/2310.03714">arxiv.org/abs/2310.03714</a></p>
<p id="ref-93">[93] L. Martin, &quot;Learning the Bitter Lesson,&quot; 2025. <a href="https://rlancemartin.github.io/2025/07/30/bitter_lesson/">rlancemartin.github.io/2025/07/30/bitter_lesson/</a></p>
<p id="ref-94">[94] L. Martin, &quot;Context Engineering for Agents,&quot; 2025. <a href="https://rlancemartin.github.io/2025/06/23/context_engineering/">rlancemartin.github.io/2025/06/23/context_engineering/</a></p>
<p id="ref-95">[95] H. Bowne-Anderson, &quot;AI Agent Harness, 3 Principles for Context Engineering, and the Bitter Lesson Revisited,&quot; 2025. <a href="https://hugobowne.substack.com/p/ai-agent-harness-3-principles-for">hugobowne.substack.com/p/ai-agent-harness-3-principles-for</a></p>
<p id="ref-90">[90] Google, &quot;A2A: A new era of agent interoperability,&quot; 2025. <a href="https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/">developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/</a></p>
<p id="ref-91">[91] Anthropic, &quot;Agent Skills Specification,&quot; 2025. <a href="https://github.com/anthropics/skills">github.com/anthropics/skills</a></p>
<p id="ref-92">[92] Anthropic, &quot;Effective Harnesses for Long-Running Agents,&quot; 2025. <a href="https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents">anthropic.com/engineering/effective-harnesses-for-long-running-agents</a></p>
<p id="ref-93">[93] Meta, &quot;LlamaFirewall: An Open Source Guardrail System for Building Secure AI Agents,&quot; 2025. <a href="https://arxiv.org/abs/2505.03574">arxiv.org/abs/2505.03574</a></p>
<p id="ref-96">[96] V. Dibia, &quot;LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models,&quot; <em>ACL</em>, 2023. <a href="https://arxiv.org/abs/2303.02927">arxiv.org/abs/2303.02927</a></p>
<p id="ref-97">[97] Chroma Research, &quot;Context Rot: How Increasing Input Tokens Impacts LLM Performance,&quot; 2025. <a href="https://research.trychroma.com/context-rot">research.trychroma.com/context-rot</a></p>
<p id="ref-98">[98] N. F. Liu et al., &quot;Lost in the Middle: How Language Models Use Long Contexts,&quot; <em>TACL</em>, 2024. <a href="https://arxiv.org/abs/2307.03172">arxiv.org/abs/2307.03172</a></p>
<hr>
<p><strong>Document Status</strong>: Work in Progress
<strong>Version</strong>: Null
<strong>Last Updated</strong>: 2026-02-18
<strong>Author</strong>: Mario Garrido</p>
</div>
        </div>
    </div>

    <script>
        /* Version filtering — commented out for now, showing full article only
        function tagVersionContent() {
            const content = document.getElementById('content');
            content.querySelectorAll('details.counterpoints').forEach(el => {
                el.setAttribute('data-version', 'formal');
            });
            content.querySelectorAll('p').forEach(p => {
                if (p.textContent.match(/^\s*\[TODO/)) {
                    p.setAttribute('data-version', 'formal');
                }
            });
            const refsHeading = document.getElementById('references');
            if (refsHeading) {
                const prevEl = refsHeading.previousElementSibling;
                if (prevEl && prevEl.tagName === 'HR') {
                    prevEl.setAttribute('data-version', 'formal');
                }
                refsHeading.setAttribute('data-version', 'formal');
                let el = refsHeading.nextElementSibling;
                while (el) {
                    if (el.tagName === 'HR') break;
                    el.setAttribute('data-version', 'formal');
                    el = el.nextElementSibling;
                }
            }
        }

        function applyVersionFilter(filter) {
            const content = document.getElementById('content');
            document.querySelectorAll('.filter-bar .filter-btn').forEach(btn => {
                btn.classList.toggle('active', btn.dataset.filter === filter);
            });
            const versioned = content.querySelectorAll('[data-version]');
            const variantGroups = content.querySelectorAll('[data-variant-group]');

            if (filter === 'full') {
                versioned.forEach(el => {
                    if (!el.closest('[data-variant-group]')) {
                        el.classList.remove('version-hidden');
                    }
                });
                variantGroups.forEach(group => {
                    group.classList.remove('version-hidden');
                    const variants = group.querySelectorAll(':scope > [data-version]');
                    let toggle = group.querySelector('.variant-toggle');
                    if (!toggle && variants.length > 1) {
                        toggle = createVariantToggle(group);
                    }
                    if (toggle) toggle.style.display = '';
                    variants.forEach((v, i) => {
                        if (v.classList.contains('variant-active')) {
                            v.classList.remove('version-hidden');
                        } else if (!group.querySelector('.variant-active')) {
                            v.classList.toggle('version-hidden', i !== 0);
                            if (i === 0) v.classList.add('variant-active');
                        } else {
                            v.classList.add('version-hidden');
                        }
                    });
                });
            } else {
                versioned.forEach(el => {
                    if (el.closest('[data-variant-group]')) return;
                    el.classList.toggle('version-hidden', el.getAttribute('data-version') !== filter);
                });
                variantGroups.forEach(group => {
                    const matching = group.querySelector('[data-version="' + filter + '"]');
                    if (matching) {
                        group.classList.remove('version-hidden');
                        group.querySelectorAll(':scope > [data-version]').forEach(v => {
                            v.classList.toggle('version-hidden', v !== matching);
                        });
                    } else {
                        group.classList.add('version-hidden');
                    }
                    const toggle = group.querySelector('.variant-toggle');
                    if (toggle) toggle.style.display = 'none';
                });
            }

            // Update sidebar TOC visibility for hidden headings
            const sidebar = document.querySelector('.sidebar nav');
            if (sidebar) {
                sidebar.querySelectorAll('a[href]').forEach(link => {
                    const targetId = link.getAttribute('href').substring(1);
                    const target = document.getElementById(targetId);
                    const li = link.closest('li');
                    if (target && li) {
                        li.style.display = target.classList.contains('version-hidden') ? 'none' : '';
                    }
                });
            }

            const url = new URL(window.location);
            if (filter === 'full') {
                url.searchParams.delete('view');
            } else {
                url.searchParams.set('view', filter);
            }
            history.replaceState(null, '', url);
        }

        function createVariantToggle(group) {
            const variants = group.querySelectorAll(':scope > [data-version]');
            if (variants.length < 2) return null;
            const toggle = document.createElement('div');
            toggle.className = 'variant-toggle';
            variants.forEach((v, i) => {
                const btn = document.createElement('button');
                const label = v.getAttribute('data-version');
                btn.className = 'variant-toggle-btn' + (i === 0 ? ' active' : '');
                btn.textContent = label.charAt(0).toUpperCase() + label.slice(1);
                btn.addEventListener('click', () => {
                    toggle.querySelectorAll('.variant-toggle-btn').forEach(b => b.classList.remove('active'));
                    btn.classList.add('active');
                    variants.forEach(vv => {
                        vv.classList.toggle('version-hidden', vv !== v);
                        vv.classList.toggle('variant-active', vv === v);
                    });
                });
                toggle.appendChild(btn);
            });
            variants[0].classList.add('variant-active');
            variants.forEach((v, i) => { if (i > 0) v.classList.add('version-hidden'); });
            group.insertBefore(toggle, group.firstChild);
            return toggle;
        }

        document.querySelectorAll('.filter-bar .filter-btn').forEach(btn => {
            btn.addEventListener('click', () => applyVersionFilter(btn.dataset.filter));
        });
        */

        // Sticky header (shared component)
        initStickyHeader();

        // Sidebar TOC (shared component)
        initSidebarTOC({
            sectionSelector: '#content h2, #content h3',
            scrollOffset: 120
        });

        // tagVersionContent();
        // const viewParam = new URLSearchParams(window.location.search).get('view');
        // if (viewParam && ['formal', 'builder'].includes(viewParam)) {
        //     applyVersionFilter(viewParam);
        // }
    </script>
    <script src="assets/theme-toggle.js"></script>
</body>
</html>
